{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_visualization import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# 为了支持中文显示\n",
    "from matplotlib.font_manager import FontProperties\n",
    "font = FontProperties(fname='./SimHei.ttf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "from models.bert_model import BertModel, BertConfig\n",
    "from dataset.inference_dataloader import preprocessing\n",
    "import warnings\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "font = FontProperties(fname='./SimHei.ttf')\n",
    "\n",
    "\n",
    "class Pretrainer:\n",
    "    def __init__(self,\n",
    "                 max_seq_len,\n",
    "                 batch_size,\n",
    "                 with_cuda=False,\n",
    "                 ):\n",
    "        # 加载配置文件\n",
    "        config_ = configparser.ConfigParser()\n",
    "        config_.read(\"./model_config.ini\")\n",
    "        self.config = config_[\"DEFAULT\"]\n",
    "        # 词量, 注意在这里实际字(词)汇量 = vocab_size - 20,\n",
    "        # 因为前20个token用来做一些特殊功能, 如padding等等\n",
    "        self.vocab_size = int(self.config[\"vocab_size\"])\n",
    "        self.batch_size = batch_size\n",
    "        # 是否使用GPU\n",
    "        cuda_condition = torch.cuda.is_available() and with_cuda\n",
    "        print('cuda_conditon is', cuda_condition)\n",
    "        self.device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "        # 限定的单句最大长度\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # 初始化超参数的配置\n",
    "        bertconfig = BertConfig(vocab_size_or_config_json_file=self.vocab_size)\n",
    "        # 初始化bert模型\n",
    "        self.bert_model = BertModel(config=bertconfig)\n",
    "        self.bert_model.to(self.device)\n",
    "        # 加载字典\n",
    "        self.word2idx = self.load_dic(self.config[\"word2idx_path\"])\n",
    "        # 初始化预处理器\n",
    "        self.process_batch = preprocessing(hidden_dim=bertconfig.hidden_size,\n",
    "                                           max_positions=max_seq_len,\n",
    "                                           word2idx=self.word2idx)\n",
    "        # 加载BERT预训练模型\n",
    "        self.load_model(self.bert_model, dir_path=self.config[\"state_dict_dir\"])\n",
    "        # disable dropout layers\n",
    "        self.bert_model.eval()\n",
    "\n",
    "    def load_dic(self, path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def load_model(self, model, dir_path=\"./output\"):\n",
    "        # 加载模型\n",
    "        checkpoint_dir = self.find_most_recent_state_dict(dir_path)\n",
    "        checkpoint = torch.load(checkpoint_dir)\n",
    "        # 不加载masked language model 和 next sentence 的参数\n",
    "        checkpoint[\"model_state_dict\"] = {k[5:]: v for k, v in checkpoint[\"model_state_dict\"].items()\n",
    "                                          if k[:4] == \"bert\"}\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"], strict=True)\n",
    "        torch.cuda.empty_cache()\n",
    "        model.to(self.device)\n",
    "        print(\"{} loaded for evaluation!\".format(checkpoint_dir))\n",
    "\n",
    "\n",
    "    def __call__(self, text_list, batch_size=1):\n",
    "        \"\"\"\n",
    "        :param text_list:\n",
    "        :param batch_size: 为了注意力矩阵的可视化, batch_size只能为1, 即单句\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 异常判断\n",
    "        if isinstance(text_list, str):\n",
    "            text_list = [text_list, ]\n",
    "        len_ = len(text_list)\n",
    "        text_list = [i for i in text_list if len(i) != 0]\n",
    "        if len(text_list) == 0:\n",
    "            raise NotImplementedError(\"输入的文本全部为空, 长度为0!\")\n",
    "        if len(text_list) < len_:\n",
    "            warnings.warn(\"输入的文本中有长度为0的句子, 它们将被忽略掉!\")\n",
    "\n",
    "        # max_seq_len=self.max_seq_len+2 因为要留出cls和sep的位置\n",
    "        max_seq_len = max([len(i) for i in text_list])\n",
    "        # 预处理, 获取batch\n",
    "        texts_tokens, positional_enc = self.process_batch(text_list, max_seq_len=max_seq_len)\n",
    "        # 准备positional encoding\n",
    "        positional_enc = torch.unsqueeze(positional_enc, dim=0).to(self.device)\n",
    "\n",
    "        # 正向\n",
    "        n_batches = math.ceil(len(texts_tokens) / batch_size)\n",
    "\n",
    "        # 数据按mini batch切片过正向, 这里为了可视化所以吧batch size设为1\n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            # 切片\n",
    "            texts_tokens_ = texts_tokens[start: end].to(self.device)\n",
    "            attention_matrices = self.bert_model.forward(input_ids=texts_tokens_,\n",
    "                                                         positional_enc=positional_enc,\n",
    "                                                         get_attention_matrices=True)\n",
    "            # 因为batch size=1所以直接返回每层的注意力矩阵\n",
    "            # return [i.detach().numpy() for i in attention_matrices]\n",
    "            return [i.cpu().detach().numpy() for i in attention_matrices]\n",
    "\n",
    "\n",
    "\n",
    "    def find_most_recent_state_dict(self, dir_path):\n",
    "        # 找到模型存储的最新的state_dict路径\n",
    "        dic_lis = [i for i in os.listdir(dir_path)]\n",
    "        if len(dic_lis) == 0:\n",
    "            raise FileNotFoundError(\"can not find any state dict in {}!\".format(dir_path))\n",
    "        dic_lis = [i for i in dic_lis if \"model\" in i]\n",
    "        dic_lis = sorted(dic_lis, key=lambda k: int(k.split(\".\")[-1]))\n",
    "        return dir_path + \"/\" + dic_lis[-1]\n",
    "\n",
    "    def plot_attention(self, text, attention_matrices, layer_num, head_num):\n",
    "        labels = [i + \" \" for i in list(text)]\n",
    "        labels = [\"#CLS# \", ] + labels + [\"#SEP# \", ]\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(attention_matrices[layer_num][0][head_num])\n",
    "        plt.yticks(range(len(labels)), labels, fontproperties=font, fontsize=18)\n",
    "        plt.xticks(range(len(labels)), labels, fontproperties=font, fontsize=18)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print('get here!')\n",
    "model = Pretrainer(max_seq_len=256,\n",
    "                   batch_size=1,with_cuda=True\n",
    "                   )\n",
    "# text = \"为什么要上班\"\n",
    "\n",
    "for i in range(100):\n",
    "    text = \"好好学习，天天向上\"\n",
    "    attention_matrices = model(text)\n",
    "    model.plot_attention(text, attention_matrices, layer_num=2, head_num=1)\n",
    "    time.sleep(5)\n",
    "\n",
    "print('prog ends here!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
