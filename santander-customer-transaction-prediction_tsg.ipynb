{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/santander-customer-transaction-prediction\n",
    "\n",
    "https://www.kaggle.com/roydatascience/santander-transaction-stacking-1-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gc\n",
    "import time\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, log_loss\n",
    "from sklearn.impute  import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE_TSG = 2019\n",
    "np.random.seed(RANDOM_STATE_TSG)\n",
    "\n",
    "file_path = 'F:/github_me_repos/Kaggle_code/santander-customer-transaction-prediction/'\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(file_path + 'data/train.csv')\n",
    "df_test = pd.read_csv(file_path + 'data/test.csv')\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 78.01 Mb (0.0% reduction)\n",
      "Mem. usage decreased to 77.82 Mb (74.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.921875</td>\n",
       "      <td>-6.785156</td>\n",
       "      <td>11.906250</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>11.460938</td>\n",
       "      <td>-9.281250</td>\n",
       "      <td>5.117188</td>\n",
       "      <td>18.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.433594</td>\n",
       "      <td>3.964844</td>\n",
       "      <td>3.136719</td>\n",
       "      <td>1.691406</td>\n",
       "      <td>18.515625</td>\n",
       "      <td>-2.398438</td>\n",
       "      <td>7.878906</td>\n",
       "      <td>8.562500</td>\n",
       "      <td>12.781250</td>\n",
       "      <td>-1.091797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>-4.148438</td>\n",
       "      <td>13.859375</td>\n",
       "      <td>5.390625</td>\n",
       "      <td>12.359375</td>\n",
       "      <td>7.042969</td>\n",
       "      <td>5.621094</td>\n",
       "      <td>16.531250</td>\n",
       "      <td>...</td>\n",
       "      <td>7.640625</td>\n",
       "      <td>7.722656</td>\n",
       "      <td>2.583984</td>\n",
       "      <td>10.953125</td>\n",
       "      <td>15.429688</td>\n",
       "      <td>2.033203</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>8.789062</td>\n",
       "      <td>18.359375</td>\n",
       "      <td>1.952148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.609375</td>\n",
       "      <td>-2.746094</td>\n",
       "      <td>12.078125</td>\n",
       "      <td>7.894531</td>\n",
       "      <td>10.585938</td>\n",
       "      <td>-9.085938</td>\n",
       "      <td>6.941406</td>\n",
       "      <td>14.617188</td>\n",
       "      <td>...</td>\n",
       "      <td>2.906250</td>\n",
       "      <td>9.789062</td>\n",
       "      <td>1.669922</td>\n",
       "      <td>1.685547</td>\n",
       "      <td>21.609375</td>\n",
       "      <td>3.142578</td>\n",
       "      <td>-6.519531</td>\n",
       "      <td>8.265625</td>\n",
       "      <td>14.718750</td>\n",
       "      <td>0.396484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.062500</td>\n",
       "      <td>-2.152344</td>\n",
       "      <td>8.953125</td>\n",
       "      <td>7.195312</td>\n",
       "      <td>12.585938</td>\n",
       "      <td>-1.835938</td>\n",
       "      <td>5.843750</td>\n",
       "      <td>14.921875</td>\n",
       "      <td>...</td>\n",
       "      <td>4.464844</td>\n",
       "      <td>4.742188</td>\n",
       "      <td>0.717773</td>\n",
       "      <td>1.421875</td>\n",
       "      <td>23.031250</td>\n",
       "      <td>-1.270508</td>\n",
       "      <td>-2.927734</td>\n",
       "      <td>10.289062</td>\n",
       "      <td>17.968750</td>\n",
       "      <td>-9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.835938</td>\n",
       "      <td>-1.483398</td>\n",
       "      <td>12.875000</td>\n",
       "      <td>6.636719</td>\n",
       "      <td>12.273438</td>\n",
       "      <td>2.449219</td>\n",
       "      <td>5.941406</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.490234</td>\n",
       "      <td>9.523438</td>\n",
       "      <td>-0.150757</td>\n",
       "      <td>9.195312</td>\n",
       "      <td>13.289062</td>\n",
       "      <td>-1.511719</td>\n",
       "      <td>3.925781</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>-8.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_5</td>\n",
       "      <td>0</td>\n",
       "      <td>11.476562</td>\n",
       "      <td>-2.318359</td>\n",
       "      <td>12.609375</td>\n",
       "      <td>8.625000</td>\n",
       "      <td>10.960938</td>\n",
       "      <td>3.560547</td>\n",
       "      <td>4.531250</td>\n",
       "      <td>15.226562</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.308594</td>\n",
       "      <td>6.601562</td>\n",
       "      <td>5.292969</td>\n",
       "      <td>0.440186</td>\n",
       "      <td>14.945312</td>\n",
       "      <td>1.031250</td>\n",
       "      <td>-3.625000</td>\n",
       "      <td>9.765625</td>\n",
       "      <td>12.578125</td>\n",
       "      <td>-4.761719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_6</td>\n",
       "      <td>0</td>\n",
       "      <td>11.812500</td>\n",
       "      <td>-0.083191</td>\n",
       "      <td>9.351562</td>\n",
       "      <td>4.292969</td>\n",
       "      <td>11.132812</td>\n",
       "      <td>-8.023438</td>\n",
       "      <td>6.195312</td>\n",
       "      <td>12.078125</td>\n",
       "      <td>...</td>\n",
       "      <td>8.781250</td>\n",
       "      <td>6.453125</td>\n",
       "      <td>3.533203</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>18.328125</td>\n",
       "      <td>0.584473</td>\n",
       "      <td>9.109375</td>\n",
       "      <td>9.117188</td>\n",
       "      <td>10.890625</td>\n",
       "      <td>-3.208984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_7</td>\n",
       "      <td>0</td>\n",
       "      <td>13.554688</td>\n",
       "      <td>-7.988281</td>\n",
       "      <td>13.875000</td>\n",
       "      <td>7.597656</td>\n",
       "      <td>8.656250</td>\n",
       "      <td>0.831055</td>\n",
       "      <td>5.687500</td>\n",
       "      <td>22.328125</td>\n",
       "      <td>...</td>\n",
       "      <td>13.171875</td>\n",
       "      <td>6.550781</td>\n",
       "      <td>3.990234</td>\n",
       "      <td>5.804688</td>\n",
       "      <td>23.140625</td>\n",
       "      <td>-0.377686</td>\n",
       "      <td>4.218750</td>\n",
       "      <td>9.421875</td>\n",
       "      <td>8.664062</td>\n",
       "      <td>3.480469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_8</td>\n",
       "      <td>0</td>\n",
       "      <td>16.109375</td>\n",
       "      <td>2.443359</td>\n",
       "      <td>13.929688</td>\n",
       "      <td>5.632812</td>\n",
       "      <td>8.804688</td>\n",
       "      <td>6.164062</td>\n",
       "      <td>4.453125</td>\n",
       "      <td>10.187500</td>\n",
       "      <td>...</td>\n",
       "      <td>1.429688</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>1.639648</td>\n",
       "      <td>1.417969</td>\n",
       "      <td>14.835938</td>\n",
       "      <td>-1.994141</td>\n",
       "      <td>-1.073242</td>\n",
       "      <td>8.195312</td>\n",
       "      <td>19.515625</td>\n",
       "      <td>4.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_9</td>\n",
       "      <td>0</td>\n",
       "      <td>12.507812</td>\n",
       "      <td>1.974609</td>\n",
       "      <td>8.898438</td>\n",
       "      <td>5.449219</td>\n",
       "      <td>13.601562</td>\n",
       "      <td>-16.281250</td>\n",
       "      <td>6.062500</td>\n",
       "      <td>16.843750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554199</td>\n",
       "      <td>6.316406</td>\n",
       "      <td>1.037109</td>\n",
       "      <td>3.689453</td>\n",
       "      <td>14.835938</td>\n",
       "      <td>0.446777</td>\n",
       "      <td>14.125000</td>\n",
       "      <td>7.914062</td>\n",
       "      <td>16.234375</td>\n",
       "      <td>14.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target      var_0     var_1      var_2     var_3      var_4  \\\n",
       "0  train_0       0   8.921875 -6.785156  11.906250  5.093750  11.460938   \n",
       "1  train_1       0  11.500000 -4.148438  13.859375  5.390625  12.359375   \n",
       "2  train_2       0   8.609375 -2.746094  12.078125  7.894531  10.585938   \n",
       "3  train_3       0  11.062500 -2.152344   8.953125  7.195312  12.585938   \n",
       "4  train_4       0   9.835938 -1.483398  12.875000  6.636719  12.273438   \n",
       "5  train_5       0  11.476562 -2.318359  12.609375  8.625000  10.960938   \n",
       "6  train_6       0  11.812500 -0.083191   9.351562  4.292969  11.132812   \n",
       "7  train_7       0  13.554688 -7.988281  13.875000  7.597656   8.656250   \n",
       "8  train_8       0  16.109375  2.443359  13.929688  5.632812   8.804688   \n",
       "9  train_9       0  12.507812  1.974609   8.898438  5.449219  13.601562   \n",
       "\n",
       "       var_5     var_6      var_7    ...        var_190    var_191   var_192  \\\n",
       "0  -9.281250  5.117188  18.625000    ...       4.433594   3.964844  3.136719   \n",
       "1   7.042969  5.621094  16.531250    ...       7.640625   7.722656  2.583984   \n",
       "2  -9.085938  6.941406  14.617188    ...       2.906250   9.789062  1.669922   \n",
       "3  -1.835938  5.843750  14.921875    ...       4.464844   4.742188  0.717773   \n",
       "4   2.449219  5.941406  19.250000    ...      -1.490234   9.523438 -0.150757   \n",
       "5   3.560547  4.531250  15.226562    ...      -6.308594   6.601562  5.292969   \n",
       "6  -8.023438  6.195312  12.078125    ...       8.781250   6.453125  3.533203   \n",
       "7   0.831055  5.687500  22.328125    ...      13.171875   6.550781  3.990234   \n",
       "8   6.164062  4.453125  10.187500    ...       1.429688  14.750000  1.639648   \n",
       "9 -16.281250  6.062500  16.843750    ...       0.554199   6.316406  1.037109   \n",
       "\n",
       "     var_193    var_194   var_195    var_196    var_197    var_198    var_199  \n",
       "0   1.691406  18.515625 -2.398438   7.878906   8.562500  12.781250  -1.091797  \n",
       "1  10.953125  15.429688  2.033203   8.125000   8.789062  18.359375   1.952148  \n",
       "2   1.685547  21.609375  3.142578  -6.519531   8.265625  14.718750   0.396484  \n",
       "3   1.421875  23.031250 -1.270508  -2.927734  10.289062  17.968750  -9.000000  \n",
       "4   9.195312  13.289062 -1.511719   3.925781   9.500000  18.000000  -8.812500  \n",
       "5   0.440186  14.945312  1.031250  -3.625000   9.765625  12.578125  -4.761719  \n",
       "6   0.177734  18.328125  0.584473   9.109375   9.117188  10.890625  -3.208984  \n",
       "7   5.804688  23.140625 -0.377686   4.218750   9.421875   8.664062   3.480469  \n",
       "8   1.417969  14.835938 -1.994141  -1.073242   8.195312  19.515625   4.843750  \n",
       "9   3.689453  14.835938  0.446777  14.125000   7.914062  16.234375  14.250000  \n",
       "\n",
       "[10 rows x 202 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_numerics is  ['ID_code', 'var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'var_6', 'var_7', 'var_8', 'var_9', 'var_10', 'var_11', 'var_12', 'var_13', 'var_14', 'var_15', 'var_16', 'var_17', 'var_18', 'var_19', 'var_20', 'var_21', 'var_22', 'var_23', 'var_24', 'var_25', 'var_26', 'var_27', 'var_28', 'var_29', 'var_30', 'var_31', 'var_32', 'var_33', 'var_34', 'var_35', 'var_36', 'var_37', 'var_38', 'var_39', 'var_40', 'var_41', 'var_42', 'var_43', 'var_44', 'var_45', 'var_46', 'var_47', 'var_48', 'var_49', 'var_50', 'var_51', 'var_52', 'var_53', 'var_54', 'var_55', 'var_56', 'var_57', 'var_58', 'var_59', 'var_60', 'var_61', 'var_62', 'var_63', 'var_64', 'var_65', 'var_66', 'var_67', 'var_68', 'var_69', 'var_70', 'var_71', 'var_72', 'var_73', 'var_74', 'var_75', 'var_76', 'var_77', 'var_78', 'var_79', 'var_80', 'var_81', 'var_82', 'var_83', 'var_84', 'var_85', 'var_86', 'var_87', 'var_88', 'var_89', 'var_90', 'var_91', 'var_92', 'var_93', 'var_94', 'var_95', 'var_96', 'var_97', 'var_98', 'var_99', 'var_100', 'var_101', 'var_102', 'var_103', 'var_104', 'var_105', 'var_106', 'var_107', 'var_108', 'var_109', 'var_110', 'var_111', 'var_112', 'var_113', 'var_114', 'var_115', 'var_116', 'var_117', 'var_118', 'var_119', 'var_120', 'var_121', 'var_122', 'var_123', 'var_124', 'var_125', 'var_126', 'var_127', 'var_128', 'var_129', 'var_130', 'var_131', 'var_132', 'var_133', 'var_134', 'var_135', 'var_136', 'var_137', 'var_138', 'var_139', 'var_140', 'var_141', 'var_142', 'var_143', 'var_144', 'var_145', 'var_146', 'var_147', 'var_148', 'var_149', 'var_150', 'var_151', 'var_152', 'var_153', 'var_154', 'var_155', 'var_156', 'var_157', 'var_158', 'var_159', 'var_160', 'var_161', 'var_162', 'var_163', 'var_164', 'var_165', 'var_166', 'var_167', 'var_168', 'var_169', 'var_170', 'var_171', 'var_172', 'var_173', 'var_174', 'var_175', 'var_176', 'var_177', 'var_178', 'var_179', 'var_180', 'var_181', 'var_182', 'var_183', 'var_184', 'var_185', 'var_186', 'var_187', 'var_188', 'var_189', 'var_190', 'var_191', 'var_192', 'var_193', 'var_194', 'var_195', 'var_196', 'var_197', 'var_198', 'var_199']\n"
     ]
    }
   ],
   "source": [
    "df_test.dtypes\n",
    "non_numerics = [x for x in df_test.columns if not (df_test[x].dtype == np.float64 or df_test[x].dtype == np.int64)]\n",
    "print('non_numerics is ', non_numerics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 202), (200000, 201))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = df_train['target']\n",
    "train_X = df_train.drop(columns=['ID_code', 'target'])\n",
    "\n",
    "test_id = df_test['ID_code']\n",
    "test_X = df_test.drop(columns=['ID_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get here!!!\n",
      "fold n°0\n",
      "start regressor training...\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.787665\tvalid_1's auc: 0.758264\n",
      "[200]\ttraining's auc: 0.827815\tvalid_1's auc: 0.794824\n",
      "[300]\ttraining's auc: 0.851109\tvalid_1's auc: 0.815019\n",
      "[400]\ttraining's auc: 0.867667\tvalid_1's auc: 0.829028\n",
      "[500]\ttraining's auc: 0.879975\tvalid_1's auc: 0.838648\n",
      "[600]\ttraining's auc: 0.889676\tvalid_1's auc: 0.846075\n",
      "[700]\ttraining's auc: 0.897772\tvalid_1's auc: 0.85233\n",
      "[800]\ttraining's auc: 0.904753\tvalid_1's auc: 0.857541\n",
      "[900]\ttraining's auc: 0.910599\tvalid_1's auc: 0.861856\n",
      "[1000]\ttraining's auc: 0.915604\tvalid_1's auc: 0.865421\n",
      "[1100]\ttraining's auc: 0.920101\tvalid_1's auc: 0.868321\n",
      "[1200]\ttraining's auc: 0.924015\tvalid_1's auc: 0.870899\n",
      "[1300]\ttraining's auc: 0.927593\tvalid_1's auc: 0.873115\n",
      "[1400]\ttraining's auc: 0.930894\tvalid_1's auc: 0.875174\n",
      "[1500]\ttraining's auc: 0.933856\tvalid_1's auc: 0.876964\n",
      "[1600]\ttraining's auc: 0.936638\tvalid_1's auc: 0.878654\n",
      "[1700]\ttraining's auc: 0.939167\tvalid_1's auc: 0.880032\n",
      "[1800]\ttraining's auc: 0.941468\tvalid_1's auc: 0.881207\n",
      "[1900]\ttraining's auc: 0.943628\tvalid_1's auc: 0.882388\n",
      "[2000]\ttraining's auc: 0.945661\tvalid_1's auc: 0.883495\n",
      "[2100]\ttraining's auc: 0.947611\tvalid_1's auc: 0.884441\n",
      "[2200]\ttraining's auc: 0.949474\tvalid_1's auc: 0.885364\n",
      "[2300]\ttraining's auc: 0.951171\tvalid_1's auc: 0.886253\n",
      "[2400]\ttraining's auc: 0.952803\tvalid_1's auc: 0.887032\n",
      "[2500]\ttraining's auc: 0.954391\tvalid_1's auc: 0.887806\n",
      "[2600]\ttraining's auc: 0.95579\tvalid_1's auc: 0.888442\n",
      "[2700]\ttraining's auc: 0.957181\tvalid_1's auc: 0.889031\n",
      "[2800]\ttraining's auc: 0.95853\tvalid_1's auc: 0.889576\n",
      "[2900]\ttraining's auc: 0.959816\tvalid_1's auc: 0.89012\n",
      "[3000]\ttraining's auc: 0.961022\tvalid_1's auc: 0.89063\n",
      "[3100]\ttraining's auc: 0.962209\tvalid_1's auc: 0.891031\n",
      "[3200]\ttraining's auc: 0.963333\tvalid_1's auc: 0.891533\n",
      "[3300]\ttraining's auc: 0.964429\tvalid_1's auc: 0.891949\n",
      "[3400]\ttraining's auc: 0.965529\tvalid_1's auc: 0.892348\n",
      "[3500]\ttraining's auc: 0.966576\tvalid_1's auc: 0.892671\n",
      "[3600]\ttraining's auc: 0.967605\tvalid_1's auc: 0.892976\n",
      "[3700]\ttraining's auc: 0.968592\tvalid_1's auc: 0.893274\n",
      "[3800]\ttraining's auc: 0.969529\tvalid_1's auc: 0.89351\n",
      "[3900]\ttraining's auc: 0.970465\tvalid_1's auc: 0.893768\n",
      "[4000]\ttraining's auc: 0.971382\tvalid_1's auc: 0.893992\n",
      "[4100]\ttraining's auc: 0.972274\tvalid_1's auc: 0.894204\n",
      "[4200]\ttraining's auc: 0.973152\tvalid_1's auc: 0.894392\n",
      "[4300]\ttraining's auc: 0.973979\tvalid_1's auc: 0.894565\n",
      "[4400]\ttraining's auc: 0.974824\tvalid_1's auc: 0.894739\n",
      "[4500]\ttraining's auc: 0.975625\tvalid_1's auc: 0.894889\n",
      "[4600]\ttraining's auc: 0.976413\tvalid_1's auc: 0.895066\n",
      "[4700]\ttraining's auc: 0.977181\tvalid_1's auc: 0.895173\n",
      "[4800]\ttraining's auc: 0.977919\tvalid_1's auc: 0.895293\n",
      "[4900]\ttraining's auc: 0.978674\tvalid_1's auc: 0.895444\n",
      "[5000]\ttraining's auc: 0.979403\tvalid_1's auc: 0.895576\n",
      "[5100]\ttraining's auc: 0.980117\tvalid_1's auc: 0.895675\n",
      "[5200]\ttraining's auc: 0.980838\tvalid_1's auc: 0.895709\n",
      "[5300]\ttraining's auc: 0.981508\tvalid_1's auc: 0.895762\n",
      "[5400]\ttraining's auc: 0.98215\tvalid_1's auc: 0.895789\n",
      "[5500]\ttraining's auc: 0.982809\tvalid_1's auc: 0.895845\n",
      "[5600]\ttraining's auc: 0.983434\tvalid_1's auc: 0.895903\n",
      "[5700]\ttraining's auc: 0.984032\tvalid_1's auc: 0.895941\n",
      "[5800]\ttraining's auc: 0.98463\tvalid_1's auc: 0.895995\n",
      "[5900]\ttraining's auc: 0.985204\tvalid_1's auc: 0.896056\n",
      "[6000]\ttraining's auc: 0.985761\tvalid_1's auc: 0.896086\n",
      "[6100]\ttraining's auc: 0.986298\tvalid_1's auc: 0.896144\n",
      "[6200]\ttraining's auc: 0.986815\tvalid_1's auc: 0.896175\n",
      "[6300]\ttraining's auc: 0.98731\tvalid_1's auc: 0.896183\n",
      "[6400]\ttraining's auc: 0.987789\tvalid_1's auc: 0.896207\n",
      "[6500]\ttraining's auc: 0.988257\tvalid_1's auc: 0.896221\n",
      "[6600]\ttraining's auc: 0.988719\tvalid_1's auc: 0.896222\n",
      "[6700]\ttraining's auc: 0.989148\tvalid_1's auc: 0.896233\n",
      "Early stopping, best iteration is:\n",
      "[6546]\ttraining's auc: 0.988463\tvalid_1's auc: 0.896247\n",
      "current auc val is  0.8962474578186842\n",
      "fold n°1\n",
      "start regressor training...\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.788913\tvalid_1's auc: 0.758177\n",
      "[200]\ttraining's auc: 0.828452\tvalid_1's auc: 0.793136\n",
      "[300]\ttraining's auc: 0.851567\tvalid_1's auc: 0.813159\n",
      "[400]\ttraining's auc: 0.868208\tvalid_1's auc: 0.827109\n",
      "[500]\ttraining's auc: 0.880098\tvalid_1's auc: 0.837098\n",
      "[600]\ttraining's auc: 0.889588\tvalid_1's auc: 0.844818\n",
      "[700]\ttraining's auc: 0.897391\tvalid_1's auc: 0.851018\n",
      "[800]\ttraining's auc: 0.904048\tvalid_1's auc: 0.856286\n",
      "[900]\ttraining's auc: 0.909813\tvalid_1's auc: 0.860723\n",
      "[1000]\ttraining's auc: 0.914883\tvalid_1's auc: 0.864558\n",
      "[1100]\ttraining's auc: 0.91951\tvalid_1's auc: 0.867956\n",
      "[1200]\ttraining's auc: 0.923511\tvalid_1's auc: 0.870874\n",
      "[1300]\ttraining's auc: 0.927203\tvalid_1's auc: 0.8734\n",
      "[1400]\ttraining's auc: 0.930431\tvalid_1's auc: 0.875565\n",
      "[1500]\ttraining's auc: 0.933346\tvalid_1's auc: 0.877468\n",
      "[1600]\ttraining's auc: 0.936036\tvalid_1's auc: 0.879163\n",
      "[1700]\ttraining's auc: 0.938588\tvalid_1's auc: 0.88087\n",
      "[1800]\ttraining's auc: 0.940924\tvalid_1's auc: 0.882267\n",
      "[1900]\ttraining's auc: 0.943105\tvalid_1's auc: 0.883565\n",
      "[2000]\ttraining's auc: 0.945117\tvalid_1's auc: 0.88485\n",
      "[2100]\ttraining's auc: 0.946983\tvalid_1's auc: 0.886013\n",
      "[2200]\ttraining's auc: 0.94875\tvalid_1's auc: 0.886926\n",
      "[2300]\ttraining's auc: 0.950501\tvalid_1's auc: 0.887831\n",
      "[2400]\ttraining's auc: 0.952113\tvalid_1's auc: 0.888648\n",
      "[2500]\ttraining's auc: 0.95363\tvalid_1's auc: 0.889405\n",
      "[2600]\ttraining's auc: 0.955079\tvalid_1's auc: 0.890109\n",
      "[2700]\ttraining's auc: 0.956457\tvalid_1's auc: 0.89081\n",
      "[2800]\ttraining's auc: 0.957759\tvalid_1's auc: 0.891387\n",
      "[2900]\ttraining's auc: 0.959035\tvalid_1's auc: 0.891979\n",
      "[3000]\ttraining's auc: 0.960258\tvalid_1's auc: 0.892437\n",
      "[3100]\ttraining's auc: 0.96147\tvalid_1's auc: 0.892872\n",
      "[3200]\ttraining's auc: 0.962637\tvalid_1's auc: 0.893352\n",
      "[3300]\ttraining's auc: 0.963742\tvalid_1's auc: 0.893778\n",
      "[3400]\ttraining's auc: 0.964828\tvalid_1's auc: 0.894163\n",
      "[3500]\ttraining's auc: 0.965856\tvalid_1's auc: 0.894487\n",
      "[3600]\ttraining's auc: 0.966898\tvalid_1's auc: 0.894808\n",
      "[3700]\ttraining's auc: 0.967895\tvalid_1's auc: 0.895177\n",
      "[3800]\ttraining's auc: 0.968881\tvalid_1's auc: 0.895467\n",
      "[3900]\ttraining's auc: 0.969818\tvalid_1's auc: 0.895714\n",
      "[4000]\ttraining's auc: 0.9707\tvalid_1's auc: 0.895948\n",
      "[4100]\ttraining's auc: 0.971582\tvalid_1's auc: 0.896132\n",
      "[4200]\ttraining's auc: 0.972443\tvalid_1's auc: 0.896294\n",
      "[4300]\ttraining's auc: 0.973291\tvalid_1's auc: 0.896512\n",
      "[4400]\ttraining's auc: 0.97413\tvalid_1's auc: 0.896676\n",
      "[4500]\ttraining's auc: 0.974961\tvalid_1's auc: 0.896867\n",
      "[4600]\ttraining's auc: 0.975746\tvalid_1's auc: 0.89701\n",
      "[4700]\ttraining's auc: 0.976503\tvalid_1's auc: 0.897103\n",
      "[4800]\ttraining's auc: 0.977275\tvalid_1's auc: 0.897244\n",
      "[4900]\ttraining's auc: 0.978024\tvalid_1's auc: 0.897374\n",
      "[5000]\ttraining's auc: 0.97875\tvalid_1's auc: 0.897506\n",
      "[5100]\ttraining's auc: 0.979438\tvalid_1's auc: 0.897572\n",
      "[5200]\ttraining's auc: 0.980131\tvalid_1's auc: 0.897655\n",
      "[5300]\ttraining's auc: 0.98078\tvalid_1's auc: 0.897718\n",
      "[5400]\ttraining's auc: 0.981436\tvalid_1's auc: 0.897829\n",
      "[5500]\ttraining's auc: 0.98205\tvalid_1's auc: 0.897898\n",
      "[5600]\ttraining's auc: 0.982692\tvalid_1's auc: 0.897945\n",
      "[5700]\ttraining's auc: 0.9833\tvalid_1's auc: 0.898006\n",
      "[5800]\ttraining's auc: 0.98391\tvalid_1's auc: 0.898083\n",
      "[5900]\ttraining's auc: 0.984517\tvalid_1's auc: 0.898106\n",
      "[6000]\ttraining's auc: 0.985076\tvalid_1's auc: 0.898113\n",
      "[6100]\ttraining's auc: 0.985618\tvalid_1's auc: 0.898153\n",
      "[6200]\ttraining's auc: 0.986139\tvalid_1's auc: 0.898177\n",
      "[6300]\ttraining's auc: 0.98666\tvalid_1's auc: 0.898227\n",
      "[6400]\ttraining's auc: 0.987152\tvalid_1's auc: 0.898239\n",
      "[6500]\ttraining's auc: 0.987629\tvalid_1's auc: 0.898279\n",
      "[6600]\ttraining's auc: 0.988099\tvalid_1's auc: 0.898267\n",
      "Early stopping, best iteration is:\n",
      "[6447]\ttraining's auc: 0.987386\tvalid_1's auc: 0.898295\n",
      "current auc val is  0.898294738050152\n",
      "fold n°2\n",
      "start regressor training...\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.78788\tvalid_1's auc: 0.765831\n",
      "[200]\ttraining's auc: 0.828754\tvalid_1's auc: 0.799827\n",
      "[300]\ttraining's auc: 0.852046\tvalid_1's auc: 0.818673\n",
      "[400]\ttraining's auc: 0.868115\tvalid_1's auc: 0.831297\n",
      "[500]\ttraining's auc: 0.880472\tvalid_1's auc: 0.84123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttraining's auc: 0.890169\tvalid_1's auc: 0.848753\n",
      "[700]\ttraining's auc: 0.897906\tvalid_1's auc: 0.854262\n",
      "[800]\ttraining's auc: 0.904424\tvalid_1's auc: 0.85903\n",
      "[900]\ttraining's auc: 0.910223\tvalid_1's auc: 0.863207\n",
      "[1000]\ttraining's auc: 0.915185\tvalid_1's auc: 0.866607\n",
      "[1100]\ttraining's auc: 0.919587\tvalid_1's auc: 0.869669\n",
      "[1200]\ttraining's auc: 0.92346\tvalid_1's auc: 0.872296\n",
      "[1300]\ttraining's auc: 0.927039\tvalid_1's auc: 0.874524\n",
      "[1400]\ttraining's auc: 0.930184\tvalid_1's auc: 0.876716\n",
      "[1500]\ttraining's auc: 0.933082\tvalid_1's auc: 0.878491\n",
      "[1600]\ttraining's auc: 0.935831\tvalid_1's auc: 0.880144\n",
      "[1700]\ttraining's auc: 0.938375\tvalid_1's auc: 0.881541\n",
      "[1800]\ttraining's auc: 0.940708\tvalid_1's auc: 0.88301\n",
      "[1900]\ttraining's auc: 0.943035\tvalid_1's auc: 0.884294\n",
      "[2000]\ttraining's auc: 0.94511\tvalid_1's auc: 0.88546\n",
      "[2100]\ttraining's auc: 0.947031\tvalid_1's auc: 0.88647\n",
      "[2200]\ttraining's auc: 0.948807\tvalid_1's auc: 0.887395\n",
      "[2300]\ttraining's auc: 0.950596\tvalid_1's auc: 0.888231\n",
      "[2400]\ttraining's auc: 0.952214\tvalid_1's auc: 0.889013\n",
      "[2500]\ttraining's auc: 0.953704\tvalid_1's auc: 0.889768\n",
      "[2600]\ttraining's auc: 0.955217\tvalid_1's auc: 0.890502\n",
      "[2700]\ttraining's auc: 0.956624\tvalid_1's auc: 0.891071\n",
      "[2800]\ttraining's auc: 0.957975\tvalid_1's auc: 0.891687\n",
      "[2900]\ttraining's auc: 0.959259\tvalid_1's auc: 0.892221\n",
      "[3000]\ttraining's auc: 0.960517\tvalid_1's auc: 0.892737\n",
      "[3100]\ttraining's auc: 0.961705\tvalid_1's auc: 0.893207\n",
      "[3200]\ttraining's auc: 0.962878\tvalid_1's auc: 0.893567\n",
      "[3300]\ttraining's auc: 0.963997\tvalid_1's auc: 0.893988\n",
      "[3400]\ttraining's auc: 0.965089\tvalid_1's auc: 0.894292\n",
      "[3500]\ttraining's auc: 0.966128\tvalid_1's auc: 0.894606\n",
      "[3600]\ttraining's auc: 0.967125\tvalid_1's auc: 0.894872\n",
      "[3700]\ttraining's auc: 0.968108\tvalid_1's auc: 0.89512\n",
      "[3800]\ttraining's auc: 0.969054\tvalid_1's auc: 0.895377\n",
      "[3900]\ttraining's auc: 0.969996\tvalid_1's auc: 0.89556\n",
      "[4000]\ttraining's auc: 0.970905\tvalid_1's auc: 0.895832\n",
      "[4100]\ttraining's auc: 0.971829\tvalid_1's auc: 0.896047\n",
      "[4200]\ttraining's auc: 0.972687\tvalid_1's auc: 0.896277\n",
      "[4300]\ttraining's auc: 0.973536\tvalid_1's auc: 0.896443\n",
      "[4400]\ttraining's auc: 0.97435\tvalid_1's auc: 0.896607\n",
      "[4500]\ttraining's auc: 0.975148\tvalid_1's auc: 0.896765\n",
      "[4600]\ttraining's auc: 0.975922\tvalid_1's auc: 0.896882\n",
      "[4700]\ttraining's auc: 0.976664\tvalid_1's auc: 0.896999\n",
      "[4800]\ttraining's auc: 0.977441\tvalid_1's auc: 0.897112\n",
      "[4900]\ttraining's auc: 0.978192\tvalid_1's auc: 0.897186\n",
      "[5000]\ttraining's auc: 0.97893\tvalid_1's auc: 0.897258\n",
      "[5100]\ttraining's auc: 0.979652\tvalid_1's auc: 0.897376\n",
      "[5200]\ttraining's auc: 0.980344\tvalid_1's auc: 0.897501\n",
      "[5300]\ttraining's auc: 0.981027\tvalid_1's auc: 0.897535\n",
      "[5400]\ttraining's auc: 0.981678\tvalid_1's auc: 0.897575\n",
      "[5500]\ttraining's auc: 0.982324\tvalid_1's auc: 0.897608\n",
      "[5600]\ttraining's auc: 0.982942\tvalid_1's auc: 0.897663\n",
      "[5700]\ttraining's auc: 0.983531\tvalid_1's auc: 0.897646\n",
      "[5800]\ttraining's auc: 0.984133\tvalid_1's auc: 0.897689\n",
      "[5900]\ttraining's auc: 0.984694\tvalid_1's auc: 0.897764\n",
      "[6000]\ttraining's auc: 0.985231\tvalid_1's auc: 0.897803\n",
      "[6100]\ttraining's auc: 0.985765\tvalid_1's auc: 0.897816\n",
      "[6200]\ttraining's auc: 0.986298\tvalid_1's auc: 0.897854\n",
      "[6300]\ttraining's auc: 0.986796\tvalid_1's auc: 0.897899\n",
      "[6400]\ttraining's auc: 0.987289\tvalid_1's auc: 0.897913\n",
      "[6500]\ttraining's auc: 0.987759\tvalid_1's auc: 0.897937\n",
      "[6600]\ttraining's auc: 0.988222\tvalid_1's auc: 0.897944\n",
      "[6700]\ttraining's auc: 0.988654\tvalid_1's auc: 0.897929\n",
      "Early stopping, best iteration is:\n",
      "[6539]\ttraining's auc: 0.987947\tvalid_1's auc: 0.897965\n",
      "current auc val is  0.8979645495296948\n",
      "fold n°3\n",
      "start regressor training...\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.788872\tvalid_1's auc: 0.768768\n",
      "[200]\ttraining's auc: 0.829564\tvalid_1's auc: 0.802571\n",
      "[300]\ttraining's auc: 0.852859\tvalid_1's auc: 0.820637\n",
      "[400]\ttraining's auc: 0.869356\tvalid_1's auc: 0.832962\n",
      "[500]\ttraining's auc: 0.881398\tvalid_1's auc: 0.841684\n",
      "[600]\ttraining's auc: 0.890846\tvalid_1's auc: 0.848775\n",
      "[700]\ttraining's auc: 0.898554\tvalid_1's auc: 0.854354\n",
      "[800]\ttraining's auc: 0.905107\tvalid_1's auc: 0.859027\n",
      "[900]\ttraining's auc: 0.910857\tvalid_1's auc: 0.863007\n",
      "[1000]\ttraining's auc: 0.915787\tvalid_1's auc: 0.866385\n",
      "[1100]\ttraining's auc: 0.920186\tvalid_1's auc: 0.869317\n",
      "[1200]\ttraining's auc: 0.924118\tvalid_1's auc: 0.871826\n",
      "[1300]\ttraining's auc: 0.927607\tvalid_1's auc: 0.874091\n",
      "[1400]\ttraining's auc: 0.930807\tvalid_1's auc: 0.876116\n",
      "[1500]\ttraining's auc: 0.933703\tvalid_1's auc: 0.877862\n",
      "[1600]\ttraining's auc: 0.936412\tvalid_1's auc: 0.879514\n",
      "[1700]\ttraining's auc: 0.938916\tvalid_1's auc: 0.880921\n",
      "[1800]\ttraining's auc: 0.941235\tvalid_1's auc: 0.882266\n",
      "[1900]\ttraining's auc: 0.943385\tvalid_1's auc: 0.883372\n",
      "[2000]\ttraining's auc: 0.945433\tvalid_1's auc: 0.884534\n",
      "[2100]\ttraining's auc: 0.947351\tvalid_1's auc: 0.885613\n",
      "[2200]\ttraining's auc: 0.949139\tvalid_1's auc: 0.886599\n",
      "[2300]\ttraining's auc: 0.950818\tvalid_1's auc: 0.887356\n",
      "[2400]\ttraining's auc: 0.952413\tvalid_1's auc: 0.888067\n",
      "[2500]\ttraining's auc: 0.953947\tvalid_1's auc: 0.888768\n",
      "[2600]\ttraining's auc: 0.955385\tvalid_1's auc: 0.889376\n",
      "[2700]\ttraining's auc: 0.956783\tvalid_1's auc: 0.889965\n",
      "[2800]\ttraining's auc: 0.958135\tvalid_1's auc: 0.890496\n",
      "[2900]\ttraining's auc: 0.959453\tvalid_1's auc: 0.890992\n",
      "[3000]\ttraining's auc: 0.960744\tvalid_1's auc: 0.891507\n",
      "[3100]\ttraining's auc: 0.96193\tvalid_1's auc: 0.891933\n",
      "[3200]\ttraining's auc: 0.963077\tvalid_1's auc: 0.892309\n",
      "[3300]\ttraining's auc: 0.96419\tvalid_1's auc: 0.892649\n",
      "[3400]\ttraining's auc: 0.965277\tvalid_1's auc: 0.893005\n",
      "[3500]\ttraining's auc: 0.96633\tvalid_1's auc: 0.893265\n",
      "[3600]\ttraining's auc: 0.967326\tvalid_1's auc: 0.893525\n",
      "[3700]\ttraining's auc: 0.968321\tvalid_1's auc: 0.893848\n",
      "[3800]\ttraining's auc: 0.969283\tvalid_1's auc: 0.8941\n",
      "[3900]\ttraining's auc: 0.970206\tvalid_1's auc: 0.894347\n",
      "[4000]\ttraining's auc: 0.971108\tvalid_1's auc: 0.894482\n",
      "[4100]\ttraining's auc: 0.972008\tvalid_1's auc: 0.894668\n",
      "[4200]\ttraining's auc: 0.972904\tvalid_1's auc: 0.894852\n",
      "[4300]\ttraining's auc: 0.973743\tvalid_1's auc: 0.895048\n",
      "[4400]\ttraining's auc: 0.974577\tvalid_1's auc: 0.895239\n",
      "[4500]\ttraining's auc: 0.975364\tvalid_1's auc: 0.895359\n",
      "[4600]\ttraining's auc: 0.976145\tvalid_1's auc: 0.895536\n",
      "[4700]\ttraining's auc: 0.976888\tvalid_1's auc: 0.895686\n",
      "[4800]\ttraining's auc: 0.97768\tvalid_1's auc: 0.895751\n",
      "[4900]\ttraining's auc: 0.97841\tvalid_1's auc: 0.895821\n",
      "[5000]\ttraining's auc: 0.979123\tvalid_1's auc: 0.895891\n",
      "[5100]\ttraining's auc: 0.979832\tvalid_1's auc: 0.895959\n",
      "[5200]\ttraining's auc: 0.980556\tvalid_1's auc: 0.89606\n",
      "[5300]\ttraining's auc: 0.981251\tvalid_1's auc: 0.896105\n",
      "[5400]\ttraining's auc: 0.981926\tvalid_1's auc: 0.896165\n",
      "[5500]\ttraining's auc: 0.982561\tvalid_1's auc: 0.89622\n",
      "[5600]\ttraining's auc: 0.983187\tvalid_1's auc: 0.89627\n",
      "[5700]\ttraining's auc: 0.983779\tvalid_1's auc: 0.896337\n",
      "[5800]\ttraining's auc: 0.984399\tvalid_1's auc: 0.896348\n",
      "[5900]\ttraining's auc: 0.984952\tvalid_1's auc: 0.896388\n",
      "[6000]\ttraining's auc: 0.985496\tvalid_1's auc: 0.896431\n",
      "[6100]\ttraining's auc: 0.986031\tvalid_1's auc: 0.896407\n",
      "[6200]\ttraining's auc: 0.986549\tvalid_1's auc: 0.896431\n",
      "[6300]\ttraining's auc: 0.987064\tvalid_1's auc: 0.896484\n",
      "[6400]\ttraining's auc: 0.98755\tvalid_1's auc: 0.896524\n",
      "[6500]\ttraining's auc: 0.988013\tvalid_1's auc: 0.896558\n",
      "[6600]\ttraining's auc: 0.988479\tvalid_1's auc: 0.896561\n",
      "[6700]\ttraining's auc: 0.988901\tvalid_1's auc: 0.896576\n",
      "[6800]\ttraining's auc: 0.989315\tvalid_1's auc: 0.896576\n",
      "[6900]\ttraining's auc: 0.98973\tvalid_1's auc: 0.896632\n",
      "[7000]\ttraining's auc: 0.99015\tvalid_1's auc: 0.896643\n",
      "[7100]\ttraining's auc: 0.990534\tvalid_1's auc: 0.89666\n",
      "[7200]\ttraining's auc: 0.990914\tvalid_1's auc: 0.896673\n",
      "[7300]\ttraining's auc: 0.991266\tvalid_1's auc: 0.896671\n",
      "[7400]\ttraining's auc: 0.991606\tvalid_1's auc: 0.896664\n",
      "Early stopping, best iteration is:\n",
      "[7250]\ttraining's auc: 0.991093\tvalid_1's auc: 0.896699\n",
      "current auc val is  0.8966989554101091\n",
      "fold n°4\n",
      "start regressor training...\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.788397\tvalid_1's auc: 0.759869\n",
      "[200]\ttraining's auc: 0.830116\tvalid_1's auc: 0.794364\n",
      "[300]\ttraining's auc: 0.852953\tvalid_1's auc: 0.812642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttraining's auc: 0.86938\tvalid_1's auc: 0.825511\n",
      "[500]\ttraining's auc: 0.881373\tvalid_1's auc: 0.834783\n",
      "[600]\ttraining's auc: 0.890761\tvalid_1's auc: 0.841946\n",
      "[700]\ttraining's auc: 0.898642\tvalid_1's auc: 0.847942\n",
      "[800]\ttraining's auc: 0.905328\tvalid_1's auc: 0.852711\n",
      "[900]\ttraining's auc: 0.911063\tvalid_1's auc: 0.856632\n",
      "[1000]\ttraining's auc: 0.915993\tvalid_1's auc: 0.860061\n",
      "[1100]\ttraining's auc: 0.920473\tvalid_1's auc: 0.863173\n",
      "[1200]\ttraining's auc: 0.924426\tvalid_1's auc: 0.865709\n",
      "[1300]\ttraining's auc: 0.927997\tvalid_1's auc: 0.868059\n",
      "[1400]\ttraining's auc: 0.931178\tvalid_1's auc: 0.87019\n",
      "[1500]\ttraining's auc: 0.934213\tvalid_1's auc: 0.872132\n",
      "[1600]\ttraining's auc: 0.936985\tvalid_1's auc: 0.873827\n",
      "[1700]\ttraining's auc: 0.939483\tvalid_1's auc: 0.875333\n",
      "[1800]\ttraining's auc: 0.941831\tvalid_1's auc: 0.876701\n",
      "[1900]\ttraining's auc: 0.943999\tvalid_1's auc: 0.877951\n",
      "[2000]\ttraining's auc: 0.946044\tvalid_1's auc: 0.878993\n",
      "[2100]\ttraining's auc: 0.947917\tvalid_1's auc: 0.880064\n",
      "[2200]\ttraining's auc: 0.949732\tvalid_1's auc: 0.881085\n",
      "[2300]\ttraining's auc: 0.951424\tvalid_1's auc: 0.881933\n",
      "[2400]\ttraining's auc: 0.952998\tvalid_1's auc: 0.882687\n",
      "[2500]\ttraining's auc: 0.954504\tvalid_1's auc: 0.883376\n",
      "[2600]\ttraining's auc: 0.955962\tvalid_1's auc: 0.884041\n",
      "[2700]\ttraining's auc: 0.957323\tvalid_1's auc: 0.884603\n",
      "[2800]\ttraining's auc: 0.958641\tvalid_1's auc: 0.885252\n",
      "[2900]\ttraining's auc: 0.959878\tvalid_1's auc: 0.885779\n",
      "[3000]\ttraining's auc: 0.961085\tvalid_1's auc: 0.886206\n",
      "[3100]\ttraining's auc: 0.962285\tvalid_1's auc: 0.886671\n",
      "[3200]\ttraining's auc: 0.963392\tvalid_1's auc: 0.887091\n",
      "[3300]\ttraining's auc: 0.964486\tvalid_1's auc: 0.887525\n",
      "[3400]\ttraining's auc: 0.965522\tvalid_1's auc: 0.887964\n",
      "[3500]\ttraining's auc: 0.966536\tvalid_1's auc: 0.888267\n",
      "[3600]\ttraining's auc: 0.967513\tvalid_1's auc: 0.888537\n",
      "[3700]\ttraining's auc: 0.968498\tvalid_1's auc: 0.888845\n",
      "[3800]\ttraining's auc: 0.969455\tvalid_1's auc: 0.889136\n",
      "[3900]\ttraining's auc: 0.970365\tvalid_1's auc: 0.889339\n",
      "[4000]\ttraining's auc: 0.971265\tvalid_1's auc: 0.889618\n",
      "[4100]\ttraining's auc: 0.97214\tvalid_1's auc: 0.889807\n",
      "[4200]\ttraining's auc: 0.972991\tvalid_1's auc: 0.890035\n",
      "[4300]\ttraining's auc: 0.973833\tvalid_1's auc: 0.890177\n",
      "[4400]\ttraining's auc: 0.974646\tvalid_1's auc: 0.890335\n",
      "[4500]\ttraining's auc: 0.97543\tvalid_1's auc: 0.890467\n",
      "[4600]\ttraining's auc: 0.976206\tvalid_1's auc: 0.890609\n",
      "[4700]\ttraining's auc: 0.976942\tvalid_1's auc: 0.890708\n",
      "[4800]\ttraining's auc: 0.977703\tvalid_1's auc: 0.890861\n",
      "[4900]\ttraining's auc: 0.978443\tvalid_1's auc: 0.890995\n",
      "[5000]\ttraining's auc: 0.979119\tvalid_1's auc: 0.891052\n",
      "[5100]\ttraining's auc: 0.979819\tvalid_1's auc: 0.891184\n",
      "[5200]\ttraining's auc: 0.980496\tvalid_1's auc: 0.891249\n",
      "[5300]\ttraining's auc: 0.98115\tvalid_1's auc: 0.891336\n",
      "[5400]\ttraining's auc: 0.981804\tvalid_1's auc: 0.891415\n",
      "[5500]\ttraining's auc: 0.982448\tvalid_1's auc: 0.89147\n",
      "[5600]\ttraining's auc: 0.983085\tvalid_1's auc: 0.891553\n",
      "[5700]\ttraining's auc: 0.983686\tvalid_1's auc: 0.891582\n",
      "[5800]\ttraining's auc: 0.984264\tvalid_1's auc: 0.89167\n",
      "[5900]\ttraining's auc: 0.98483\tvalid_1's auc: 0.891713\n",
      "[6000]\ttraining's auc: 0.985393\tvalid_1's auc: 0.891769\n",
      "[6100]\ttraining's auc: 0.985945\tvalid_1's auc: 0.891814\n",
      "[6200]\ttraining's auc: 0.986453\tvalid_1's auc: 0.891807\n",
      "[6300]\ttraining's auc: 0.987\tvalid_1's auc: 0.891828\n",
      "[6400]\ttraining's auc: 0.987476\tvalid_1's auc: 0.891837\n",
      "[6500]\ttraining's auc: 0.987946\tvalid_1's auc: 0.891864\n",
      "[6600]\ttraining's auc: 0.988382\tvalid_1's auc: 0.891885\n",
      "[6700]\ttraining's auc: 0.98882\tvalid_1's auc: 0.891899\n",
      "[6800]\ttraining's auc: 0.989254\tvalid_1's auc: 0.891913\n",
      "[6900]\ttraining's auc: 0.989673\tvalid_1's auc: 0.891938\n",
      "[7000]\ttraining's auc: 0.990089\tvalid_1's auc: 0.891999\n",
      "[7100]\ttraining's auc: 0.990487\tvalid_1's auc: 0.892016\n",
      "[7200]\ttraining's auc: 0.990858\tvalid_1's auc: 0.892038\n",
      "[7300]\ttraining's auc: 0.991213\tvalid_1's auc: 0.892023\n",
      "[7400]\ttraining's auc: 0.991553\tvalid_1's auc: 0.891991\n",
      "Early stopping, best iteration is:\n",
      "[7257]\ttraining's auc: 0.991062\tvalid_1's auc: 0.892052\n",
      "current auc val is  0.8920524000880529\n",
      "final rmse val 1 is  0.8962026125458811\n"
     ]
    }
   ],
   "source": [
    "param_regressor_1 = {\n",
    "         'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'binary',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.007,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'auc',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         'random_state': 2019\n",
    "}\n",
    "\n",
    "\n",
    "#folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "\n",
    "print('get here!!!')\n",
    "\n",
    "\n",
    "oof_regressor_full_1 = np.zeros(len(df_train))\n",
    "predictions_test = np.zeros(len(df_test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train_X.iloc[trn_idx], label=train_y.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train_X.iloc[val_idx], label=train_y.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "#     num_round = 200\n",
    "    print('start regressor training...')\n",
    "    clf = lgb.train(param_regressor_1, trn_data, num_round, valid_sets = [trn_data, val_data], \n",
    "                    verbose_eval=100, early_stopping_rounds = 200)\n",
    "    oof_regressor_full_1[val_idx] = clf.predict(train_X.iloc[val_idx], num_iteration=clf.best_iteration)    \n",
    "    predictions_test += clf.predict(test_X, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    auc_val = roc_auc_score(train_y.iloc[val_idx], oof_regressor_full_1[val_idx])\n",
    "    print('current auc val is ', auc_val)\n",
    "    \n",
    "print('final rmse val 1 is ', roc_auc_score(train_y, oof_regressor_full_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = pd.DataFrame()\n",
    "outcome['ID_code'] = test_id\n",
    "outcome['target'] = predictions_test\n",
    "outcome.to_csv(file_path + '/submission/outcome_8962.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
