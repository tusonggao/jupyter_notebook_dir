{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://www.kaggle.com/chauhuynh/my-first-kernel-3-699\n",
    "\n",
    "http://socsci2.ucsd.edu/~aronatas/project/academic/A%20survey%20of%20credit%20and%20behavioural%20scoring%20Forecasting%20fina.pdf\n",
    "\n",
    "https://pdfs.semanticscholar.org/1744/22f763ff53cf5e9feeb5f42b0c29eb17ed84.pdf\n",
    "\n",
    "https://www.creditcards.com/credit-card-news/behavior-scores-impact-credit.php\n",
    "\n",
    "http://www.statsoft.com/textbook/credit-scoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6df55b542f152882e00385a0f73198f4e3bc4316"
   },
   "source": [
    "**FEEL FREE TO UPVOTE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(4590)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'F:/github_me_repos/Kaggle_code/elo-merchant-category-recommendation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(file_path + 'data/train.csv')\n",
    "df_test = pd.read_csv(file_path + 'data/test.csv')\n",
    "df_hist_trans = pd.read_csv(file_path + 'data/historical_transactions.csv')\n",
    "df_new_merchant_trans = pd.read_csv(file_path + 'data/new_merchant_transactions.csv')\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "71f89a3b8a93b2f2feb2cd0a45f860cde33687be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n"
     ]
    }
   ],
   "source": [
    "for df in [df_hist_trans,df_new_merchant_trans]:\n",
    "    df['category_2'].fillna(1.0,inplace=True)\n",
    "    df['category_3'].fillna('A',inplace=True)\n",
    "    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    \n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dda90662d05e22310dd713df106ea07f4b8bccfc"
   },
   "outputs": [],
   "source": [
    "def get_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "690ba01a38f524e9345b419200f588f937bc067a"
   },
   "outputs": [],
   "source": [
    "for df in [df_hist_trans, df_new_merchant_trans]:\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['year'] = df['purchase_date'].dt.year\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
    "    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n",
    "    df['category_1'] = df['category_1'].map({'Y':1, 'N':0})\n",
    "    #https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/73244\n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "\n",
    "\n",
    "df_hist_trans = reduce_mem_usage(df_hist_trans)\n",
    "df_new_merchant_trans = reduce_mem_usage(df_new_merchant_trans)\n",
    "    \n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddf1d5bb0ade2b22b0f072c208c1506ea64503ea"
   },
   "outputs": [],
   "source": [
    "aggs = {}\n",
    "for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
    "    aggs[col] = ['nunique']\n",
    "\n",
    "#---------- added by tsg ---------\n",
    "aggs['city_id'] = ['nunique']\n",
    "aggs['state_id'] = ['nunique']\n",
    "#---------------------------------\n",
    "\n",
    "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "aggs['installments'] = ['sum','max','min','mean','var']\n",
    "aggs['purchase_date'] = ['max','min']\n",
    "aggs['month_lag'] = ['max','min','mean','var']\n",
    "aggs['month_diff'] = ['mean']\n",
    "aggs['authorized_flag'] = ['sum', 'mean']\n",
    "aggs['weekend'] = ['sum', 'mean']\n",
    "aggs['category_1'] = ['sum', 'mean']\n",
    "aggs['card_id'] = ['size']\n",
    "\n",
    "for col in ['category_2','category_3']:\n",
    "    df_hist_trans[col+'_mean'] = df_hist_trans.groupby([col])['purchase_amount'].transform('mean')\n",
    "    aggs[col+'_mean'] = ['mean']\n",
    "    \n",
    "df_hist_trans = reduce_mem_usage(df_hist_trans)\n",
    "\n",
    "new_columns = get_new_columns('hist',aggs)\n",
    "df_hist_trans_group = df_hist_trans.groupby('card_id').agg(aggs)\n",
    "df_hist_trans_group.columns = new_columns\n",
    "df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
    "df_hist_trans_group['hist_purchase_date_diff'] = (df_hist_trans_group['hist_purchase_date_max'] - df_hist_trans_group['hist_purchase_date_min']).dt.days\n",
    "df_hist_trans_group['hist_purchase_date_average'] = df_hist_trans_group['hist_purchase_date_diff']/df_hist_trans_group['hist_card_id_size']\n",
    "df_hist_trans_group['hist_purchase_date_uptonow'] = (datetime.datetime.today() - df_hist_trans_group['hist_purchase_date_max']).dt.days\n",
    "df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n",
    "df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n",
    "del df_hist_trans_group;gc.collect()\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7f5625db40db4395374991124fb796c9decd60b"
   },
   "outputs": [],
   "source": [
    "aggs = {}\n",
    "for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
    "    aggs[col] = ['nunique']\n",
    "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "aggs['installments'] = ['sum','max','min','mean','var']\n",
    "aggs['purchase_date'] = ['max','min']\n",
    "aggs['month_lag'] = ['max','min','mean','var']\n",
    "aggs['month_diff'] = ['mean']\n",
    "aggs['weekend'] = ['sum', 'mean']\n",
    "aggs['category_1'] = ['sum', 'mean']\n",
    "aggs['card_id'] = ['size']\n",
    "\n",
    "for col in ['category_2','category_3']:\n",
    "    df_new_merchant_trans[col+'_mean'] = df_new_merchant_trans.groupby([col])['purchase_amount'].transform('mean')\n",
    "    aggs[col+'_mean'] = ['mean']\n",
    "    \n",
    "new_columns = get_new_columns('new_hist',aggs)\n",
    "df_hist_trans_group = df_new_merchant_trans.groupby('card_id').agg(aggs)\n",
    "df_hist_trans_group.columns = new_columns\n",
    "df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
    "df_hist_trans_group['new_hist_purchase_date_diff'] = (df_hist_trans_group['new_hist_purchase_date_max'] - df_hist_trans_group['new_hist_purchase_date_min']).dt.days\n",
    "df_hist_trans_group['new_hist_purchase_date_average'] = df_hist_trans_group['new_hist_purchase_date_diff']/df_hist_trans_group['new_hist_card_id_size']\n",
    "df_hist_trans_group['new_hist_purchase_date_uptonow'] = (datetime.datetime.today() - df_hist_trans_group['new_hist_purchase_date_max']).dt.days\n",
    "df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n",
    "df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n",
    "del df_hist_trans_group;\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features added by tsg\n",
    "# 最近三个月 最近半年 最近一年的 购买次数 购买金额 F M\n",
    "# 购买的星期几占比ratio\n",
    "# 购买的 Morning Afternoon Evening Night占比\n",
    "print('shape is ', df_hist_trans.shape, df_new_merchant_trans.shape)\n",
    "\n",
    "aggs = {\n",
    "    'month_lag': ['size'],\n",
    "#     'purchase_amount': ['sum', 'max', 'min', 'mean', 'var']\n",
    "    'purchase_amount': ['sum', 'max', 'min', 'mean', 'var']\n",
    "}\n",
    "\n",
    "df_trans_month_lag = pd.concat([df_hist_trans[['card_id', 'month_lag', 'purchase_amount']], \n",
    "                                df_new_merchant_trans[['card_id', 'month_lag', 'purchase_amount']]])\n",
    "\n",
    "print('after concat df_trans_month_lag.shape is ', df_trans_month_lag.shape)\n",
    "\n",
    "df_3_month_lag = (df_trans_month_lag[df_trans_month_lag['month_lag']<=3].\n",
    "                   groupby('card_id').agg(aggs))\n",
    "df_3_month_lag.columns = ['3_month_' + '_'.join(col).strip() for col in df_3_month_lag.columns.values]\n",
    "df_3_month_lag.reset_index(inplace=True)\n",
    "print('df_3_month_lag.shape is', df_3_month_lag.shape)\n",
    "\n",
    "df_6_month_lag = (df_trans_month_lag[df_trans_month_lag['month_lag']<=6].\n",
    "                   groupby('card_id').agg(aggs))\n",
    "df_6_month_lag.columns = ['6_month_' + '_'.join(col).strip() for col in df_6_month_lag.columns.values]\n",
    "df_6_month_lag.reset_index(inplace=True)\n",
    "print('df_6_month_lag.shape is', df_6_month_lag.shape)\n",
    "\n",
    "df_12_month_lag = (df_trans_month_lag[df_trans_month_lag['month_lag']<=12].\n",
    "                   groupby('card_id').agg(aggs))\n",
    "df_12_month_lag.columns = ['12_month_' + '_'.join(col).strip() for col in df_12_month_lag.columns.values]\n",
    "df_12_month_lag.reset_index(inplace=True)\n",
    "print('df_12_month_lag.shape is', df_12_month_lag.shape)\n",
    "\n",
    "print('df_train.columns is', df_train.columns)\n",
    "print('df_test.columns is', df_test.columns)\n",
    "\n",
    "df_train = df_train.merge(df_3_month_lag,on='card_id',how='left')\n",
    "df_test = df_test.merge(df_3_month_lag,on='card_id',how='left')\n",
    "\n",
    "df_train = df_train.merge(df_6_month_lag,on='card_id',how='left')\n",
    "df_test = df_test.merge(df_6_month_lag,on='card_id',how='left')\n",
    "\n",
    "df_train = df_train.merge(df_12_month_lag, on='card_id',how='left')\n",
    "df_test = df_test.merge(df_12_month_lag, on='card_id',how='left')\n",
    "\n",
    "print('df_train.columns is', df_train.columns)\n",
    "print('df_test.columns is', df_test.columns)\n",
    "\n",
    "del df_trans_month_lag, df_3_month_lag, df_6_month_lag, df_12_month_lag;\n",
    "gc.collect()\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a075cc90ab1322829e4fad3ff39fce307c5db93c"
   },
   "outputs": [],
   "source": [
    "del df_hist_trans;gc.collect()\n",
    "del df_new_merchant_trans;gc.collect()\n",
    "df_train.head(5)\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f3182aeac0c3bf7a061a1b9e25e859f25fee9b5"
   },
   "outputs": [],
   "source": [
    "df_train['outliers'] = 0\n",
    "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "df_train['outliers'].value_counts()\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce2082fc1fb0e3f8f7d27fc166aa7a8351b65504"
   },
   "outputs": [],
   "source": [
    "for df in [df_train,df_test]:\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n",
    "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
    "    df['month'] = df['first_active_month'].dt.month\n",
    "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n",
    "                     'new_hist_purchase_date_min']:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n",
    "    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
    "\n",
    "for f in ['feature_1','feature_2','feature_3']:\n",
    "    order_label = df_train.groupby([f])['outliers'].mean()\n",
    "    df_train[f] = df_train[f].map(order_label)\n",
    "    df_test[f] = df_test[f].map(order_label)\n",
    "    \n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4f20f27679889542acfd60d1f1ac381b201ac43"
   },
   "outputs": [],
   "source": [
    "df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n",
    "target = df_train['target']\n",
    "del df_train['target']\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9bbc95244978b519d94131907b547c2b6c94191"
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE_TSG = 10007\n",
    "# RANDOM_STATE_TSG = 42\n",
    "\n",
    "param = {'num_leaves': 31, \n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression', \n",
    "         'max_depth': -1, \n",
    "         'learning_rate': 0.01, \n",
    "         \"min_child_samples\": 20, \n",
    "         \"boosting\": \"gbdt\", \n",
    "         \"feature_fraction\": 0.9, \n",
    "         \"bagging_freq\": 1, \n",
    "         \"bagging_fraction\": 0.9,\n",
    "         \"bagging_seed\": 11, \n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1, \n",
    "         \"verbosity\": -1, \n",
    "         \"nthread\": 4, \n",
    "         \"random_state\": RANDOM_STATE_TSG\n",
    "}\n",
    "\n",
    "# final rmse is  3.6531506032733994\n",
    "# final rmse is  3.652673988138324\n",
    "\n",
    "# final rmse is  3.650857717994587\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "# param = {'num_leaves': 31,\n",
    "#          'min_data_in_leaf': 30, \n",
    "#          'objective':'regression',\n",
    "#          'max_depth': -1,\n",
    "#          'learning_rate': 0.007,\n",
    "#          \"min_child_samples\": 20,\n",
    "#          \"boosting\": \"gbdt\",\n",
    "#          \"feature_fraction\": 0.9,\n",
    "#          \"bagging_freq\": 1,\n",
    "#          \"bagging_fraction\": 0.9 ,\n",
    "#          \"bagging_seed\": 11,\n",
    "#          \"metric\": 'rmse',\n",
    "#          \"lambda_l1\": 0.1,\n",
    "#          \"verbosity\": -1,\n",
    "#          \"nthread\": 4,\n",
    "#          \"random_state\": 2019}\n",
    "# final rmse is  3.653225305416142\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "# param = {'num_leaves': 23,\n",
    "#          'min_data_in_leaf': 31, \n",
    "#          'objective':'regression',\n",
    "#          'max_depth': -1,\n",
    "#          'learning_rate': 0.005,\n",
    "#          \"boosting\": \"gbdt\",\n",
    "#          \"feature_fraction\": 0.9,\n",
    "#          \"bagging_freq\": 1,\n",
    "#          \"bagging_fraction\": 0.9 ,\n",
    "#          \"bagging_seed\": 11,\n",
    "#          \"metric\": 'rmse',\n",
    "#          \"lambda_l1\": 0.21,\n",
    "#          \"verbosity\": -1,\n",
    "#          \"nthread\": 4,\n",
    "#          \"random_state\": 2019}\n",
    "# final rmse is  3.6525831052105073\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "# param = {'num_leaves': 37,\n",
    "#          'min_data_in_leaf': 31, \n",
    "#          'objective':'regression',\n",
    "#          'max_depth': -1,\n",
    "#          'learning_rate': 0.01,\n",
    "#          \"boosting\": \"dart\",\n",
    "#          \"feature_fraction\": 0.9,\n",
    "#          \"bagging_freq\": 1,\n",
    "#          \"bagging_fraction\": 0.9 ,\n",
    "#          \"bagging_seed\": 11,\n",
    "#          \"metric\": 'rmse',\n",
    "#          \"lambda_l1\": 0.21,\n",
    "#          \"verbosity\": -1,\n",
    "#          \"nthread\": 4,\n",
    "#          \"random_state\": 2019}\n",
    "# final rmse is  3.6582834308270673\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE_TSG)\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train,df_train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][df_train_columns], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][df_train_columns], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=200)\n",
    "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][df_train_columns], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = df_train_columns\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(df_test[df_train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "final_rmse = np.sqrt(mean_squared_error(oof, target))\n",
    "print('final rmse is ', final_rmse)\n",
    "\n",
    "sub_df = pd.DataFrame({\"card_id\": df_test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = predictions\n",
    "sub_df.to_csv(file_path + '/submission/submission_first_kernel.csv', index=False)\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40b64481054fa71e692829c7039eccceb31b77fe"
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"Feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "355e9c24949b8e5d677fe5a2f117228c3310dab6"
   },
   "outputs": [],
   "source": [
    "# sub_df = pd.DataFrame({\"card_id\": df_test[\"card_id\"].values})\n",
    "# sub_df[\"target\"] = predictions\n",
    "# sub_df.to_csv(file_path + '/submission/submission_first_kernel_dart.csv', index=False)\n",
    "              \n",
    "# print('finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58c9a5445698e42dfbd9548695290487a2ce171a"
   },
   "source": [
    "**To be continued ...**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
