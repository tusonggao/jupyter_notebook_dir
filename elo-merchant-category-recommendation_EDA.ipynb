{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相关链接\n",
    "\n",
    "**[Elo Merchant Category Recommendation](https://www.kaggle.com/c/elo-merchant-category-recommendation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from plotly import tools\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from dateutil import relativedelta\n",
    "from datetime import datetime\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'F:/git_repos/Kaggle_code/elo-merchant-category-recommendation/'\n",
    "# submission_path = 'F:/git_repos/Kaggle_code/elo-merchant-category-recommendation/submission/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns in train set :  (201917, 6)\n",
      "Number of rows and columns in test set :  (123623, 5)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(file_path + '/data/train.csv', parse_dates=['first_active_month'])\n",
    "test_df = pd.read_csv(file_path + '/data/test.csv', parse_dates=['first_active_month'])\n",
    "print(\"Number of rows and columns in train set : \",train_df.shape)\n",
    "print(\"Number of rows and columns in test set : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.read_csv(file_path + '/data/historical_transactions.csv', parse_dates=['purchase_date'])\n",
    "new_trans_df = pd.read_csv(file_path + '/data/new_merchant_transactions.csv', parse_dates=['purchase_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Y    26595452\n",
       "N     2516909\n",
       "Name: authorized_flag, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_df['authorized_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Y    1963031\n",
       "Name: authorized_flag, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_trans_df['authorized_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目标列探索\n",
    "\n",
    "通过图形可视化展现目标值的分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'target'\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(train_df.shape[0]), np.sort(train_df[target_col].values))\n",
    "plt.title(\"value distrubition of Loyalty score\")\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('Loyalty Score', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.distplot(train_df[target_col].values, bins=50, kde=False, color=\"red\")\n",
    "plt.title(\"Histogram of Loyalty score\")\n",
    "plt.xlabel('Loyalty score', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['target'].max(), train_df['target'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_df[target_col]<-30).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_srs = train_df['first_active_month'].dt.date.value_counts()\n",
    "cnt_srs = cnt_srs.sort_index()\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('First active month', fontsize=12)\n",
    "plt.ylabel('Number of cards', fontsize=12)\n",
    "plt.title(\"First active month count in train set\")\n",
    "plt.show()\n",
    "\n",
    "cnt_srs = test_df['first_active_month'].dt.date.value_counts()\n",
    "cnt_srs = cnt_srs.sort_index()\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('First active month', fontsize=12)\n",
    "plt.ylabel('Number of cards', fontsize=12)\n",
    "plt.title(\"First active month count in test set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature 1\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.violinplot(x=\"feature_1\", y=target_col, data=train_df)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Loyalty score', fontsize=12)\n",
    "plt.title(\"Feature 1 distribution\")\n",
    "# plt.show()\n",
    "\n",
    "# feature 2\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.violinplot(x=\"feature_2\", y=target_col, data=train_df)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('Feature 2', fontsize=12)\n",
    "plt.ylabel('Loyalty score', fontsize=12)\n",
    "plt.title(\"Feature 2 distribution\")\n",
    "# plt.show()\n",
    "\n",
    "# feature 3\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.violinplot(x=\"feature_3\", y=target_col, data=train_df, palette=\"Set1\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('Feature 3', fontsize=12)\n",
    "plt.ylabel('Loyalty score', fontsize=12)\n",
    "plt.title(\"Feature 3 distribution\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 历史交易数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.read_csv(data_path + 'historical_transactions.csv', parse_dates=['purchase_date'])\n",
    "hist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df['merchant_category_id'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = hist_df.groupby(\"card_id\")\n",
    "gdf = gdf[\"purchase_amount\"].size().reset_index()\n",
    "gdf.columns = [\"card_id\", \"num_hist_transactions\"]\n",
    "train_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_srs = train_df.groupby(\"num_hist_transactions\")[target_col].mean()\n",
    "cnt_srs = cnt_srs.sort_index()\n",
    "cnt_srs = cnt_srs[:-50]\n",
    "\n",
    "def scatter_plot(cnt_srs, color):\n",
    "    trace = go.Scatter(\n",
    "        x=cnt_srs.index[::-1],\n",
    "        y=cnt_srs.values[::-1],\n",
    "        showlegend=False,\n",
    "        marker=dict(\n",
    "            color=color,\n",
    "        ),\n",
    "    )\n",
    "    return trace\n",
    "\n",
    "trace = scatter_plot(cnt_srs, \"orange\")\n",
    "layout = dict(\n",
    "    title='Loyalty score by Number of historical transactions',\n",
    "    )\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=\"Histtranscnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 10, 20, 30, 40, 50, 75, 100, 150, 200, 500, 10000]\n",
    "train_df['binned_num_hist_transactions'] = pd.cut(train_df['num_hist_transactions'], bins)\n",
    "cnt_srs = train_df.groupby(\"binned_num_hist_transactions\")[target_col].mean()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(x=\"binned_num_hist_transactions\", y=target_col, data=train_df, showfliers=False)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('binned_num_hist_transactions', fontsize=12)\n",
    "plt.ylabel('Loyalty score', fontsize=12)\n",
    "plt.title(\"binned_num_hist_transactions distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = hist_df.groupby(\"card_id\")\n",
    "gdf = gdf[\"purchase_amount\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\n",
    "gdf.columns = [\"card_id\", \"sum_hist_trans\", \"mean_hist_trans\", \"std_hist_trans\", \"min_hist_trans\", \"max_hist_trans\"]\n",
    "train_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.percentile(train_df[\"sum_hist_trans\"], range(0,101,10))\n",
    "train_df['binned_sum_hist_trans'] = pd.cut(train_df['sum_hist_trans'], bins)\n",
    "#cnt_srs = train_df.groupby(\"binned_sum_hist_trans\")[target_col].mean()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(x=\"binned_sum_hist_trans\", y=target_col, data=train_df, showfliers=False)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('binned_sum_hist_trans', fontsize=12)\n",
    "plt.ylabel('Loyalty score', fontsize=12)\n",
    "plt.title(\"Sum of historical transaction value (Binned) distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.percentile(train_df[\"mean_hist_trans\"], range(0,101,10))\n",
    "train_df['binned_mean_hist_trans'] = pd.cut(train_df['mean_hist_trans'], bins)\n",
    "#cnt_srs = train_df.groupby(\"binned_mean_hist_trans\")[target_col].mean()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(x=\"binned_mean_hist_trans\", y=target_col, data=train_df, showfliers=False)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('Binned Mean Historical Transactions', fontsize=12)\n",
    "plt.ylabel('Loyalty score', fontsize=12)\n",
    "plt.title(\"Mean of historical transaction value (Binned) distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trans_df = pd.read_csv(data_path + 'new_merchant_transactions.csv', parse_dates=['purchase_date'])\n",
    "new_trans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = new_trans_df.groupby(\"card_id\")\n",
    "gdf = gdf[\"purchase_amount\"].size().reset_index()\n",
    "gdf.columns = [\"card_id\", \"num_merch_transactions\"]\n",
    "train_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 10, 20, 30, 40, 50, 75, 10000]\n",
    "train_df['binned_num_merch_transactions'] = pd.cut(train_df['num_merch_transactions'], bins)\n",
    "cnt_srs = train_df.groupby(\"binned_num_merch_transactions\")[target_col].mean()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(x=\"binned_num_merch_transactions\", y=target_col, data=train_df, showfliers=False)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('binned_num_merch_transactions', fontsize=12)\n",
    "plt.ylabel('Loyalty score', fontsize=12)\n",
    "plt.title(\"Number of new merchants transaction (Binned) distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = new_trans_df.groupby(\"card_id\")\n",
    "gdf = gdf[\"purchase_amount\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\n",
    "gdf.columns = [\"card_id\", \"sum_merch_trans\", \"mean_merch_trans\", \"std_merch_trans\", \"min_merch_trans\", \"max_merch_trans\"]\n",
    "train_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.nanpercentile(train_df[\"sum_merch_trans\"], range(0,101,10))\n",
    "train_df['binned_sum_merch_trans'] = pd.cut(train_df['sum_merch_trans'], bins)\n",
    "#cnt_srs = train_df.groupby(\"binned_sum_hist_trans\")[target_col].mean()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(x=\"binned_sum_merch_trans\", y=target_col, data=train_df, showfliers=False)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('binned sum of new merchant transactions', fontsize=12)\n",
    "plt.ylabel('Loyalty score', fontsize=12)\n",
    "plt.title(\"Sum of New merchants transaction value (Binned) distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加feature: 持卡人在多少个城市消费过\n",
    "\n",
    "train_df.drop(columns=['num_hist_distinct_city', 'num_new_distinct_city'], \n",
    "              inplace=True, errors='ignore')\n",
    "test_df.drop(columns=['num_hist_distinct_city', 'num_new_distinct_city'],\n",
    "             inplace=True, errors='ignore')\n",
    "\n",
    "gdf = hist_df.groupby(\"card_id\")\n",
    "gdf = gdf[\"city_id\"].apply(lambda x: len(x.unique())).reset_index()\n",
    "gdf.columns = [\"card_id\", \"num_hist_distinct_city\"]\n",
    "train_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")\n",
    "\n",
    "gdf = new_trans_df.groupby(\"card_id\")\n",
    "gdf = gdf[\"city_id\"].apply(lambda x: len(x.unique())).reset_index()\n",
    "gdf.columns = [\"card_id\", \"num_new_distinct_city\"]\n",
    "train_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")\n",
    "print('train_df.shape is ', train_df.shape)\n",
    "\n",
    "###########################################################################################\n",
    "# 增加feature: 持卡人在多少个不同商家消费过\n",
    "\n",
    "train_df.drop(columns=['num_hist_distinct_merchant', 'num_new_distinct_merchant'], \n",
    "              inplace=True, errors='ignore')\n",
    "test_df.drop(columns=['num_hist_distinct_merchant', 'num_new_distinct_merchant'], \n",
    "             inplace=True, errors='ignore')\n",
    "print('train_df.shape is ', train_df.shape)\n",
    "\n",
    "gdf = hist_df.groupby(\"card_id\")\n",
    "gdf = gdf[\"merchant_id\"].apply(lambda x: len(x.unique())).reset_index()\n",
    "gdf.columns = [\"card_id\", \"num_hist_distinct_merchant\"]\n",
    "train_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")\n",
    "\n",
    "gdf = new_trans_df.groupby(\"card_id\")\n",
    "gdf = gdf[\"city_id\"].apply(lambda x: len(x.unique())).reset_index()\n",
    "gdf.columns = [\"card_id\", \"num_new_distinct_merchant\"]\n",
    "train_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")\n",
    "\n",
    "print('train_df.shape is ', train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加feature: 持卡人用卡已经有多少天（截止到2018年5月1日）\n",
    "\n",
    "#  '2018-04-30 23:59:59'\n",
    "end_dt = datetime.strptime(str('2018-05-01 00:00:00'), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def diff_days(dt):\n",
    "    global end_dt\n",
    "    return (end_dt - dt).days\n",
    "\n",
    "train_df[\"active_days\"] = train_df[\"first_active_month\"].apply(diff_days)\n",
    "test_df[\"active_days\"] = test_df[\"first_active_month\"].apply(diff_days)\n",
    "\n",
    "print('train_df.shape is ', train_df.shape)\n",
    "print(train_df[['active_days', 'first_active_month']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO Do:\n",
    "# 1. R 最近一次购买距离当前多少天\n",
    "# 2. F 最近半年购买次数；总的购买次数\n",
    "# 3. M 最近半年购买金额；总的购买次数\n",
    "# 4. 总的购买的频率：购买次数/活跃月份\n",
    "\n",
    "###########################################################################################\n",
    "# 增加feature: 持卡人最近一次消费距离当前的天数（截止到2018年5月1日）\n",
    "\n",
    "# hist_df['purchase_date'] =  pd.to_datetime(hist_df['purchase_date'], format='%Y-%m-%d %H:%M:%S')\n",
    "# new_trans_df['purchase_date'] =  pd.to_datetime(new_trans_df['purchase_date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "merged_purchase_date_df = pd.concat([new_trans_df[['card_id', 'purchase_date']],\n",
    "    hist_df[['card_id', 'purchase_date']]])\n",
    "\n",
    "\n",
    "# print('new_trans_df.shape is ', new_trans_df.shape)\n",
    "# print('hist_df.shape is ', hist_df.shape)\n",
    "# print('merged_purchase_date_df.shape is ', merged_purchase_date_df.shape)\n",
    "\n",
    "# merged_purchase_date_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# print('after drop duplicates merged_purchase_date_df.shape is ', \n",
    "#       merged_purchase_date_df.shape)\n",
    "\n",
    "train_df.drop(columns=['max_purchase_date', 'recency'], inplace=True, errors='ignore')\n",
    "test_df.drop(columns=['max_purchase_date', 'recency'], inplace=True, errors='ignore')\n",
    "\n",
    "gdf = merged_purchase_date_df.groupby(\"card_id\")\n",
    "gdf = gdf[\"purchase_date\"].apply(lambda x: x.max()).reset_index()\n",
    "gdf.columns = [\"card_id\", \"max_purchase_date\"]\n",
    "train_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")\n",
    "train_df['recency'] = train_df['max_purchase_date'].apply(diff_days)\n",
    "test_df['recency'] = test_df['max_purchase_date'].apply(diff_days)\n",
    "\n",
    "print('train_df.shape is ', train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# 增加feature: 持卡人最近三个月、最近半年、最近一年的消费金额（截止到2018年5月1日）\n",
    "\n",
    "# merged_purchase_date_df = pd.concat([new_trans_df[['card_id', 'purchase_amount', 'purchase_date']],\n",
    "#     hist_df[['card_id', 'purchase_amount', 'purchase_date']]])\n",
    "\n",
    "# print('merged_purchase_date_df.shape is ', merged_purchase_date_df.shape)\n",
    "\n",
    "\n",
    "# fig,ax=plt.subplots()\n",
    "\n",
    "# ax.hist(merged_purchase_date_df['purchase_amount'].values, bins=50, \n",
    "#         histtype=\"stepfilled\",alpha=0.6,label=\"robbery\")\n",
    "\n",
    "# print(merged_purchase_date_df['purchase_amount'].min(), merged_purchase_date_df['purchase_amount'].max())\n",
    "\n",
    "print(merged_purchase_date_df.shape)\n",
    "print((merged_purchase_date_df['purchase_amount']>=10**5).sum())\n",
    "print((merged_purchase_date_df['purchase_amount']>=0.0).sum())\n",
    "print((merged_purchase_date_df['purchase_amount']<0.0).sum())\n",
    "print((merged_purchase_date_df['purchase_amount']>-1.0).sum())\n",
    "print((merged_purchase_date_df['purchase_amount']>-0.988).sum())\n",
    "print('median is ', merged_purchase_date_df['purchase_amount'].median())\n",
    "print('min max is', merged_purchase_date_df['purchase_amount'].min(),\n",
    "     merged_purchase_date_df['purchase_amount'].max())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "# sns.distplot(merged_purchase_date_df[(merged_purchase_date_df['purchase_amount']>=0.0) & \n",
    "#              (merged_purchase_date_df['purchase_amount']<=20.0)]['purchase_amount'].values, \n",
    "#              bins=50, kde=False, color=\"red\")\n",
    "sns.distplot(merged_purchase_date_df[merged_purchase_date_df['purchase_amount']<=20.0]['purchase_amount'].values, \n",
    "             bins=50, kde=False, color=\"red\")\n",
    "plt.title(\"Histogram of Loyalty score\")\n",
    "plt.xlabel('Loyalty score', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# fig,ax=plt.subplots()\n",
    "# ax.hist(merged_purchase_date_df[merged_purchase_date_df['purchase_amount']<=10]['purchase_amount'].values, \n",
    "#         bins=50, histtype=\"stepfilled\",alpha=0.6,label=\"robbery\")\n",
    "\n",
    "\n",
    "# print('new_trans_df.shape is ', new_trans_df.shape)\n",
    "# print('hist_df.shape is ', hist_df.shape)\n",
    "# print('merged_purchase_date_df.shape is ', merged_purchase_date_df.shape)\n",
    "\n",
    "# merged_purchase_date_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# print('after drop duplicates merged_purchase_date_df.shape is ', \n",
    "#       merged_purchase_date_df.shape)\n",
    "\n",
    "# train_df.drop(columns=['max_purchase_date', 'recency'], inplace=True, errors='ignore')\n",
    "# test_df.drop(columns=['max_purchase_date', 'recency'], inplace=True, errors='ignore')\n",
    "\n",
    "# gdf = merged_purchase_date_df.groupby(\"card_id\")\n",
    "# gdf = gdf[\"purchase_date\"].apply(lambda x: x.max()).reset_index()\n",
    "# gdf.columns = [\"card_id\", \"max_purchase_date\"]\n",
    "# train_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\n",
    "# test_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")\n",
    "# train_df['recency'] = train_df['max_purchase_date'].apply(diff_days)\n",
    "# test_df['recency'] = test_df['max_purchase_date'].apply(diff_days)\n",
    "\n",
    "# print('train_df.shape is ', train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# 增加feature: 持卡人最近三个月、最近半年、最近一年的消费次数（截止到2018年5月1日）\n",
    "\n",
    "merged_purchase_date_df = pd.concat([new_trans_df[['card_id', 'purchase_date']],\n",
    "    hist_df[['card_id', 'purchase_date']]])\n",
    "\n",
    "# print('new_trans_df.shape is ', new_trans_df.shape)\n",
    "# print('hist_df.shape is ', hist_df.shape)\n",
    "# print('merged_purchase_date_df.shape is ', merged_purchase_date_df.shape)\n",
    "\n",
    "# merged_purchase_date_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# print('after drop duplicates merged_purchase_date_df.shape is ', \n",
    "#       merged_purchase_date_df.shape)\n",
    "\n",
    "train_df.drop(columns=['max_purchase_date', 'recency'], inplace=True, errors='ignore')\n",
    "test_df.drop(columns=['max_purchase_date', 'recency'], inplace=True, errors='ignore')\n",
    "\n",
    "gdf = merged_purchase_date_df.groupby(\"card_id\")\n",
    "gdf = gdf[\"purchase_date\"].apply(lambda x: x.max()).reset_index()\n",
    "gdf.columns = [\"card_id\", \"max_purchase_date\"]\n",
    "train_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")\n",
    "train_df['recency'] = train_df['max_purchase_date'].apply(diff_days)\n",
    "test_df['recency'] = test_df['max_purchase_date'].apply(diff_days)\n",
    "\n",
    "print('train_df.shape is ', train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果证明上面的特征中，R的作用很大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_trans_df.month_lag.value_counts().sort_index()\n",
    "# hist_df.month_lag.value_counts()\n",
    "###########################################################################################\n",
    "# 增加feature: 处理month_lag特征\n",
    "\n",
    "features_to_contruct = ['sum_month_lag', 'mean_month_lag', \n",
    "                        'std_month_lag', 'min_month_lag', 'max_month_lag']\n",
    "\n",
    "train_df.drop(columns=features_to_contruct, inplace=True, errors='ignore')\n",
    "test_df.drop(columns=features_to_contruct, inplace=True, errors='ignore')\n",
    "\n",
    "gdf = new_trans_df.groupby(\"card_id\")\n",
    "gdf = gdf[\"month_lag\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\n",
    "gdf.columns = ['card_id', 'sum_month_lag', 'mean_month_lag', 'std_month_lag', 'min_month_lag', 'max_month_lag']\n",
    "train_df = pd.merge(train_df, gdf, on='card_id', how='left')\n",
    "test_df = pd.merge(test_df, gdf, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_trans_df.installments.value_counts().sort_index()\n",
    "# hist_df.installments.value_counts().sort_index()\n",
    "###########################################################################################\n",
    "# 增加feature: 处理installments特征， todo: -1和999都设置为NA\n",
    "\n",
    "features_to_contruct = ['sum_installments', 'mean_installments', \n",
    "                        'std_installments', 'min_installments', 'max_installments']\n",
    "\n",
    "train_df.drop(columns=features_to_contruct, inplace=True, errors='ignore')\n",
    "test_df.drop(columns=features_to_contruct, inplace=True, errors='ignore')\n",
    "\n",
    "gdf = new_trans_df.groupby(\"card_id\")\n",
    "gdf = gdf[\"installments\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\n",
    "gdf.columns = ['card_id', 'sum_installments', 'mean_installments', 'std_installments', 'min_installments', 'max_installments']\n",
    "train_df = pd.merge(train_df, gdf, on='card_id', how='left')\n",
    "test_df = pd.merge(test_df, gdf, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"year\"] = train_df[\"first_active_month\"].dt.year\n",
    "test_df[\"year\"] = test_df[\"first_active_month\"].dt.year\n",
    "train_df[\"month\"] = train_df[\"first_active_month\"].dt.month\n",
    "test_df[\"month\"] = test_df[\"first_active_month\"].dt.month\n",
    "\n",
    "cols_to_use = [\"feature_1\", \"feature_2\", \"feature_3\", \"year\", \"month\", \n",
    "               \"num_hist_transactions\", \"sum_hist_trans\", \"mean_hist_trans\", \"std_hist_trans\", \n",
    "               \"min_hist_trans\", \"max_hist_trans\",\n",
    "               \"num_merch_transactions\", \"sum_merch_trans\", \"mean_merch_trans\", \"std_merch_trans\",\n",
    "               \"min_merch_trans\", \"max_merch_trans\"]\n",
    "\n",
    "# features added by tsg\n",
    "cols_tsg = ['active_days', \n",
    "            'recency',\n",
    "            'num_hist_distinct_city', 'num_new_distinct_city',\n",
    "            'num_hist_distinct_merchant', 'num_new_distinct_merchant',\n",
    "            'sum_month_lag', 'mean_month_lag', \n",
    "            'std_month_lag', 'min_month_lag', 'max_month_lag',\n",
    "            'sum_installments', 'mean_installments',\n",
    "            'std_installments', 'min_installments', 'max_installments',\n",
    "           ]\n",
    "\n",
    "cols_to_use += cols_tsg\n",
    "\n",
    "# upper_threshold = mean + 3*std\n",
    "# lower_threshold = mean - 3*std\n",
    "\n",
    "# train_df_filtered = train_df[(train_df[target_col]<upper_threshold) & (train_df[target_col]>lower_threshold)]\n",
    "# print('train_df_filtered.shape is', train_df_filtered.shape)\n",
    "# print('train_df.shape is', train_df.shape)\n",
    "\n",
    "# train_X = train_df_filtered[cols_to_use]\n",
    "# train_y = train_df_filtered[target_col].values\n",
    "\n",
    "train_X = train_df[cols_to_use]\n",
    "train_y = train_df[target_col].values\n",
    "test_X = test_df[cols_to_use]\n",
    "\n",
    "## 本地分5份 CV跑\n",
    "\n",
    "def run_lgb(dev_X, dev_y, val_X, val_y, test_X):\n",
    "    print('in run_lgb')\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"num_leaves\" : 100,\n",
    "        \"min_child_weight\" : 0.5,\n",
    "        \"learning_rate\" : 0.02,\n",
    "        \"bagging_fraction\" : 0.7,\n",
    "        \"feature_fraction\" : 0.7,\n",
    "        \"bagging_frequency\" : 5,\n",
    "        \"bagging_seed\" : 2018,\n",
    "        \"verbosity\" : -1\n",
    "    }\n",
    "    lgtrain = lgb.Dataset(dev_X, label=dev_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=100, evals_result=evals_result)\n",
    "    best_rmse = model.best_score['valid_0']['rmse']\n",
    "    best_iter_num = model.best_iteration\n",
    "    print('model.best_score: {} model.best_iteration: {} '.format(best_rmse, best_iter_num))\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    return pred_test_y, best_rmse, best_iter_num, model\n",
    "\n",
    "def get_CV_outcome(train_X, train_y, test_X, CV_num=5):\n",
    "    pred_test = 0\n",
    "    best_rmse = 0\n",
    "    best_iter_num = 0\n",
    "    kf = model_selection.KFold(n_splits=CV_num, random_state=2018, shuffle=True)\n",
    "    for dev_index, val_index in kf.split(train_df):\n",
    "    # for dev_index, val_index in kf.split(train_df_filtered):\n",
    "        dev_X, val_X = train_X.loc[dev_index,:], train_X.loc[val_index,:]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        pred_test_tmp, rmse, iter_num, model = run_lgb(dev_X, dev_y, val_X, val_y, test_X)\n",
    "        pred_test += pred_test_tmp\n",
    "        best_rmse += rmse\n",
    "        best_iter_num += iter_num\n",
    "    pred_test /= float(CV_num)\n",
    "    best_rmse /= float(CV_num)\n",
    "    best_iter_num /= float(CV_num)        \n",
    "    print('CV running finished!')\n",
    "    print('final is best_rmse: {} best_iter_num: {}'.format(best_rmse, best_iter_num))\n",
    "    return pred_test\n",
    "\n",
    "pred_test_y_CV = get_CV_outcome(train_X, train_y, test_X)\n",
    "\n",
    "sub_df = pd.DataFrame({\"card_id\": test_df[\"card_id\"].values})\n",
    "sub_df[\"target\"] = pred_test_y_CV\n",
    "sub_df.to_csv(submission_path + 'submission_lgb_CV.csv', index=False)\n",
    "\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用完整的训练样例去跑模型\n",
    "print('full running starts')\n",
    "params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\",\n",
    "        \"num_leaves\" : 100,\n",
    "        \"min_child_weight\" : 50,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        \"bagging_fraction\" : 0.7,\n",
    "        \"feature_fraction\" : 0.7,\n",
    "        \"bagging_frequency\" : 5,\n",
    "        \"bagging_seed\" : 2018,\n",
    "        \"verbosity\" : 1\n",
    "}    \n",
    "lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "model = lgb.train(params, lgtrain, 1000, verbose_eval=200)\n",
    "pred_test_y = model.predict(test_X)\n",
    "\n",
    "print('full running ends')\n",
    "\n",
    "sub_df = pd.DataFrame({\"card_id\":test_df[\"card_id\"].values})\n",
    "sub_df[\"target\"] = pred_test_y\n",
    "sub_df.to_csv(submission_path + 'submission_lgb_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 展示lightGBM的特征重要性排序\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "ax.grid(False)\n",
    "plt.title(\"LightGBM - Feature Importance\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 去除outlier **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_df[target_col].mean()\n",
    "std = train_df[target_col].std()\n",
    "\n",
    "upper_threshold = mean + 2*std\n",
    "lower_threshold = mean - 2*std\n",
    "\n",
    "train_df_filtered = train_df[(train_df[target_col]<upper_threshold) & (train_df[target_col]>lower_threshold)]\n",
    "print('train_df_filtered.shape is', train_df_filtered.shape)\n",
    "print('train_df.shape is', train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.shape, train_df.shape\n",
    "# and_set = set(test_df['card_id']) & set(train_df['card_id'])\n",
    "\n",
    "origin_len = len(set(merged_purchase_date_df['card_id']))\n",
    "and_set = set(merged_purchase_date_df['card_id']) | set(train_df['card_id'])\n",
    "origin_len, len(and_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 疑问：\n",
    "## 在两份历史记录合并后 有同样的数据（同一个card_id 同一个购物时间）\n",
    "\n",
    "## To-Do features List:\n",
    "## authorized_flag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
