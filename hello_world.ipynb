{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始时间：2018.07.01-01:01:04\n",
      "结束时间：2018.07.01-01:01:50\n",
      "花费时间：37.952815\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "str_p = '2019-05-07 10:21:15'\n",
    "dateTime_p = datetime.datetime.strptime(str_p,'%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "time_start = datetime.datetime(2018, 7, 1,1,1,4)\n",
    "print(\"开始时间：\" + time_start.strftime('%Y.%m.%d-%H:%M:%S'))\n",
    "time_end = datetime.datetime(2018, 7, 1,1,1,50)\n",
    "print(\"结束时间：\" + time_end.strftime('%Y.%m.%d-%H:%M:%S'))\n",
    "\n",
    "time_end = datetime.datetime.now()\n",
    "time_all=(time_end-dateTime_p).total_seconds()\n",
    "print(\"花费时间：\" + str(time_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:/using_tensorflow/send_email_test.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "def get_newest_file_name(dir_path):\n",
    "    lst = os.listdir(dir_path) #列出文件夹下所有的目录与文件\n",
    "    max_t = -9999\n",
    "    max_file_name = ''\n",
    "    for i in range(0,len(lst)):\n",
    "        name = os.path.join(dir_path,lst[i])\n",
    "        if os.path.isfile(name):\n",
    "            t = os.path.getctime(name)\n",
    "            if max_t<t:\n",
    "                max_t = t\n",
    "                max_file_name = name\n",
    "    return max_file_name\n",
    "\n",
    "\n",
    "latest_file_name = get_newest_file_name('F:/using_tensorflow/')\n",
    "\n",
    "print(latest_file_name)\n",
    "\n",
    "ttt = os.path.getctime('F:/using_tensorflow/send_email_test.py')\n",
    "# datetime.datetime.strptime(ttt, '%Y.%m.%d-%H:%M:%S')\n",
    "\n",
    "# datetime.datetime.utcfromtimestamp(ttt)\n",
    "\n",
    "'333_444' in '333_444_abc'\n",
    "\n",
    "# get_newest_file_name('F:\\using_tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line is  0\n",
      "line is  21\n",
      "line is  21\n"
     ]
    }
   ],
   "source": [
    "file_name = 'F:/github_me_repos/Kaggle_code/tencent_algo_2019/data.txt'\n",
    "\n",
    "with open(file_name) as file:\n",
    "    for line in file:\n",
    "        print('line is ', len(line.strip('\\n').split(',')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 books.\n"
     ]
    }
   ],
   "source": [
    "model_values = {'auc_score': 0.688, 'upload_check': True, 'model_final_time': '2019-05-07 12:00:00'}\n",
    "'auc:{}&&&upload_check:{}&&&model_final_time:{}'.format(model_values['auc_score'],\n",
    "    model_values['upload_check'], model_values['model_final_time'])\n",
    "\n",
    "books = [3, 2, 5]\n",
    "print(f'Found {len(books)} books.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 0, 1: 1, 0: 2, -1: 3, 9: 4, 5: 5, '1': 6, nan: 7}\n"
     ]
    }
   ],
   "source": [
    "def get_v_dict(v_lst):\n",
    "    v_dict = {}\n",
    "    for v in v_lst:\n",
    "        if v in v_dict:\n",
    "            continue\n",
    "        else:\n",
    "            v_dict[v] = len(v_dict)\n",
    "    return v_dict\n",
    "print(get_v_dict([3, 1, 0, -1, 1, 3, 0, -1, 9, 3, 5, 1, '1', np.nan]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,0,0,1,0,0,0,0,0,0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_str(num, total_num):\n",
    "    str_lst = ['0']*total_num\n",
    "    str_lst[num] = '1'\n",
    "    return ','.join(str_lst)\n",
    "\n",
    "generate_str(3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df is      val\n",
      "0   1.0\n",
      "1   2.0\n",
      "2   0.0\n",
      "3   3.0\n",
      "4   4.0\n",
      "5   5.0\n",
      "6   1.0\n",
      "7   2.0\n",
      "8   2.6\n",
      "9   2.0\n",
      "10  6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.9322745317068013,\n",
       " -0.34960294939005054,\n",
       " -1.514946114023552,\n",
       " 0.23306863292670027,\n",
       " 0.8157402152434511,\n",
       " 1.398411797560202,\n",
       " -0.9322745317068013,\n",
       " -0.34960294939005054,\n",
       " 0.0,\n",
       " -0.34960294939005054,\n",
       " 1.9810833798769527]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardized(v_lst):\n",
    "    df = pd.DataFrame({'val': v_lst})\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    print('df is ', df)\n",
    "    v_lst = StandardScaler().fit_transform(df['val'].values.reshape(-1, 1))\n",
    "    v_lst = v_lst.flatten()\n",
    "    return list(v_lst)\n",
    "\n",
    "v_lst = [1, 2, 0, 3, 4, 5, 1, 2, np.nan, 2, 6]\n",
    "standardized(v_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read cost time  1.4868407249450684\n",
      "outcome.shape is outcome.shape (3000, 742)\n",
      "aa.shape is  (1000, 742)\n"
     ]
    }
   ],
   "source": [
    "def generate_instances_from_file(columns, file_nums, data_dir='./'):\n",
    "    concated_matrix = None\n",
    "    for file_num in file_nums:\n",
    "        matrix = None\n",
    "        for column in columns:\n",
    "            file_name = data_dir + '/' + column + '_' + str(file_num).zfill(6)\n",
    "            part_matrix = np.loadtxt(file_name, delimiter=',')\n",
    "            if matrix is None:\n",
    "                matrix = part_matrix\n",
    "            else:\n",
    "                matrix = np.c_[matrix, part_matrix]\n",
    "        if concated_matrix is None:\n",
    "            concated_matrix = matrix\n",
    "        else:\n",
    "            concated_matrix = np.r_[concated_matrix, matrix]\n",
    "    np.random.shuffle(concated_matrix) # 按行打乱顺序\n",
    "    return concated_matrix\n",
    "\n",
    "# columns = ['first_payment_type', 'branch_code']\n",
    "columns = ['buy_cnt', 'cost_sum', 'first_order_cost',\n",
    "               'first_origin_type', 'first_payment_type',\n",
    "               'first_order_status', 'gap_days_first_order',\n",
    "               'last_order_cost', 'last_origin_type', 'last_payment_type',\n",
    "               'last_order_status', 'gap_days_last_order', 'address_code',\n",
    "               'class_code', 'branch_code', 'call_month',\n",
    "               'call_weekday', 'address_num']\n",
    "\n",
    "start_t = time.time()\n",
    "outcome = generate_instances_from_file(columns, [3, 4, 7], 'F:/kefu_zhineng/using_tensorflow/data')\n",
    "print('read cost time ', time.time()-start_t)\n",
    "\n",
    "print('outcome.shape is outcome.shape', outcome.shape)\n",
    "\n",
    "aa, bb, cc = np.split(outcome, 3)\n",
    "\n",
    "print('aa.shape is ', aa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy.sparse import csr\n",
    "\n",
    "\n",
    "def vectorize_dic(dic, ix=None, p=None):\n",
    "    \"\"\" \n",
    "    Creates a scipy csr matrix from a list of lists (each inner list is a set of values corresponding to a feature) \n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    dic -- dictionary of feature lists. Keys are the name of features\n",
    "    ix -- index generator (default None)\n",
    "    p -- dimension of featrure space (number of columns in the sparse matrix) (default None)\n",
    "    \"\"\"\n",
    "    if (ix == None):\n",
    "        d = count(0)\n",
    "        ix = defaultdict(lambda: next(d)) \n",
    "        \n",
    "    n = len(list(dic.values())[0]) # num samples\n",
    "    g = len(list(dic.keys())) # num groups\n",
    "    nz = n * g # number of non-zeros\n",
    "\n",
    "    col_ix = np.empty(nz, dtype=int)     \n",
    "    \n",
    "    i = 0\n",
    "    for k, lis in dic.items():     \n",
    "        # append index el with k in order to prevet mapping different columns with same id to same index\n",
    "        col_ix[i::g] = [ix[str(el) + str(k)] for el in lis]\n",
    "        i += 1\n",
    "        \n",
    "    row_ix = np.repeat(np.arange(0, n), g)      \n",
    "    data = np.ones(nz)\n",
    "    \n",
    "    if (p == None):\n",
    "        p = len(ix)\n",
    "        \n",
    "    ixx = np.where(col_ix < p)\n",
    "\n",
    "    return csr.csr_matrix((data[ixx],(row_ix[ixx], col_ix[ixx])), shape=(n, p)), ix\n",
    "\n",
    "    \n",
    "def vectorize(lil, ix=None, p=None):\n",
    "    \"\"\" \n",
    "    Creates a scipy csr matrix from a list of lists (each inner list is a set of values corresponding to a feature) \n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    lil -- list of lists (dimension of inner lists should be the same)\n",
    "    ix -- index generator (default None)\n",
    "    p -- dimension of featrure space (number of columns in the sparse matrix) (default None)\n",
    "    \"\"\"\n",
    "    if (ix == None):\n",
    "        d = count(0)\n",
    "        ix = defaultdict(lambda: next(d)) \n",
    "#         ix = defaultdict(count(0).next)\n",
    "    \n",
    "    n = len(lil[0]) # num samples\n",
    "    g = len(lil) # num groups\n",
    "    nz = n * g # number of non-zeros\n",
    "\n",
    "    col_ix = np.empty(nz, dtype=int)\n",
    "    \n",
    "    for i, d in enumerate(lil):\n",
    "        # append index k with __i in order to prevet mapping different columns with same id to same index\n",
    "        col_ix[i::g] = [ix[str(k) + '__' + str(i)] for k in d]\n",
    "\n",
    "    row_ix = np.repeat(np.arange(0, n), g)\n",
    "    data = np.ones(nz)\n",
    "    \n",
    "    if (p == None):\n",
    "        p = len(ix)\n",
    "    \n",
    "    # only features that are less than p (siz of feature vector) are considered\n",
    "    ixx = np.where(col_ix < p)\n",
    "    \n",
    "    return csr.csr_matrix((data[ixx], (row_ix[ixx], col_ix[ixx])), shape=(n, p)), ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 0., 1., 0.],\n",
       "        [0., 1., 1., 0.],\n",
       "        [1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ids = ['A', 'B', 'A']\n",
    "movie_ids = ['a', 'a', 'b']\n",
    "\n",
    "X, ix = vectorize_dic({'aa': user_ids, 'bb': movie_ids})\n",
    "X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(np.arange(10), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-2045caafa68d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# scaler = StandardScaler()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_std\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mconverted_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m# StandardScaler(copy=True, with_mean=True, with_std=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \"\"\"\n\u001b[0;32m    611\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m--> 612\u001b[1;33m                         warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 44\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# data = np.array([[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "data = np.array([1, 2, 0, 3, 4, 5, 1, 2, np.nan, 2, 6])\n",
    "data = data.reshape(-1, 1)\n",
    "\n",
    "print(data.mean())\n",
    "\n",
    "                                \n",
    "# scaler = StandardScaler()\n",
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "converted_data = scaler.fit_transform(data)\n",
    "# StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "print(converted_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
