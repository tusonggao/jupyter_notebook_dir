{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "file_path = 'F:/github_me_repos/Kaggle_code/elo-merchant-category-recommendation/'\n",
    "data_path = file_path + '/data/'\n",
    "submission_path = file_path + '/submission/'\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n"
     ]
    }
   ],
   "source": [
    "FEATS_EXCLUDED = ['first_active_month', 'target', 'card_id', 'outliers',\n",
    "                  'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n",
    "                  'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size',\n",
    "                  'OOF_PRED', 'month_0']\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# rmse\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    print('in one_hot_encoder')\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "    \n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    print('in display_importances')\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances.png')\n",
    "\n",
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    print('in reduce_mem_usage')\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "    \n",
    "# preprocessing train & test\n",
    "def train_test(num_rows=None):\n",
    "    print('in train_test()')\n",
    "\n",
    "    # load csv\n",
    "    train_df = pd.read_csv(data_path + '/train.csv', index_col=['card_id'], nrows=num_rows)\n",
    "    test_df = pd.read_csv(data_path + '/test.csv', index_col=['card_id'], nrows=num_rows)\n",
    "\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(train_df), len(test_df)))\n",
    "\n",
    "    # outlier\n",
    "    train_df['outliers'] = 0\n",
    "    train_df.loc[train_df['target'] < -30, 'outliers'] = 1\n",
    "\n",
    "    # set target as nan\n",
    "    test_df['target'] = np.nan\n",
    "\n",
    "    # merge\n",
    "    df = train_df.append(test_df)\n",
    "\n",
    "    del train_df, test_df\n",
    "    gc.collect()\n",
    "\n",
    "    # to datetime\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "\n",
    "    # datetime features\n",
    "    df['quarter'] = df['first_active_month'].dt.quarter\n",
    "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
    "\n",
    "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
    "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
    "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
    "\n",
    "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
    "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
    "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
    "\n",
    "    # one hot encoding\n",
    "    df, cols = one_hot_encoder(df, nan_as_category=False)\n",
    "\n",
    "    for f in ['feature_1','feature_2','feature_3']:\n",
    "        order_label = df.groupby([f])['outliers'].mean()\n",
    "        df[f] = df[f].map(order_label)\n",
    "\n",
    "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
    "    df['feature_mean'] = df['feature_sum']/3\n",
    "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# preprocessing historical transactions\n",
    "def historical_transactions(num_rows=None):\n",
    "    print('in historical_transactions()')\n",
    "    # load csv\n",
    "    hist_df = pd.read_csv(data_path + '/historical_transactions.csv', nrows=num_rows)\n",
    "    print('read historical_transactions.csv finished!')\n",
    "    \n",
    "    hist_df = reduce_mem_usage(hist_df)\n",
    "\n",
    "    # fillna\n",
    "    hist_df['category_2'].fillna(1.0,inplace=True)\n",
    "    hist_df['category_3'].fillna('A',inplace=True)\n",
    "    hist_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    hist_df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    hist_df['installments'].replace(999, np.nan,inplace=True)\n",
    "\n",
    "    # trim\n",
    "    hist_df['purchase_amount'] = hist_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    hist_df['authorized_flag'] = hist_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    hist_df['category_1'] = hist_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    hist_df['category_3'] = hist_df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "\n",
    "    # datetime features\n",
    "    hist_df['purchase_date'] = pd.to_datetime(hist_df['purchase_date'])\n",
    "    hist_df['month'] = hist_df['purchase_date'].dt.month\n",
    "    hist_df['day'] = hist_df['purchase_date'].dt.day\n",
    "    hist_df['hour'] = hist_df['purchase_date'].dt.hour\n",
    "    hist_df['weekofyear'] = hist_df['purchase_date'].dt.weekofyear\n",
    "    hist_df['weekday'] = hist_df['purchase_date'].dt.weekday\n",
    "    hist_df['weekend'] = (hist_df['purchase_date'].dt.weekday >=5).astype(int)\n",
    "    \n",
    "    print('get here 111 in historical_transactions')\n",
    "\n",
    "    # additional features\n",
    "    hist_df['price'] = hist_df['purchase_amount'] / hist_df['installments']\n",
    "    \n",
    "    print('get here 111 AAA in historical_transactions')\n",
    "\n",
    "    #Christmas : December 25 2017\n",
    "    hist_df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Mothers Day: May 14 2017\n",
    "    hist_df['Mothers_Day_2017']=(pd.to_datetime('2017-06-04')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #fathers day: August 13 2017\n",
    "    \n",
    "    print('get here 111 BBB in historical_transactions')\n",
    "    \n",
    "    \n",
    "    hist_df['fathers_day_2017']=(pd.to_datetime('2017-08-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Childrens day: October 12 2017\n",
    "    hist_df['Children_day_2017']=(pd.to_datetime('2017-10-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Valentine's Day : 12th June, 2017\n",
    "    \n",
    "    print('get here 111 CCC in historical_transactions')\n",
    "    \n",
    "    hist_df['Valentine_Day_2017']=(pd.to_datetime('2017-06-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Black Friday : 24th November 2017\n",
    "    hist_df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    #2018\n",
    "    #Mothers Day: May 13 2018\n",
    "    hist_df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    hist_df['month_diff'] = ((datetime.datetime.today() - hist_df['purchase_date']).dt.days)//30\n",
    "    hist_df['month_diff'] += hist_df['month_lag']\n",
    "    \n",
    "    \n",
    "    print('get here 111 DDD in historical_transactions')\n",
    "\n",
    "    # additional features\n",
    "    hist_df['duration'] = hist_df['purchase_amount']*hist_df['month_diff']\n",
    "    hist_df['amount_month_ratio'] = hist_df['purchase_amount']/hist_df['month_diff']\n",
    "    \n",
    "    \n",
    "    print('get here 111 EEE in historical_transactions')\n",
    "\n",
    "    # reduce memory usage\n",
    "    hist_df = reduce_mem_usage(hist_df)\n",
    "    \n",
    "    print('get here 222')\n",
    "\n",
    "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "\n",
    "    aggs = {}\n",
    "    for col in col_unique:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    for col in col_seas:\n",
    "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "    aggs['month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['authorized_flag'] = ['mean']\n",
    "    aggs['weekend'] = ['mean'] # overwrite\n",
    "    aggs['weekday'] = ['mean'] # overwrite\n",
    "    aggs['day'] = ['nunique', 'mean', 'min'] # overwrite\n",
    "    aggs['category_1'] = ['mean']\n",
    "    aggs['category_2'] = ['mean']\n",
    "    aggs['category_3'] = ['mean']\n",
    "    aggs['card_id'] = ['size','count']\n",
    "    aggs['price'] = ['sum','mean','max','min','var']\n",
    "    aggs['Christmas_Day_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2017'] = ['mean']\n",
    "    aggs['fathers_day_2017'] = ['mean']\n",
    "    aggs['Children_day_2017'] = ['mean']\n",
    "    aggs['Valentine_Day_2017'] = ['mean']\n",
    "    aggs['Black_Friday_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2018'] = ['mean']\n",
    "    aggs['duration']=['mean','min','max','var','skew']\n",
    "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "\n",
    "    print('get here 333 in historical_transactions')\n",
    "    \n",
    "    for col in ['category_2','category_3']:\n",
    "        hist_df[col+'_mean'] = hist_df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        hist_df[col+'_min'] = hist_df.groupby([col])['purchase_amount'].transform('min')\n",
    "        hist_df[col+'_max'] = hist_df.groupby([col])['purchase_amount'].transform('max')\n",
    "        hist_df[col+'_sum'] = hist_df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col+'_mean'] = ['mean']\n",
    "\n",
    "    print('start agg in historical_transactions')\n",
    "    hist_df = hist_df.reset_index().groupby('card_id').agg(aggs)\n",
    "    print('end agg in historical_transactions')\n",
    "    \n",
    "    # reduce memory usage\n",
    "    hist_df = reduce_mem_usage(hist_df)\n",
    "\n",
    "    # change column name\n",
    "    hist_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in hist_df.columns.tolist()])\n",
    "    hist_df.columns = ['hist_'+ c for c in hist_df.columns]\n",
    "\n",
    "    hist_df['hist_purchase_date_diff'] = (hist_df['hist_purchase_date_max']-hist_df['hist_purchase_date_min']).dt.days\n",
    "    hist_df['hist_purchase_date_average'] = hist_df['hist_purchase_date_diff']/hist_df['hist_card_id_size']\n",
    "    hist_df['hist_purchase_date_uptonow'] = (datetime.datetime.today()-hist_df['hist_purchase_date_max']).dt.days\n",
    "    hist_df['hist_purchase_date_uptomin'] = (datetime.datetime.today()-hist_df['hist_purchase_date_min']).dt.days\n",
    "    \n",
    "    print('get here 444 in historical_transactions')\n",
    "\n",
    "    # reduce memory usage\n",
    "    hist_df = reduce_mem_usage(hist_df)\n",
    "\n",
    "    return hist_df\n",
    "    \n",
    "# preprocessing new_merchant_transactions\n",
    "def new_merchant_transactions(num_rows=None):\n",
    "    print('in new_merchant_transactions()')\n",
    "    # load csv\n",
    "    new_merchant_df = pd.read_csv(data_path + '/new_merchant_transactions.csv', nrows=num_rows)\n",
    "    print('read new_merchant_transactions.csv finished!')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # reduce memory usage\n",
    "#     new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
    "    \n",
    "    print('get here 111 in new_merchant_transactions')\n",
    "\n",
    "    # fillna\n",
    "    new_merchant_df['category_2'].fillna(1.0,inplace=True)\n",
    "    print('get here RRR 111 in new_merchant_transactions')\n",
    "    \n",
    "    new_merchant_df['category_3'].fillna('A',inplace=True)\n",
    "    print('get here RRR 222 in new_merchant_transactions')\n",
    "    \n",
    "    new_merchant_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    print('get here RRR 333 in new_merchant_transactions')\n",
    "    \n",
    "    new_merchant_df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    print('get here RRR 444 in new_merchant_transactions')\n",
    "    \n",
    "    new_merchant_df['installments'].replace(999, np.nan,inplace=True)\n",
    "    print('get here RRR 555 in new_merchant_transactions')\n",
    "    \n",
    "    print('get here 111 aaa in new_merchant_transactions')\n",
    "\n",
    "    # trim\n",
    "    new_merchant_df['purchase_amount'] = new_merchant_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "    \n",
    "    print('get here 111 bbb in new_merchant_transactions')\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    new_merchant_df['authorized_flag'] = new_merchant_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    new_merchant_df['category_1'] = new_merchant_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    new_merchant_df['category_3'] = new_merchant_df['category_3'].map({'A':0, 'B':1, 'C':2}).astype(int)\n",
    "    \n",
    "    print('get here 111 ccc in new_merchant_transactions')\n",
    "\n",
    "    # datetime features\n",
    "    new_merchant_df['purchase_date'] = pd.to_datetime(new_merchant_df['purchase_date'])\n",
    "    new_merchant_df['month'] = new_merchant_df['purchase_date'].dt.month\n",
    "    new_merchant_df['day'] = new_merchant_df['purchase_date'].dt.day\n",
    "    new_merchant_df['hour'] = new_merchant_df['purchase_date'].dt.hour\n",
    "    new_merchant_df['weekofyear'] = new_merchant_df['purchase_date'].dt.weekofyear\n",
    "    new_merchant_df['weekday'] = new_merchant_df['purchase_date'].dt.weekday\n",
    "    new_merchant_df['weekend'] = (new_merchant_df['purchase_date'].dt.weekday >=5).astype(int)\n",
    "    \n",
    "    print('get here 222 in new_merchant_transactions')\n",
    "\n",
    "    # additional features\n",
    "    new_merchant_df['price'] = new_merchant_df['purchase_amount'] / new_merchant_df['installments']\n",
    "\n",
    "    #Christmas : December 25 2017\n",
    "    new_merchant_df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Childrens day: October 12 2017\n",
    "    new_merchant_df['Children_day_2017']=(pd.to_datetime('2017-10-12')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Black Friday : 24th November 2017\n",
    "    new_merchant_df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    #Mothers Day: May 13 2018\n",
    "    new_merchant_df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    new_merchant_df['month_diff'] = ((datetime.datetime.today() - new_merchant_df['purchase_date']).dt.days)//30\n",
    "    new_merchant_df['month_diff'] += new_merchant_df['month_lag']\n",
    "\n",
    "    # additional features\n",
    "    new_merchant_df['duration'] = new_merchant_df['purchase_amount']*new_merchant_df['month_diff']\n",
    "    new_merchant_df['amount_month_ratio'] = new_merchant_df['purchase_amount']/new_merchant_df['month_diff']\n",
    "\n",
    "    # reduce memory usage\n",
    "    new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
    "    \n",
    "    print('get here 333 in new_merchant_transactions')\n",
    "\n",
    "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "\n",
    "    aggs = {}\n",
    "    for col in col_unique:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    for col in col_seas:\n",
    "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "    aggs['month_diff'] = ['mean','var','skew']\n",
    "    aggs['weekend'] = ['mean']\n",
    "    aggs['month'] = ['mean', 'min', 'max']\n",
    "    aggs['weekday'] = ['mean', 'min', 'max']\n",
    "    aggs['category_1'] = ['mean']\n",
    "    aggs['category_2'] = ['mean']\n",
    "    aggs['category_3'] = ['mean']\n",
    "    aggs['card_id'] = ['size','count']\n",
    "    aggs['price'] = ['mean','max','min','var']\n",
    "    aggs['Christmas_Day_2017'] = ['mean']\n",
    "    aggs['Children_day_2017'] = ['mean']\n",
    "    aggs['Black_Friday_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2018'] = ['mean']\n",
    "    aggs['duration']=['mean','min','max','var','skew']\n",
    "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "\n",
    "    for col in ['category_2','category_3']:\n",
    "        new_merchant_df[col+'_mean'] = new_merchant_df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        new_merchant_df[col+'_min'] = new_merchant_df.groupby([col])['purchase_amount'].transform('min')\n",
    "        new_merchant_df[col+'_max'] = new_merchant_df.groupby([col])['purchase_amount'].transform('max')\n",
    "        new_merchant_df[col+'_sum'] = new_merchant_df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col+'_mean'] = ['mean']\n",
    "\n",
    "    print('get here 444 in new_merchant_transactions')\n",
    "    new_merchant_df = new_merchant_df.reset_index().groupby('card_id').agg(aggs)\n",
    "    \n",
    "    print('get here 555 in new_merchant_transactions')\n",
    "\n",
    "    # change column name\n",
    "    new_merchant_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in new_merchant_df.columns.tolist()])\n",
    "    new_merchant_df.columns = ['new_'+ c for c in new_merchant_df.columns]\n",
    "\n",
    "    new_merchant_df['new_purchase_date_diff'] = (new_merchant_df['new_purchase_date_max']-new_merchant_df['new_purchase_date_min']).dt.days\n",
    "    new_merchant_df['new_purchase_date_average'] = new_merchant_df['new_purchase_date_diff']/new_merchant_df['new_card_id_size']\n",
    "    new_merchant_df['new_purchase_date_uptonow'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_max']).dt.days\n",
    "    new_merchant_df['new_purchase_date_uptomin'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_min']).dt.days\n",
    "    \n",
    "    print('get here 666 in new_merchant_transactions')\n",
    "\n",
    "    # reduce memory usage\n",
    "    new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
    "\n",
    "    return new_merchant_df\n",
    "\n",
    "# additional features\n",
    "def additional_features(df):\n",
    "    print('in additional_features()')\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_last_buy'] = (df['new_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "\n",
    "    date_features=['hist_purchase_date_max','hist_purchase_date_min',\n",
    "                   'new_purchase_date_max', 'new_purchase_date_min']\n",
    "\n",
    "    for f in date_features:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "\n",
    "    df['card_id_total'] = df['new_card_id_size']+df['hist_card_id_size']\n",
    "    df['card_id_cnt_total'] = df['new_card_id_count']+df['hist_card_id_count']\n",
    "    df['card_id_cnt_ratio'] = df['new_card_id_count']/df['hist_card_id_count']\n",
    "    df['purchase_amount_total'] = df['new_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
    "    df['purchase_amount_mean'] = df['new_purchase_amount_mean']+df['hist_purchase_amount_mean']\n",
    "    df['purchase_amount_max'] = df['new_purchase_amount_max']+df['hist_purchase_amount_max']\n",
    "    df['purchase_amount_min'] = df['new_purchase_amount_min']+df['hist_purchase_amount_min']\n",
    "    df['purchase_amount_ratio'] = df['new_purchase_amount_sum']/df['hist_purchase_amount_sum']\n",
    "    df['month_diff_mean'] = df['new_month_diff_mean']+df['hist_month_diff_mean']\n",
    "    df['month_diff_ratio'] = df['new_month_diff_mean']/df['hist_month_diff_mean']\n",
    "    df['month_lag_mean'] = df['new_month_lag_mean']+df['hist_month_lag_mean']\n",
    "    df['month_lag_max'] = df['new_month_lag_max']+df['hist_month_lag_max']\n",
    "    df['month_lag_min'] = df['new_month_lag_min']+df['hist_month_lag_min']\n",
    "    df['category_1_mean'] = df['new_category_1_mean']+df['hist_category_1_mean']\n",
    "    df['installments_total'] = df['new_installments_sum']+df['hist_installments_sum']\n",
    "    df['installments_mean'] = df['new_installments_mean']+df['hist_installments_mean']\n",
    "    df['installments_max'] = df['new_installments_max']+df['hist_installments_max']\n",
    "    df['installments_ratio'] = df['new_installments_sum']/df['hist_installments_sum']\n",
    "    df['price_total'] = df['purchase_amount_total'] / df['installments_total']\n",
    "    df['price_mean'] = df['purchase_amount_mean'] / df['installments_mean']\n",
    "    df['price_max'] = df['purchase_amount_max'] / df['installments_max']\n",
    "    df['duration_mean'] = df['new_duration_mean']+df['hist_duration_mean']\n",
    "    df['duration_min'] = df['new_duration_min']+df['hist_duration_min']\n",
    "    df['duration_max'] = df['new_duration_max']+df['hist_duration_max']\n",
    "    df['amount_month_ratio_mean']=df['new_amount_month_ratio_mean']+df['hist_amount_month_ratio_mean']\n",
    "    df['amount_month_ratio_min']=df['new_amount_month_ratio_min']+df['hist_amount_month_ratio_min']\n",
    "    df['amount_month_ratio_max']=df['new_amount_month_ratio_max']+df['hist_amount_month_ratio_max']\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_purchase_amount_sum'] / df['new_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_purchase_amount_sum'] / df['hist_month_diff_mean']\n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']\n",
    "\n",
    "    return df\n",
    "\n",
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "def kfold_lightgbm(train_df, test_df, num_folds, stratified = False, debug= False):\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED]\n",
    "\n",
    "    # k-fold\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['outliers'])):\n",
    "        print('n_fold: ', n_fold)\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['target'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['target'].iloc[valid_idx]\n",
    "\n",
    "        # set data structure\n",
    "        lgb_train = lgb.Dataset(train_x,\n",
    "                                label=train_y,\n",
    "                                free_raw_data=False)\n",
    "        lgb_test = lgb.Dataset(valid_x,\n",
    "                               label=valid_y,\n",
    "                               free_raw_data=False)\n",
    "\n",
    "        # params optimized by optuna\n",
    "        params ={\n",
    "                'task': 'train',\n",
    "                'boosting': 'goss',\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'learning_rate': 0.01,\n",
    "                'subsample': 0.9855232997390695,\n",
    "                'max_depth': 7,\n",
    "                'top_rate': 0.9064148448434349,\n",
    "                'num_leaves': 63,\n",
    "                'min_child_weight': 41.9612869171337,\n",
    "                'other_rate': 0.0721768246018207,\n",
    "                'reg_alpha': 9.677537745007898,\n",
    "                'colsample_bytree': 0.5665320670155495,\n",
    "                'min_split_gain': 9.820197773625843,\n",
    "                'reg_lambda': 8.2532317400459,\n",
    "                'min_data_in_leaf': 21,\n",
    "                'verbose': -1,\n",
    "                'seed':int(2**n_fold),\n",
    "                'bagging_seed':int(2**n_fold),\n",
    "                'drop_seed':int(2**n_fold)\n",
    "                }\n",
    "\n",
    "        reg = lgb.train(\n",
    "                        params,\n",
    "                        lgb_train,\n",
    "                        valid_sets=[lgb_train, lgb_test],\n",
    "                        valid_names=['train', 'test'],\n",
    "                        num_boost_round=10000,\n",
    "                        early_stopping_rounds= 200,\n",
    "                        verbose_eval=100\n",
    "                        )\n",
    "\n",
    "        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n",
    "        sub_preds += reg.predict(test_df[feats], num_iteration=reg.best_iteration) / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration))\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d RMSE : %.6f' % (n_fold + 1, rmse(valid_y, oof_preds[valid_idx])))\n",
    "        del reg, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    # display importances\n",
    "    display_importances(feature_importance_df)\n",
    "\n",
    "    if not debug:\n",
    "        # save submission file\n",
    "        test_df.loc[:,'target'] = sub_preds\n",
    "        test_df = test_df.reset_index()\n",
    "        test_df[['card_id', 'target']].to_csv(submission_file_name, index=False)\n",
    "        \n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in main() \n",
      "in train_test()\n",
      "Train samples: 10000, test samples: 10000\n",
      "in one_hot_encoder\n",
      "in reduce_mem_usage\n",
      "Memory usage after optimization is: 0.97 MB\n",
      "Decreased by 68.1%\n",
      "train & test - done in 0s\n",
      "in historical_transactions()\n",
      "read historical_transactions.csv finished!\n",
      "in reduce_mem_usage\n",
      "Memory usage after optimization is: 0.57 MB\n",
      "Decreased by 46.4%\n",
      "get here 111 in historical_transactions\n",
      "get here 111 AAA in historical_transactions\n",
      "get here 111 BBB in historical_transactions\n",
      "get here 111 CCC in historical_transactions\n",
      "get here 111 DDD in historical_transactions\n",
      "get here 111 EEE in historical_transactions\n",
      "in reduce_mem_usage\n",
      "Memory usage after optimization is: 0.63 MB\n",
      "Decreased by 66.5%\n",
      "get here 222\n",
      "get here 333 in historical_transactions\n",
      "start agg in historical_transactions\n",
      "end agg in historical_transactions\n",
      "in reduce_mem_usage\n",
      "Memory usage after optimization is: 0.01 MB\n",
      "Decreased by 56.2%\n",
      "get here 444 in historical_transactions\n",
      "in reduce_mem_usage\n",
      "Memory usage after optimization is: 0.01 MB\n",
      "Decreased by 10.0%\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'card_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2524\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2525\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'card_id'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b8b5153fda85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0msubmission_file_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmission_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/with_out_blending_submission.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Full model run\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'total finished!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-b8b5153fda85>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(debug)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"historical transactions\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistorical_transactions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'card_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'outer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_mem_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get 111'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     55\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                          validate=validate)\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    563\u001b[0m         (self.left_join_keys,\n\u001b[0;32m    564\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    822\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_rkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 824\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    825\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2146\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1842\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3842\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3843\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3844\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2525\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'card_id'"
     ]
    }
   ],
   "source": [
    "def main(debug=False):\n",
    "    print('in main() ')\n",
    "    num_rows = 10000 if debug else None\n",
    "    with timer(\"train & test\"):\n",
    "        df = train_test(num_rows)\n",
    "        df = reduce_mem_usage(df)\n",
    "        \n",
    "    with timer(\"historical transactions\"):\n",
    "        df = pd.merge(df, historical_transactions(num_rows), on='card_id', how='outer')\n",
    "        df = reduce_mem_usage(df)\n",
    "    print('get 111')\n",
    "    \n",
    "    with timer(\"new merchants\"):\n",
    "        df = pd.merge(df, new_merchant_transactions(num_rows), on='card_id', how='outer')\n",
    "        df = reduce_mem_usage(df)\n",
    "    print('get 222')\n",
    "    \n",
    "    with timer(\"additional features\"):\n",
    "        df = additional_features(df)\n",
    "        df = reduce_mem_usage(df)\n",
    "        \n",
    "    print('get 333')\n",
    "    with timer(\"split train & test\"):\n",
    "        train_df = df[df['target'].notnull()]\n",
    "        test_df = df[df['target'].isnull()]\n",
    "        del df\n",
    "        gc.collect()\n",
    "    print('get 444')\n",
    "    \n",
    "    with timer(\"Run LightGBM with kfold\"):\n",
    "        kfold_lightgbm(train_df, test_df, num_folds=11, stratified=False, debug=debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    submission_file_name = submission_path + '/with_out_blending_submission.csv'\n",
    "    with timer(\"Full model run\"):\n",
    "        main(debug=True)\n",
    "    print('total finished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
