{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Python Course Home Page](https://www.kaggle.com/learn/python)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "import json\n",
    "\n",
    "import gensim\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "rng = np.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 1.162276864 W= 3.1514933 b= 0.22083336\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 2000\n",
    "display_step = 50\n",
    "\n",
    "\n",
    "# Training Data\n",
    "# train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "#                          7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_X = 100*np.random.rand(100)\n",
    "train_Y = 3.1*train_X + 2.85 + 0.01*np.random.rand(len(train_X))\n",
    "# train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "#                          2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(rng.randn(), name=\"weight\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")\n",
    "\n",
    "# Construct a linear model\n",
    "pred = tf.add(tf.multiply(X, W), b)\n",
    "\n",
    "# Mean squared error\n",
    "cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n",
    "# Gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (x, y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \n",
    "                \"W=\", sess.run(W), \"b=\", sess.run(b) )\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
    "    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
    "\n",
    "    #Graphic display\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "# Set Eager API\n",
    "print(\"Setting Eager mode...\")\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "# Define constant tensors\n",
    "print(\"Define constant tensors\")\n",
    "a = tf.constant(2)\n",
    "print(\"a = %i\" % a)\n",
    "b = tf.constant(3)\n",
    "print(\"b = %i\" % b)\n",
    "\n",
    "# # Run the operation without the need for tf.Session\n",
    "# print(\"Running operations, without tf.Session\")\n",
    "# c = a + b\n",
    "# print(\"a + b = %i\" % c)\n",
    "# d = a * b\n",
    "# print(\"a * b = %i\" % d)\n",
    "\n",
    "\n",
    "# # Full compatibility with Numpy\n",
    "# print(\"Mixing operations with Tensors and Numpy Arrays\")\n",
    "\n",
    "# # Define constant tensors\n",
    "# a = tf.constant([[2., 1.],\n",
    "#                  [1., 0.]], dtype=tf.float32)\n",
    "# print(\"Tensor:\\n a = %s\" % a)\n",
    "# b = np.array([[3., 0.],\n",
    "#               [5., 1.]], dtype=np.float32)\n",
    "# print(\"NumpyArray:\\n b = %s\" % b)\n",
    "\n",
    "# # Run the operation without the need for tf.Session\n",
    "# print(\"Running operations, without tf.Session\")\n",
    "\n",
    "# c = a + b\n",
    "# print(\"a + b = %s\" % c)\n",
    "\n",
    "# d = tf.matmul(a, b)\n",
    "# print(\"a * b = %s\" % d)\n",
    "\n",
    "# print(\"Iterate through Tensor 'a':\")\n",
    "# for i in range(a.shape[0]):\n",
    "#     for j in range(a.shape[1]):\n",
    "#         print(a[i][j])\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = 32\n",
    "x = tf.linspace(-3.0, 3.0, n_values)\n",
    "\n",
    "sess = tf.Session()\n",
    "result1 = sess.run(x)\n",
    "print('result1 is ', result1)\n",
    "\n",
    "# %% Alternatively pass a session to the eval fn:\n",
    "result2 = x.eval(session=sess)\n",
    "print('result2 is ', result2)\n",
    "# x.eval() does not work, as it requires a session!\n",
    "\n",
    "sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# %% Now this will work!\n",
    "x.eval()\n",
    "\n",
    "# %% Now a tf.Operation\n",
    "# We'll use our values from [-3, 3] to create a Gaussian Distribution\n",
    "sigma = 1.0\n",
    "mean = 0.0\n",
    "z = (tf.exp(tf.negative(tf.pow(x - mean, 2.0) /\n",
    "                   (2.0 * tf.pow(sigma, 2.0)))) *\n",
    "     (1.0 / (sigma * tf.sqrt(2.0 * 3.1415))))\n",
    "\n",
    "# %% By default, new operations are added to the default Graph\n",
    "assert z.graph is tf.get_default_graph()\n",
    "\n",
    "# %% Execute the graph and plot the result\n",
    "plt.plot(z.eval())\n",
    "\n",
    "# %% We can find out the shape of a tensor like so:\n",
    "print(z.get_shape())\n",
    "\n",
    "# %% Or in a more friendly format\n",
    "print(z.get_shape().as_list())\n",
    "\n",
    "\n",
    "print(tf.shape(z).eval())\n",
    "\n",
    "# %% We can combine tensors like so:\n",
    "print(tf.stack([tf.shape(z), tf.shape(z), [3], [4]]).eval())\n",
    "\n",
    "# %% Let's multiply the two to get a 2d gaussian\n",
    "z_2d = tf.matmul(tf.reshape(z, [n_values, 1]), tf.reshape(z, [1, n_values]))\n",
    "\n",
    "# %% Execute the graph and store the value that `out` represents in `result`.\n",
    "plt.imshow(z_2d.eval())\n",
    "\n",
    "# %% We can also list all the operations of a graph:\n",
    "ops = tf.get_default_graph().get_operations()\n",
    "print([op.name for op in ops])\n",
    "\n",
    "# %% Lets try creating a generic function for computing the same thing:\n",
    "def gabor(n_values=32, sigma=1.0, mean=0.0):\n",
    "    x = tf.linspace(-3.0, 3.0, n_values)\n",
    "    z = (tf.exp(tf.negative(tf.pow(x - mean, 2.0) /\n",
    "                       (2.0 * tf.pow(sigma, 2.0)))) *\n",
    "         (1.0 / (sigma * tf.sqrt(2.0 * 3.1415))))\n",
    "    gauss_kernel = tf.matmul(\n",
    "        tf.reshape(z, [n_values, 1]), tf.reshape(z, [1, n_values]))\n",
    "    x = tf.reshape(tf.sin(tf.linspace(-3.0, 3.0, n_values)), [n_values, 1])\n",
    "    y = tf.reshape(tf.ones_like(x), [1, n_values])\n",
    "    gabor_kernel = tf.multiply(tf.matmul(x, y), gauss_kernel)\n",
    "    return gabor_kernel\n",
    "\n",
    "# %% Confirm this does something:\n",
    "plt.imshow(gabor().eval())\n",
    "\n",
    "\n",
    "#%% And another function which can convolve\n",
    "def convolve(img, W):\n",
    "    # The W matrix is only 2D\n",
    "    # But conv2d will need a tensor which is 4d:\n",
    "    # height x width x n_input x n_output\n",
    "    if len(W.get_shape()) == 2:\n",
    "        dims = W.get_shape().as_list() + [1, 1]\n",
    "        W = tf.reshape(W, dims)\n",
    "\n",
    "    if len(img.get_shape()) == 2:\n",
    "        # num x height x width x channels\n",
    "        dims = [1] + img.get_shape().as_list() + [1]\n",
    "        img = tf.reshape(img, dims)\n",
    "    elif len(img.get_shape()) == 3:\n",
    "        dims = [1] + img.get_shape().as_list()\n",
    "        img = tf.reshape(img, dims)\n",
    "        # if the image is 3 channels, then our convolution\n",
    "        # kernel needs to be repeated for each input channel\n",
    "        W = tf.concat(axis=2, values=[W, W, W])\n",
    "\n",
    "    # Stride is how many values to skip for the dimensions of\n",
    "    # num, height, width, channels\n",
    "    convolved = tf.nn.conv2d(img, W,\n",
    "                             strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return convolved\n",
    "\n",
    "# %% Load up an image:\n",
    "from skimage import data\n",
    "img = data.astronaut()\n",
    "plt.imshow(img)\n",
    "print(img.shape)\n",
    "\n",
    "# %% Now create a placeholder for our graph which can store any input:\n",
    "x = tf.placeholder(tf.float32, shape=img.shape)\n",
    "\n",
    "# %% And a graph which can convolve our image with a gabor\n",
    "out = convolve(x, gabor())\n",
    "\n",
    "# %% Now send the image into the graph and compute the result\n",
    "result = tf.squeeze(out).eval(feed_dict={x: img})\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Summary of tensorflow basics.\n",
    "Parag K. Mital, Jan 2016.\"\"\"\n",
    "# %% Import tensorflow and pyplot\n",
    "\n",
    "# %% tf.Graph represents a collection of tf.Operations\n",
    "# You can create operations by writing out equations.\n",
    "# By default, there is a graph: tf.get_default_graph()\n",
    "# and any new operations are added to this graph.\n",
    "# The result of a tf.Operation is a tf.Tensor, which holds\n",
    "# the values.\n",
    "\n",
    "# %% First a tf.Tensor\n",
    "n_values = 32\n",
    "x = tf.linspace(-3.0, 3.0, n_values)\n",
    "\n",
    "# %% Construct a tf.Session to execute the graph.\n",
    "sess = tf.Session()\n",
    "result = sess.run(x)\n",
    "\n",
    "# %% Alternatively pass a session to the eval fn:\n",
    "x.eval(session=sess)\n",
    "# x.eval() does not work, as it requires a session!\n",
    "\n",
    "# %% We can setup an interactive session if we don't\n",
    "# want to keep passing the session around:\n",
    "sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# %% Now this will work!\n",
    "x.eval()\n",
    "\n",
    "# %% Now a tf.Operation\n",
    "# We'll use our values from [-3, 3] to create a Gaussian Distribution\n",
    "sigma = 1.0\n",
    "mean = 0.0\n",
    "z = (tf.exp(tf.negative(tf.pow(x - mean, 2.0) /\n",
    "                   (2.0 * tf.pow(sigma, 2.0)))) *\n",
    "     (1.0 / (sigma * tf.sqrt(2.0 * 3.1415))))\n",
    "\n",
    "# %% By default, new operations are added to the default Graph\n",
    "assert z.graph is tf.get_default_graph()\n",
    "\n",
    "# %% Execute the graph and plot the result\n",
    "plt.plot(z.eval())\n",
    "\n",
    "# %% We can find out the shape of a tensor like so:\n",
    "print(z.get_shape())\n",
    "\n",
    "# %% Or in a more friendly format\n",
    "print(z.get_shape().as_list())\n",
    "\n",
    "# %% Sometimes we may not know the shape of a tensor\n",
    "# until it is computed in the graph.  In that case\n",
    "# we should use the tf.shape fn, which will return a\n",
    "# Tensor which can be eval'ed, rather than a discrete\n",
    "# value of tf.Dimension\n",
    "print(tf.shape(z).eval())\n",
    "\n",
    "# %% We can combine tensors like so:\n",
    "print(tf.stack([tf.shape(z), tf.shape(z), [3], [4]]).eval())\n",
    "\n",
    "# %% Let's multiply the two to get a 2d gaussian\n",
    "z_2d = tf.matmul(tf.reshape(z, [n_values, 1]), tf.reshape(z, [1, n_values]))\n",
    "\n",
    "# %% Execute the graph and store the value that `out` represents in `result`.\n",
    "plt.imshow(z_2d.eval())\n",
    "\n",
    "# %% For fun let's create a gabor patch:\n",
    "x = tf.reshape(tf.sin(tf.linspace(-3.0, 3.0, n_values)), [n_values, 1])\n",
    "y = tf.reshape(tf.ones_like(x), [1, n_values])\n",
    "z = tf.multiply(tf.matmul(x, y), z_2d)\n",
    "plt.imshow(z.eval())\n",
    "\n",
    "# %% We can also list all the operations of a graph:\n",
    "ops = tf.get_default_graph().get_operations()\n",
    "print([op.name for op in ops])\n",
    "\n",
    "# %% Lets try creating a generic function for computing the same thing:\n",
    "def gabor(n_values=32, sigma=1.0, mean=0.0):\n",
    "    x = tf.linspace(-3.0, 3.0, n_values)\n",
    "    z = (tf.exp(tf.negative(tf.pow(x - mean, 2.0) /\n",
    "                       (2.0 * tf.pow(sigma, 2.0)))) *\n",
    "         (1.0 / (sigma * tf.sqrt(2.0 * 3.1415))))\n",
    "    gauss_kernel = tf.matmul(\n",
    "        tf.reshape(z, [n_values, 1]), tf.reshape(z, [1, n_values]))\n",
    "    x = tf.reshape(tf.sin(tf.linspace(-3.0, 3.0, n_values)), [n_values, 1])\n",
    "    y = tf.reshape(tf.ones_like(x), [1, n_values])\n",
    "    gabor_kernel = tf.multiply(tf.matmul(x, y), gauss_kernel)\n",
    "    return gabor_kernel\n",
    "\n",
    "# %% Confirm this does something:\n",
    "plt.imshow(gabor().eval())\n",
    "\n",
    "# %% And another function which can convolve\n",
    "def convolve(img, W):\n",
    "    # The W matrix is only 2D\n",
    "    # But conv2d will need a tensor which is 4d:\n",
    "    # height x width x n_input x n_output\n",
    "    if len(W.get_shape()) == 2:\n",
    "        dims = W.get_shape().as_list() + [1, 1]\n",
    "        W = tf.reshape(W, dims)\n",
    "\n",
    "    if len(img.get_shape()) == 2:\n",
    "        # num x height x width x channels\n",
    "        dims = [1] + img.get_shape().as_list() + [1]\n",
    "        img = tf.reshape(img, dims)\n",
    "    elif len(img.get_shape()) == 3:\n",
    "        dims = [1] + img.get_shape().as_list()\n",
    "        img = tf.reshape(img, dims)\n",
    "        # if the image is 3 channels, then our convolution\n",
    "        # kernel needs to be repeated for each input channel\n",
    "        W = tf.concat(axis=2, values=[W, W, W])\n",
    "\n",
    "    # Stride is how many values to skip for the dimensions of\n",
    "    # num, height, width, channels\n",
    "    convolved = tf.nn.conv2d(img, W,\n",
    "                             strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return convolved\n",
    "\n",
    "# %% Load up an image:\n",
    "from skimage import data\n",
    "img = data.astronaut()\n",
    "plt.imshow(img)\n",
    "print(img.shape)\n",
    "\n",
    "# %% Now create a placeholder for our graph which can store any input:\n",
    "x = tf.placeholder(tf.float32, shape=img.shape)\n",
    "\n",
    "# %% And a graph which can convolve our image with a gabor\n",
    "out = convolve(x, gabor())\n",
    "\n",
    "# %% Now send the image into the graph and compute the result\n",
    "result = tf.squeeze(out).eval(feed_dict={x: img})\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_filename = 'C:/Tencent_Word2Vec/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "word2vec_filename = 'F:/tencent_word2vec/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "\n",
    "\n",
    "with open(word2vec_filename, encoding='utf8') as file:\n",
    "    line_cnt = 0\n",
    "    for line in file:\n",
    "        line_cnt += 1\n",
    "        print('line num: ', line_cnt, 'line length is ', len(line.strip().split()), 'content: ', line)\n",
    "        if line_cnt >=1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = time.time()\n",
    "\n",
    "print('start loading')\n",
    "wv_from_text = gensim.models.KeyedVectors.load_word2vec_format(word2vec_filename, binary=False)\n",
    "\n",
    "def is_number(n):\n",
    "    try:\n",
    "        float(n) \n",
    "    except ValueError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "print('total cost time: ', time.time()-start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text.init_sims(replace=True)  # 神奇，很省内存，可以运算most_similar\n",
    "\n",
    "def compute_ngrams(word, min_n, max_n):\n",
    "    print('in compute_ngrams')\n",
    "    start_t = time.time()\n",
    "    #BOW, EOW = ('<', '>')  # Used by FastText to attach to all words as prefix and suffix\n",
    "    extended_word =  word\n",
    "    ngrams = []\n",
    "    for ngram_length in range(min_n, min(len(extended_word), max_n) + 1):\n",
    "        for i in range(0, len(extended_word) - ngram_length + 1):\n",
    "            ngrams.append(extended_word[i:i + ngram_length])\n",
    "    print('compute_ngrams ends cost time', time.time()-start_t)\n",
    "    return list(set(ngrams))\n",
    "\n",
    "\n",
    "def wordVec(word,wv_from_text,min_n = 1, max_n = 3):\n",
    "    '''\n",
    "    ngrams_single/ngrams_more,主要是为了当出现oov的情况下,最好先不考虑单字词向量\n",
    "    '''\n",
    "    print('in wordVec')\n",
    "    start_t = time.time()\n",
    "    # 确认词向量维度\n",
    "    word_size = wv_from_text.wv.syn0[0].shape[0]   \n",
    "    # 计算word的ngrams词组\n",
    "    ngrams = compute_ngrams(word,min_n = min_n, max_n = max_n)\n",
    "    # 如果在词典之中，直接返回词向量\n",
    "    if word in wv_from_text.wv.vocab.keys():\n",
    "        return wv_from_text[word]\n",
    "    else:  \n",
    "        # 不在词典的情况下\n",
    "        word_vec = np.zeros(word_size, dtype=np.float32)\n",
    "        ngrams_found = 0\n",
    "        ngrams_single = [ng for ng in ngrams if len(ng) == 1]\n",
    "        ngrams_more = [ng for ng in ngrams if len(ng) > 1]\n",
    "        # 先只接受2个单词长度以上的词向量\n",
    "        for ngram in ngrams_more:\n",
    "            if ngram in wv_from_text.wv.vocab.keys():\n",
    "                word_vec += wv_from_text[ngram]\n",
    "                ngrams_found += 1\n",
    "                #print(ngram)\n",
    "        # 如果，没有匹配到，那么最后是考虑单个词向量\n",
    "        if ngrams_found == 0:\n",
    "            for ngram in ngrams_single:\n",
    "                word_vec += wv_from_text[ngram]\n",
    "                ngrams_found += 1\n",
    "        print('compute_ngrams ends cost time', time.time()-start_t)\n",
    "        if word_vec.any():\n",
    "            return word_vec / max(1, ngrams_found)\n",
    "        else:\n",
    "            raise KeyError('all ngrams for word %s absent from model' % word)\n",
    "\n",
    "print('start WordVec')\n",
    "vec = wordVec('千奇百怪的词向量',wv_from_text,min_n = 1, max_n = 3)  # 词向量获取\n",
    "wv_from_text.most_similar(positive=[vec], topn=10)    # 相似词查找\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'aaa': 3.0, 'bbb': 3.5}\n",
    "r = json.dumps(r)\n",
    "print(r, r[3:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用post方式时，数据放在data或者body中，不能放在url中，放在url中将被忽略。\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "print('start running')\n",
    "\n",
    "\n",
    "def split_all_text(response_text):\n",
    "    idx_right = -1\n",
    "    outcome = {}\n",
    "\n",
    "    while True:\n",
    "        start_idx = idx_right + 1\n",
    "        idx_left = response_text.find('\"', start_idx)\n",
    "        if idx_left==-1:\n",
    "            break\n",
    "\n",
    "        start_idx = idx_left + 1\n",
    "        idx_right = response_text.find('\"', start_idx)\n",
    "        if idx_right==-1:\n",
    "            break\n",
    "\n",
    "        buy_user_id = response_text[idx_left+1:idx_right]\n",
    "\n",
    "        start_idx = idx_right + 1\n",
    "        idx_left = response_text.find('\"', start_idx)\n",
    "        if idx_left==-1:\n",
    "            break\n",
    "\n",
    "        start_idx = idx_left + 1\n",
    "        idx_right = response_text.find('\"', start_idx)\n",
    "        if idx_right==-1:\n",
    "            break\n",
    "\n",
    "        score = float(response_text[idx_left+1:idx_right])\n",
    "        outcome[buy_user_id] = score\n",
    "\n",
    "    return outcome\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# files={'file':open('test.xls','rb')}\n",
    "# # data = {'file': open('./model_outcome_formated.csv_bakup_2019-03-21', 'r')}\n",
    "# # r = requests.post(url_post, data=data)\n",
    "# r = requests.post(url_post, params=data)\n",
    "# r = requests.post(url_post, files=data, headers={'Content-Type':'binary'})\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# start_t = time.time()\n",
    "\n",
    "# url_post = 'http://172.17.30.36:9003/model/v1/upload/csv_model_data'\n",
    "# file_read = open('F:/model_upload/model_outcome_formated.csv_bakup_2019-03-22', 'rb')\n",
    "# data = {'file': file_read}\n",
    "# print('start posting')\n",
    "# r = requests.post(url_post, files=data,\n",
    "#                   headers={'Content-type': 'application/binary', 'Accept': 'text/plain'})\n",
    "# file_read.close()\n",
    "# print('end posting')\n",
    "# print('post r.text is ', r.text)\n",
    "# end_t = time.time()\n",
    "# print('post file cost time: ', end_t-start_t)\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# 99C0972D-B1EC-4000-824F-462791D433F7\n",
    "# AFAC834F-EB7F-4A88-8C9D-3CA9D0523A64\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "url_get_by_ids = 'http://172.17.30.36:9003/callcenter/v1/get_possibility_by_user_ids'\n",
    "user_ids_lst = ['99C0972D-B1EC-4000-824F-462791D433F7', 'AFAC834F-EB7F-4A88-8C9D-3CA9D0523A64']\n",
    "\n",
    "response = requests.get(url_get_by_ids, params={'user_ids': user_ids_lst})\n",
    "print('get by ids response.text', response.text)\n",
    "outcome = split_all_text(response.text)\n",
    "print('outcome is ', outcome)\n",
    "\n",
    "end_t = time.time()\n",
    "print('get by ids cost time: ', end_t-start_t)\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# start_t = time.time()\n",
    "# url_get_all = 'http://172.17.30.36:9003/callcenter/v1/get_all_possibility'\n",
    "\n",
    "# response = requests.get(url_get_all, params={}, timeout=100)\n",
    "# print('get by all response.text ', response.text[:200])\n",
    "\n",
    "# outcome_all = split_all_text(response.text)\n",
    "# print('all outcome len is ', len(outcome_all))\n",
    "# end_t = time.time()\n",
    "# print('get all ids possibilty cost time: ', end_t-start_t)\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df111 = pd.read_csv('F:/model_upload/model_outcome_formated.csv_bakup_2019-03-22')\n",
    "df111.shape\n",
    "# data = {'file': file_read}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(0.9991111, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df111['possibility'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome['99C0972D-B1EC-4000-824F-462791D433F']\n",
    "outcome_all['99C0972D-B1EC-4000-824F-462791D433F7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_features_importance_bar(features, feature_importance):\n",
    "    plt.figure(figsize=(25, 6))\n",
    "    #plt.yscale('log', nonposy='clip')\n",
    "    plt.bar(range(len(feature_importance)), feature_importance, align='center')\n",
    "    plt.xticks(range(len(feature_importance)), features, rotation='vertical')\n",
    "    plt.title('Feature importance')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "df = pd.read_csv('F:/model_upload/lgb_feat_importance_split.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features_importance_bar(df['feature_name'][:20], df['importance'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def split_all_text(response_text):\n",
    "    idx_right = -1\n",
    "    outcome = {}\n",
    "\n",
    "    while True:\n",
    "        start_idx = idx_right + 1\n",
    "        idx_left = response_text.find('\"', start_idx)\n",
    "        if idx_left==-1:\n",
    "            break\n",
    "\n",
    "        start_idx = idx_left + 1\n",
    "        idx_right = response_text.find('\"', start_idx)\n",
    "        if idx_right==-1:\n",
    "            break\n",
    "\n",
    "        buy_user_id = response_text[idx_left+1:idx_right-1]\n",
    "\n",
    "        start_idx = idx_right + 1\n",
    "        idx_left = response_text.find('\"', start_idx)\n",
    "        if idx_left==-1:\n",
    "            break\n",
    "\n",
    "        start_idx = idx_left + 1\n",
    "        idx_right = response_text.find('\"', start_idx)\n",
    "        if idx_right==-1:\n",
    "            break\n",
    "\n",
    "        score = float(response_text[idx_left+1:idx_right-1])\n",
    "        outcome[buy_user_id] = score\n",
    "\n",
    "    return outcome\n",
    "\n",
    "url_get_by_ids = 'http://172.17.30.36:9003/callcenter/v1/get_possibility_by_user_ids'\n",
    "user_ids_lst = ['99C0972D-B1EC-4000-824F-462791D433F7', '9850E2BD-C649-41B7-A543-B6E20D0CF8E2']\n",
    "\n",
    "response = requests.get(url_get_by_ids,\n",
    "                        params={'user_ids':user_ids_lst})\n",
    "print('get by ids response.text', response.text)\n",
    "\n",
    "outcome = split_all_text(response.text)\n",
    "print('outcome is ', outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_get_all = 'http://172.17.30.36:9003/callcenter/v1/get_all_possibility'\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "response = requests.get(url_get_all, params={}, timeout=100)\n",
    "print('get by all finished! ', len(response.text))\n",
    "\n",
    "# print('get by all response.text ', response.text)\n",
    "\n",
    "outcome = split_all_text(response.text)\n",
    "\n",
    "end_t = time.time()\n",
    "print('outcome is ', len(outcome), \n",
    "      'cost time: ', end_t-start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(x,y,t=2):\n",
    "    xs,xn = [],[]\n",
    "    for i in range(t):\n",
    "        mask = y>0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xs.append(x1)\n",
    "\n",
    "    for i in range(t//2):\n",
    "        mask = y==0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xn.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "    xn = np.vstack(xn)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "    yn = np.zeros(xn.shape[0])\n",
    "    x = np.vstack([x,xs,xn])\n",
    "    y = np.concatenate([y,ys,yn])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_datetime_str = time.strftime('%Y-%m-%d 00:00:00', time.localtime(time.time()))\n",
    "current_datetime_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = [\n",
    "    0.4764, 0.4764, 0.4488, 0.4276, 0.3976, 0.3716, 0.3576, 0.3728, 0.3372, 0.35, \n",
    "    0.3176, 0.2892, 0.29, 0.2804, 0.2532, 0.26, 0.2388, 0.2316, 0.2292, 0.2084]\n",
    "\n",
    "ratio_list = [ratio*100.0 for ratio in ratio_list]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "# plt.rcParams['font.sans-serif']=['Microsoft YaHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "\n",
    "\n",
    "# Microsoft YaHei\n",
    "\n",
    "fig=plt.figure(figsize=(16,5))\n",
    " \n",
    "name_list = [str(i) for i in range(20)]\n",
    "rects= plt.bar(range(len(ratio_list)), ratio_list, color='rgby')\n",
    "\n",
    "\n",
    "index = list(range(20))\n",
    "plt.ylim(ymax=70, ymin=0)\n",
    "plt.xticks(index, name_list)\n",
    "plt.ylabel(\"ratio(%)\") #X轴标签\n",
    "for rect in rects:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2, height, str(round(height, 1))+'%', ha='center', va='bottom')\n",
    "    \n",
    "plt.title('回访用户在模型top50000中的分布比例')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "name_list = ['lambda=0', 'lambda=0.05', 'lambda=0.1', 'lambda=0.5']\n",
    "num_list = [52.4, 57.8, 59.1, 54.6]\n",
    "rects=plt.bar(range(len(num_list)), num_list, color='rgby')\n",
    "# X轴标题\n",
    "index=[0,1,2,3]\n",
    "index=[float(c)+0.4 for c in index]\n",
    "plt.ylim(ymax=80, ymin=0)\n",
    "plt.xticks(index, name_list)\n",
    "plt.ylabel(\"arrucay(%)\") #X轴标签\n",
    "for rect in rects:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2, height, str(height)+'%', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances_no_loops(X, X_train):\n",
    "    dists = -2 * np.dot(X, X_train.T) + np.sum(X_train**2,    axis=1) + np.sum(X**2, axis=1)[:, np.newaxis]\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "\n",
    "# #############################################################################\n",
    "# Generating simulated data with Gaussian weights\n",
    "np.random.seed(0)\n",
    "n_samples, n_features = 100, 100\n",
    "X = np.random.randn(n_samples, n_features)  # Create Gaussian data\n",
    "# Create weights with a precision lambda_ of 4.\n",
    "lambda_ = 4.\n",
    "w = np.zeros(n_features)\n",
    "# Only keep 10 weights of interest\n",
    "relevant_features = np.random.randint(0, n_features, 10)\n",
    "for i in relevant_features:\n",
    "    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_))\n",
    "# Create noise with a precision alpha of 50.\n",
    "alpha_ = 50.\n",
    "noise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples)\n",
    "# Create the target\n",
    "y = np.dot(X, w) + noise\n",
    "\n",
    "# #############################################################################\n",
    "# Fit the Bayesian Ridge Regression and an OLS for comparison\n",
    "clf = BayesianRidge(compute_score=True)\n",
    "clf.fit(X, y)\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X, y)\n",
    "\n",
    "# #############################################################################\n",
    "# Plot true weights, estimated weights, histogram of the weights, and\n",
    "# predictions with standard deviations\n",
    "lw = 2\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title(\"Weights of the model\")\n",
    "plt.plot(clf.coef_, color='lightgreen', linewidth=lw,\n",
    "         label=\"Bayesian Ridge estimate\")\n",
    "plt.plot(w, color='gold', linewidth=lw, label=\"Ground truth\")\n",
    "plt.plot(ols.coef_, color='navy', linestyle='--', label=\"OLS estimate\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Values of the weights\")\n",
    "plt.legend(loc=\"best\", prop=dict(size=12))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title(\"Histogram of the weights\")\n",
    "plt.hist(clf.coef_, bins=n_features, color='gold', log=True,\n",
    "         edgecolor='black')\n",
    "plt.scatter(clf.coef_[relevant_features], np.full(len(relevant_features), 5.),\n",
    "            color='navy', label=\"Relevant features\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.xlabel(\"Values of the weights\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title(\"Marginal log-likelihood\")\n",
    "plt.plot(clf.scores_, color='navy', linewidth=lw)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "\n",
    "\n",
    "# Plotting some predictions for polynomial regression\n",
    "def f(x, noise_amount):\n",
    "    y = np.sqrt(x) * np.sin(x)\n",
    "    noise = np.random.normal(0, 1, len(x))\n",
    "    return y + noise_amount * noise\n",
    "\n",
    "\n",
    "degree = 10\n",
    "X = np.linspace(0, 10, 100)\n",
    "y = f(X, noise_amount=0.1)\n",
    "clf_poly = BayesianRidge()\n",
    "clf_poly.fit(np.vander(X, degree), y)\n",
    "\n",
    "X_plot = np.linspace(0, 11, 25)\n",
    "y_plot = f(X_plot, noise_amount=0)\n",
    "y_mean, y_std = clf_poly.predict(np.vander(X_plot, degree), return_std=True)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.errorbar(X_plot, y_mean, y_std, color='navy',\n",
    "             label=\"Polynomial Bayesian Ridge Regression\", linewidth=lw)\n",
    "plt.plot(X_plot, y_plot, color='gold', linewidth=lw,\n",
    "         label=\"Ground Truth\")\n",
    "plt.ylabel(\"Output y\")\n",
    "plt.xlabel(\"Feature X\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "\n",
    "def convert_y(y):\n",
    "    y_new_val_lst = []\n",
    "    bias = 0.099\n",
    "    for y_val in y:\n",
    "#         print('y_val is ', y_val)\n",
    "        if y_val>=0.0:\n",
    "            y_new_val = np.e**(y_val)/1\n",
    "        else:\n",
    "            y_new_val = -np.e**(-1*y_val)/1\n",
    "#         y_new += bias\n",
    "        y_new_val_lst.append(y_new_val)\n",
    "    y_new = np.array(y_new_val_lst).T\n",
    "#     y_new = np.clip(y_new, -20, 20)\n",
    "    return y_new\n",
    "\n",
    "def convert_back_y(y):\n",
    "    y_new_val_lst = []\n",
    "    bias = 0.099\n",
    "    for y_val in y:\n",
    "        if y_val<0:\n",
    "            y_new_val = -np.log(-y_val*1)\n",
    "        else:\n",
    "            y_new_val = np.log(y_val*1)\n",
    "        y_new_val_lst.append(y_new_val)\n",
    "    y_new = np.array(y_new_val_lst).T\n",
    "    return y_new\n",
    "\n",
    "# #############################################################################\n",
    "# Generating simulated data with Gaussian weights\n",
    "print('start generate data!')\n",
    "\n",
    "np.random.seed(0)\n",
    "n_samples, n_features, n_features_used = 10000, 300, 100\n",
    "X = np.random.randn(n_samples, n_features)  # Create Gaussian data\n",
    "# Create weights with a precision lambda_ of 4.\n",
    "\n",
    "w = np.zeros(n_features)\n",
    "# Only keep 10 weights of interest\n",
    "relevant_features = np.random.randint(0, n_features, n_features_used)\n",
    "for i in relevant_features:\n",
    "    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(3))\n",
    "# Create noise with a precision alpha of 50.\n",
    "\n",
    "noise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(80.), size=n_samples)\n",
    "# Create the target\n",
    "y = np.dot(X, w) + noise\n",
    "print('before y.shape is ', y.shape)\n",
    "y = convert_y(y)\n",
    "print('after y.shape is ', y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print('generat num finished!')\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(len(y)), np.sort(y))\n",
    "plt.title(\"value distrubition of Loyalty score\")\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('Loyalty Score', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'target'\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(len(y)), np.sort(y))\n",
    "plt.title(\"value distrubition of Loyalty score\")\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('Loyalty Score', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_origial = convert_back_y(y_train)\n",
    "\n",
    "# #############################################################################\n",
    "# Fit the Bayesian Ridge Regression and an OLS for comparison\n",
    "brr = BayesianRidge(compute_score=True)\n",
    "brr.fit(X_train, y_train)\n",
    "y_pred_brr = brr.predict(X_test)\n",
    "\n",
    "print('brr with no convert rmse val is ', np.sqrt(mean_squared_error(y_pred_brr, y_test)))\n",
    "\n",
    "brr = BayesianRidge(compute_score=True)\n",
    "brr.fit(X_train, y_train_origial)\n",
    "y_pred_brr = brr.predict(X_test)\n",
    "y_pred_brr_coverted = convert_back_y(y_pred_brr)\n",
    "\n",
    "print('brr with convert rmse val is ', np.sqrt(mean_squared_error(y_pred_brr_coverted, y_test)))\n",
    "\n",
    "print('clf fit finished!')\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "y_pred_ols = ols.predict(X_test)\n",
    "print('ols rmse val is ', np.sqrt(mean_squared_error(y_pred_ols, y_test)))\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train_origial)\n",
    "y_pred_ols = ols.predict(X_test)\n",
    "y_pred_ols_coverted = convert_back_y(y_pred_ols)\n",
    "print('ols with convert rmse val is ', np.sqrt(mean_squared_error(y_pred_ols_coverted, y_test)))\n",
    "\n",
    "print('ols fit finished!')\n",
    "param_regressor = {\n",
    "         'num_leaves': 101,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.007,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         'random_state': 2019\n",
    "}\n",
    "\n",
    "trn_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# print('start regressor training...')\n",
    "# lgb_model = lgb.train(param_regressor, trn_data, 500, verbose_eval=100)   \n",
    "# y_pred_lgb = lgb_model.predict(X_test)\n",
    "# print('lgb rmse val is ', np.sqrt(mean_squared_error(y_pred_lgb, y_test)))\n",
    "\n",
    "# clf rmse val is  589956.9420349462\n",
    "# clf fit finished!\n",
    "# ols rmse val is  795307.2592917023\n",
    "# ols fit finished!\n",
    "\n",
    "4292015.400922055\n",
    "2063346.6911373537\n",
    "clf fit finished!\n",
    "ols rmse val is  53089918.734005354\n",
    "ols with convert rmse val is  2063346.6911366559\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_regressor = {\n",
    "         'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.007,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         'random_state': 2019\n",
    "}\n",
    "\n",
    "X_train_lgb, X_val_lgb, y_train_lgb, y_val_lgb = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "trn_data = lgb.Dataset(X_train_lgb, label=y_train_lgb)\n",
    "val_data = lgb.Dataset(X_val_lgb, label=y_val_lgb)\n",
    "\n",
    "num_round = 2000\n",
    "print('start regressor training...')\n",
    "lgb_model = lgb.train(param_regressor, trn_data, num_round, valid_sets = [trn_data, val_data], \n",
    "                verbose_eval=100, early_stopping_rounds = 200)\n",
    "y_pred_lgb = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
    "print('lgb rmse val is ', np.sqrt(mean_squared_error(y_pred_lgb, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.81*1.052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = stats.norm.rvs(loc=0, scale=1. /5, size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.mean(), vv.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.3/4.7-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "53.6/42.1-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32.64*1.042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12*18992.3/208000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': 1.,\n",
    "                    'B': pd.Timestamp('20130102'),\n",
    "                    'C': [100., 50., -30., -50.]})\n",
    "\n",
    "        \n",
    "df2 = pd.DataFrame({'A': 1.,\n",
    "                  'B': pd.Timestamp('20130102'),\n",
    "                  'C': pd.Series(1, index=list(range(4)), dtype='float32')}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_func(df1, df2):\n",
    "    for df in (df1, df2):\n",
    "        df['c_new111'] = df['C']*0.77\n",
    "new_func(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3], [2, 3, 5]])\n",
    "X_train = np.array([1, 2, 3])\n",
    "\n",
    "print('X.shape is', X.shape, 'X_train.shape is', X_train.shape)\n",
    "# results = compute_distances_no_loops(X, X_train)\n",
    "\n",
    "print(X-X_train)\n",
    "\n",
    "np.linalg.norm(X-X_train, axis=1, ord=2), np.sqrt(6)\n",
    "\n",
    "result = np.linalg.norm(X-X_train, axis=1, ord=2)\n",
    "print(result==result.min())\n",
    "\n",
    "# results\n",
    "# from numpy.linalg import norm\n",
    "# norm(X-(X_train[:, np.newaxis]), axis=0, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [(1160, 7792), (693,2375), (369,961), \n",
    "       (301,871), (311,802), (218,656), (221,521)]\n",
    "fork_num_lst = list(map(lambda x: x[1], lst))\n",
    "print(fork_num_lst)\n",
    "\n",
    "# fig = plt.figure(1) \n",
    "\n",
    "# #figure对象的add_axes()可以在其中创建一个axes对象,\n",
    "# # add_axes()的参数为一个形如[left, bottom, width, height]的列表,取值范围在0与1之间;\n",
    "# ax = fig.add_axes([0.1, 0.5, 0.8, 0.5]) # 我们把它放在了figure图形的上半部分，对应参数分别为：left, bottom, width, height;\n",
    "# ax.set_xlabel('time')     #用axes对象的set_xlabel函数来设置它的xlabel\n",
    "\n",
    "# line =ax.plot(range(5))[0]  #用axes对象的plot()进行绘图,它返回一个Line2D的对象;\n",
    "# line.set_color('r')             # 再调用Line2D的对象的set_color函数设置color的属性;\n",
    "# plt.show()\n",
    "\n",
    "plt.plot(fork_num_lst)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([('bird', 389.0),\n",
    "                   ('bird', 24.0),\n",
    "                   ('mammal', 80.5),\n",
    "                   ('mammal', np.nan)],\n",
    "                   index=['falcon', 'parrot', 'lion', 'monkey'],\n",
    "                   columns=('class', 'max_speed'))\n",
    "\n",
    "# >>> df\n",
    "#          class  max_speed\n",
    "# falcon    bird      389.0\n",
    "# parrot    bird       24.0\n",
    "# lion    mammal       80.5\n",
    "# monkey  mammal        NaN\n",
    "df['aaa'] = [4, 5, 6, 8]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.reset_index(name='new_col')\n",
    "\n",
    "# (df.groupby('class')\n",
    "#           .size()\n",
    "#           .reset_index(name='transactions_count'))\n",
    "\n",
    "x = np.arange(6).reshape((2,3))\n",
    "\n",
    "x = np.array([[0, -100], [30, 22], [11, 44]])\n",
    "\n",
    "print('x is', x)\n",
    "np.ptp(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(data))\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "print(scaler.mean_)\n",
    "print(scaler.var_)\n",
    "print(scaler.transform(data))\n",
    "print(scaler.transform([[2, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_lst = [0.001, 0.01, 0.1]\n",
    "alpha_1_lst = [1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01]\n",
    "alpha_2_lst = [1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01]\n",
    "lambda_1_lst = [1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01]\n",
    "lambda_2_lst = [1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01]\n",
    "normalize_lst = [True, False]\n",
    "\n",
    "import itertools\n",
    "itertools.combinations(tol_lst,2)\n",
    "\n",
    "import itertools\n",
    "a = [[1,2,3],[4,5,6],[7,8,9,10]] # 3*3*4\n",
    "ll = list(itertools.product(*a))\n",
    "ll = list(itertools.product(tol_lst, alpha_1_lst, normalize_lst))\n",
    "\n",
    "for tol, alpha, normalize in itertools.product(tol_lst, alpha_1_lst, normalize_lst):\n",
    "    print('tol ', tol, 'alpha ', alpha, 'normalize ', normalize)\n",
    "# print(ll)\n",
    "print('len of ll is ', len(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "# https://datawhatnow.com/feature-importance/\n",
    "# https://github.com/Microsoft/LightGBM/issues/826\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "data_path = 'F:/liver_disease_patients_intelligent_segmentation/data/'\n",
    "output_path = 'F:/liver_disease_patients_intelligent_segmentation/output/'\n",
    "\n",
    "print('data_path is ', data_path)\n",
    "print('output_path is ', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_features_importance_bar(features, feature_importance):\n",
    "    df_feat_importance = pd.DataFrame({\n",
    "            'column': features,\n",
    "            'importance': feature_importance,\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "    df_feat_importance.to_csv(output_path + '/df_feat_importance.csv', index=0, sep='\\t')\n",
    "\n",
    "    plt.figure(figsize=(25, 6))\n",
    "    #plt.yscale('log', nonposy='clip')\n",
    "    plt.bar(range(len(feature_importance)), feature_importance, align='center')\n",
    "    plt.xticks(range(len(feature_importance)), features, rotation='vertical')\n",
    "    plt.title('Feature importance')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def convert_2_md5(value):\n",
    "    return hashlib.md5(str(value).encode('utf-8')).hexdigest()\n",
    "    # return hashlib.md5(str(value)).hexdigest()\n",
    "\n",
    "def split_by_user_id(df_merged, train_ratio=0.67):\n",
    "    df_merged['md5_val'] = df_merged['buy_user_id'].apply(convert_2_md5)\n",
    "    # df_merged['md5_val'] = df_merged['md5_val'].apply(str)\n",
    "    print('df_merged.dtypes is ', df_merged.dtypes)\n",
    "\n",
    "    df_user_id_md5_val = pd.DataFrame()\n",
    "    df_user_id_md5_val['md5_val'] = df_merged['md5_val'].sort_values(ascending=True)\n",
    "\n",
    "    # df_merged_sorted = df_merged.sort_values(by=['md5_val'])\n",
    "    # print('df_merged_sorted head is ', df_merged_sorted.head(5))\n",
    "\n",
    "    # df_merged_sorted.to_csv('./data/hive_sql_merged_instances_sorted.csv', \n",
    "    #     sep='\\t', date_format='%Y/%m/%d', index=0)  # date_format='%Y-%m-%d %H:%M:%s'\n",
    "    train_num = int(df_merged.shape[0]*train_ratio)\n",
    "    print('train_num is ', train_num)\n",
    "    pivot_val = df_user_id_md5_val['md5_val'][train_num]\n",
    "\n",
    "    print('train_num is: ', train_num, 'pivot_val is: ', pivot_val,\n",
    "          'type of pivot_val is', type(pivot_val))\n",
    "\n",
    "    # df_merged_train = df_merged_sorted[df_merged_sorted['md5_val']<=pivot_val]\n",
    "    # df_merged_test = df_merged_sorted[df_merged_sorted['md5_val']>pivot_val]\n",
    "\n",
    "    df_merged_train = df_merged.loc[df_merged['md5_val'] <= pivot_val]\n",
    "    df_merged_test = df_merged.loc[df_merged['md5_val'] > pivot_val]\n",
    "\n",
    "    print('after split df_merged_train.shape is ', df_merged_train.shape,\n",
    "          'df_merged_test.shape is ', df_merged_test.shape)\n",
    "\n",
    "    # df_merged_train = df_merged_sorted[:train_num]\n",
    "    # df_merged_test = df_merged_sorted[train_num:]\n",
    "\n",
    "    # df_merged_train.to_csv('./data/hive_sql_merged_instances_train.csv', sep='\\t', index=0)\n",
    "    # df_merged_test.to_csv('./data/hive_sql_merged_instances_test.csv', sep='\\t', index=0)\n",
    "\n",
    "    return df_merged_train, df_merged_test\n",
    "\n",
    "\n",
    "def compute_density_multiple(y_true, y_predict, threshold=10, by_percentage=True, top=True):\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_predict': y_predict})\n",
    "    df.sort_values(by=['y_predict'], ascending=False, inplace=True)\n",
    "\n",
    "    density_whole = sum(df['y_true'])/df.shape[0]\n",
    "    if by_percentage:\n",
    "        if top:\n",
    "            df_target = df[:int(threshold*0.01*df.shape[0])]\n",
    "        else:\n",
    "            df_target = df[-int(threshold*0.01*df.shape[0]):]\n",
    "    else:\n",
    "        if top:\n",
    "            df_target = df[:threshold]\n",
    "        else:\n",
    "            df_target = df[-threshold:]\n",
    "    density_partial = sum(df_target['y_true'])/df_target.shape[0]\n",
    "    density_mutiple = density_partial/density_whole\n",
    "    return density_mutiple\n",
    "\n",
    "def generate_real_test_df_quick():\n",
    "    df_test_real = pd.read_csv(data_path + '/hive_sql_unassigned_buyuser_output_processed.csv',\n",
    "                               sep='\\t')\n",
    "    print('df_test_real.shape is ', df_test_real.shape)\n",
    "    return df_test_real\n",
    "\n",
    "def generate_real_test_df():\n",
    "    print('in generate_real_test_df')\n",
    "\n",
    "    df_test_real = pd.read_csv(data_path + '/hql_liver_merged_visits_instances_df_data.csv',\n",
    "                               parse_dates=[1], infer_datetime_format=True,\n",
    "                               names=['buy_user_id', 'creation_date', 'is_pos'])\n",
    "\n",
    "    print('df_test_real.shape is ', df_test_real.shape)\n",
    "\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###------ 加上F的feature  ----- ###\n",
    "    ###----------------------------###\n",
    "    df_frequency = pd.read_csv('./unassigned/data/hql_liver_F_data.csv', parse_dates=[1], infer_datetime_format=True)\n",
    "    df_test_real = pd.merge(df_test_real, df_frequency, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_test_real.shape after add frequency is ', df_test_real.shape)\n",
    "    del df_frequency\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###------ 加上M的feature  -----###\n",
    "    ###----------------------------###\n",
    "    df_monetary = pd.read_csv('./unassigned/data/hive_sql_M_data.csv', parse_dates=[1], infer_datetime_format=True)\n",
    "    df_test_real = pd.merge(df_test_real, df_monetary, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_test_real.shape after add monetary is ', df_test_real.shape)\n",
    "    del df_monetary\n",
    "\n",
    "    ###---------------------------------###\n",
    "    ###   加上first order的features      ###\n",
    "    ###   包含：                         ###\n",
    "    ###   1. 订单距离电话回访时间的天数    ###\n",
    "    ###   2. 订单金额                    ###\n",
    "    ###   3. 订单来源                    ###\n",
    "    ###   4. 订单支付方式                ###\n",
    "    ###---------------------------------###\n",
    "    df_first_order = pd.read_csv('./unassigned/data/hive_sql_first_order_data.csv', \n",
    "                                 parse_dates=[1, 2], infer_datetime_format=True,\n",
    "                                 dtype={'first_origin_type': str, 'first_payment_type': str})\n",
    "    df_first_order['gap_days_first_order'] = (df_first_order['creation_date'] - df_first_order['order_dt']).dt.days\n",
    "    df_first_order.drop(['order_dt'], axis=1, inplace=True)\n",
    "    df_test_real = pd.merge(df_test_real, df_first_order, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_test_real.shape after add first order is ', df_test_real.shape)\n",
    "    del df_first_order\n",
    "\n",
    "    ###---------------------------------###\n",
    "    ###   加上last order的features       ###\n",
    "    ###   包含：                         ###\n",
    "    ###   1. 订单距离电话回访时间的天数    ###\n",
    "    ###   2. 订单金额                    ###\n",
    "    ###   3. 订单来源                    ###\n",
    "    ###   4. 订单支付方式                ###\n",
    "    ###---------------------------------###\n",
    "    df_last_order = pd.read_csv('./unassigned/data/hive_sql_last_order_data.csv', \n",
    "                                parse_dates=[1, 2], infer_datetime_format=True,\n",
    "                                dtype={'last_origin_type': str, 'last_payment_type': str})\n",
    "    df_last_order['gap_days_last_order'] = (df_last_order['creation_date'] - df_last_order['order_dt']).dt.days\n",
    "    df_last_order.drop(['order_dt'], axis=1, inplace=True)\n",
    "    df_test_real = pd.merge(df_test_real, df_last_order, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_test_real.shape after add last order is ', df_test_real.shape)\n",
    "    del df_last_order\n",
    "\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###--- 收货地址省份的feature  --###\n",
    "    ###----------------------------###\n",
    "    df_address = pd.read_csv('./unassigned/data/hive_sql_address_data.csv', dtype={'rand_address_code': str})\n",
    "    df_address.rename(columns={'rand_address_code':'address_code'}, inplace = True)\n",
    "    # df_address['address_code'] = df_address['rand_address_code'].apply(str)\n",
    "    # df_address.drop(['rand_address_code'], axis=1, inplace=True)\n",
    "    df_test_real = pd.merge(df_test_real, df_address, how='left', on=['buy_user_id'])\n",
    "    print('df_test_real.shape after add address code is ', df_test_real.shape)\n",
    "    del df_address\n",
    "\n",
    "\n",
    "    ###----------------------------------------------------------###\n",
    "    ###------ 加上class_code 和 branch_code的feature -------------###\n",
    "    ###----------------------------------------------------------###\n",
    "    df_class_code = pd.read_csv('./unassigned/data/hive_sql_patient_class_data.csv', \n",
    "                                dtype={'class_code': str, 'branch_code': str})\n",
    "    df_test_real = pd.merge(df_test_real, df_class_code, how='left', on=['buy_user_id'])\n",
    "    print('df_test_real.shape after add class_code, branch_code code is ', df_test_real.shape)\n",
    "    del df_class_code\n",
    "\n",
    "    ###----------------------------------------------------------###\n",
    "    ###------ 加上电话回访时间所在的月份的feature -----------------###\n",
    "    ###----------------------------------------------------------###\n",
    "    df_test_real['call_month'] = df_test_real['creation_date'].dt.month.apply(str)\n",
    "    df_test_real['call_weekday'] = df_test_real['creation_date'].dt.weekday.apply(str)\n",
    "    print('df_test_real.shape after add call_month call_weekday is ', df_test_real.shape)\n",
    "\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###--- 收货地址个数的feature  --###\n",
    "    ###----------------------------###\n",
    "    df_address_num = pd.read_csv('./unassigned/data/hive_sql_address_num_data.csv')\n",
    "    df_merged = pd.merge(df_test_real, df_address_num, how='left', on=['buy_user_id'])\n",
    "    print('df_test_real.shape after add address number feature is ', df_test_real.shape)\n",
    "    # print('df_test_real.dtypes after add address number feature is ', df_test_real.dtypes)\n",
    "    del df_address_num\n",
    "\n",
    "    start_t = time.time()\n",
    "    df_test_real.to_csv('./unassigned/data/hive_sql_unassigned_buyuser_output_processed', \n",
    "                        index=False, sep='\\t')    \n",
    "    print('df_test_real store cost time:', time.time()-start_t)\n",
    "\n",
    "    return df_test_real\n",
    "\n",
    "def get_training_data_quick():\n",
    "    print('in get_training_data_quick()')\n",
    "    start_t = time.time()\n",
    "    df_merged = pd.read_csv(data_path + '/df_merged_processed.csv')\n",
    "    print('read df_merged from csv cost time: ', time.time()-start_t,\n",
    "          'df_merged.shape:', df_merged.shape)\n",
    "    return df_merged\n",
    "\n",
    "def get_training_data():\n",
    "    print('in get_training_data()')\n",
    "    df_merged = pd.read_csv(data_path + '/hql_liver_merged_visits_instances_df_data.csv',\n",
    "                            parse_dates=[1], infer_datetime_format=True)\n",
    "\n",
    "    # df_merged['is_pos']= pd.to_numeric(df_merged['is_pos'], downcast='integer', errors='coerce')\n",
    "    # df_merged['row_num'] = list(np.array(list(range(len(df_merged)))))\n",
    "    # print('the row_num is ', df_merged.loc[df_merged.is_pos.isnull()]['row_num'])\n",
    "#     print('df_merged.head(20) is ', df_merged.head(20))\n",
    "    print(df_merged.isnull().sum())\n",
    "    print('df_merged.shape is ', df_merged.shape)\n",
    "    # df_merged['is_pos'] = df_merged['is_pos'].astype(np.int8)\n",
    "                            # dtype={'is_pos': np.int8})\n",
    "    # df_merged['is_pos'] = df_merged['is_pos'].map({'True': 1, 'False': 0})\n",
    "    # df_merged['is_pos'] = df_merged['is_pos'].astype(np.int32)\n",
    "\n",
    "    #抽样做训练集\n",
    "    sample_num = 800000\n",
    "    df_merged = df_merged.sample(n=sample_num, random_state=42)\n",
    "    print('df_merged shape is ', df_merged.shape)\n",
    "    print('df_merged dtypes is ', df_merged.dtypes)\n",
    "\n",
    "    # return df_merged\n",
    "\n",
    "    # split_by_user_id(df_merged)\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###------ 加上R的feature  -----###\n",
    "    ###----------------------------###\n",
    "    # df_recency = pd.read_csv('./data/hive_sql_R_data.csv', parse_dates=[1, 2], infer_datetime_format=True)\n",
    "    # df_recency = pd.read_csv('./data/hive_sql_R_data.csv')\n",
    "    # df_recency['creation_date'] = pd.to_datetime(df_recency['creation_date'], \n",
    "    #     format='%Y-%m-%d %H:%M:%S', errors='ignore')\n",
    "    # df_recency['recency_date'] = pd.to_datetime(df_recency['recency_date'], \n",
    "    #     format='%Y-%m-%d %H:%M:%S', errors='ignore')\n",
    "\n",
    "    # df_recency['gap_days'] = (df_recency['creation_date'] - df_recency['recency_date']).dt.days\n",
    "    # df_merged = pd.merge(df_merged, df_recency, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    # df_merged.drop(['recency_date'], axis=1, inplace=True)\n",
    "    # print('df_merged.shape after add R is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add R is ', df_merged.dtypes)\n",
    "\n",
    "    # df_merged.drop(['gap_days'], axis=1, inplace=True)\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###------ 加上F的feature  ----- ###\n",
    "    ###----------------------------###\n",
    "    df_frequency = pd.read_csv(data_path + '/hql_liver_F_data.csv', parse_dates=[1],\n",
    "                               infer_datetime_format=True)\n",
    "    df_merged = pd.merge(df_merged, df_frequency, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_merged.shape after add frequency is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add frequency is ', df_merged.dtypes)\n",
    "    del df_frequency\n",
    "\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###------ 加上M的feature  -----###\n",
    "    ###----------------------------###\n",
    "    df_monetary = pd.read_csv(data_path + '/hql_liver_M_data.csv', parse_dates=[1],\n",
    "                              infer_datetime_format=True)\n",
    "    df_merged = pd.merge(df_merged, df_monetary, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_merged.shape after add monetary is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add monetary is ', df_merged.dtypes)\n",
    "    del df_monetary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###---------------------------------###\n",
    "    ###   加上first order的features      ###\n",
    "    ###   包含：                         ###\n",
    "    ###   1. 订单距离电话回访时间的天数    ###\n",
    "    ###   2. 订单金额                    ###\n",
    "    ###   3. 订单来源                    ###\n",
    "    ###   4. 订单支付方式                ###\n",
    "    ###---------------------------------###\n",
    "    df_first_order = pd.read_csv(data_path + '/hql_liver_first_order_data.csv',\n",
    "                                 parse_dates=[1, 2], infer_datetime_format=True,\n",
    "                                 dtype={'first_origin_type': str, 'first_payment_type': str})\n",
    "    df_first_order['gap_days_first_order'] = (df_first_order['creation_date'] - df_first_order['order_dt']).dt.days\n",
    "    df_first_order.drop(['order_dt'], axis=1, inplace=True)\n",
    "    df_merged = pd.merge(df_merged, df_first_order, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_merged.shape after add first order is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add first order is ', df_merged.dtypes)\n",
    "    del df_first_order\n",
    "\n",
    "\n",
    "\n",
    "    ###---------------------------------###\n",
    "    ###   加上last order的features       ###\n",
    "    ###   包含：                         ###\n",
    "    ###   1. 订单距离电话回访时间的天数    ###\n",
    "    ###   2. 订单金额                    ###\n",
    "    ###   3. 订单来源                    ###\n",
    "    ###   4. 订单支付方式                ###\n",
    "    ###---------------------------------###\n",
    "    df_last_order = pd.read_csv(data_path + '/hql_liver_last_order_data.csv',\n",
    "                                parse_dates=[1, 2], infer_datetime_format=True,\n",
    "                                dtype={'last_origin_type': str, 'last_payment_type': str})\n",
    "    df_last_order['gap_days_last_order'] = (df_last_order['creation_date'] - df_last_order['order_dt']).dt.days\n",
    "    df_last_order.drop(['order_dt'], axis=1, inplace=True)\n",
    "    df_merged = pd.merge(df_merged, df_last_order, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_merged.shape after add last order is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add last order is ', df_merged.dtypes)\n",
    "    del df_last_order\n",
    "\n",
    "\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###--- 收货地址省份的feature  --###\n",
    "    ###----------------------------###\n",
    "    df_address = pd.read_csv(data_path + '/hql_liver_address_data.csv', dtype={'rand_address_code': str})\n",
    "    df_address.rename(columns={'rand_address_code':'address_code'}, inplace = True)\n",
    "    # df_address['address_code'] = df_address['rand_address_code'].apply(str)\n",
    "    # df_address.drop(['rand_address_code'], axis=1, inplace=True)\n",
    "    df_merged = pd.merge(df_merged, df_address, how='left', on=['buy_user_id'])\n",
    "    print('df_merged.shape after add address code is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add address code is ', df_merged.dtypes)\n",
    "    del df_address\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###----------------------------------------------------------###\n",
    "    ###------ 加上class_code 和 branch_code的feature -------------###\n",
    "    ###----------------------------------------------------------###\n",
    "    # df_class_code = pd.read_csv('./data/hive_sql_patient_class_data.csv',\n",
    "    #                             dtype={'class_code': str, 'branch_code': str})\n",
    "    # # df_class_code['class_code'] = df_class_code['class_code'].apply(str)\n",
    "    # # df_class_code['branch_code'] = df_class_code['branch_code'].apply(str)\n",
    "    # df_merged = pd.merge(df_merged, df_class_code, how='left', on=['buy_user_id'])\n",
    "    # print('df_merged.shape after add class_code, branch_code code is ', df_merged.shape)\n",
    "    # # print('df_merged.dtypes after add class_code, branch_code code is ', df_merged.dtypes)\n",
    "    # del df_class_code\n",
    "\n",
    "\n",
    "    ###----------------------------------------------------------###\n",
    "    ###------ 加上start_app count的feature -----------------------###\n",
    "    ###----------------------------------------------------------###\n",
    "    # df_start_app_cnt = pd.read_csv('./data/hive_sql_startapp_cnt_data.csv')\n",
    "    # df_start_app_cnt.rename(columns={'cnt':'start_app_cnt'}, inplace = True)\n",
    "    # df_merged = pd.merge(df_merged, df_start_app_cnt, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    # print('df_merged.shape after add start_app count, branch_code code is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add start_app count, branch_code code is ', df_merged.dtypes)\n",
    "    # del df_start_app_cnt\n",
    "\n",
    "\n",
    "    ###----------------------------------------------------------###\n",
    "    ###------ 加上电话回访时间所在的月份的feature -----------------###\n",
    "    ###----------------------------------------------------------###\n",
    "    df_merged['call_month'] = df_merged['creation_date'].dt.month.apply(str)\n",
    "    df_merged['call_weekday'] = df_merged['creation_date'].dt.weekday.apply(str)\n",
    "    print('df_merged.shape after add start_app count, branch_code code is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add start_app count, branch_code code is ', df_merged.dtypes)\n",
    "\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###--- 收货地址个数的feature  --###\n",
    "    ###----------------------------###\n",
    "    df_address_num = pd.read_csv(data_path + '/hql_liver_address_num_data.csv')\n",
    "    df_merged = pd.merge(df_merged, df_address_num, how='left', on=['buy_user_id'])\n",
    "    print('df_merged.shape after add address number feature is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add address number feature is ', df_merged.dtypes)\n",
    "\n",
    "    del df_address_num\n",
    "\n",
    "    print('after all preprocessing, final df_merged.dtypes is ', df_merged.dtypes)\n",
    "\n",
    "    start_t = time.time()\n",
    "    df_merged.to_csv(data_path + '/df_merged_processed.csv', index=False)\n",
    "    print('store df_merged to_csv cost time: ', time.time()-start_t)\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "def training_model(df_merged, category_feas_onehot=False):\n",
    "    if category_feas_onehot:\n",
    "        print('pre get_dummies, df_merged.dtypes: ', df_merged.dtypes)\n",
    "        df_merged = pd.get_dummies(df_merged,\n",
    "                                   columns=['address_code', 'call_month', 'call_weekday',\n",
    "                                            'first_payment_type', 'first_origin_type',\n",
    "                                            'last_payment_type', 'last_origin_type',\n",
    "                                            'first_order_status', 'last_order_status'])\n",
    "                                   # sparse=True)\n",
    "\n",
    "        # print('after get_dummies, df_merged.dtypes: ', df_merged.dtypes)\n",
    "\n",
    "    df_merged_train, df_merged_test = split_by_user_id(df_merged, train_ratio=0.67)\n",
    "    del df_merged\n",
    "\n",
    "    df_merged_train.drop(['buy_user_id', 'creation_date', 'md5_val'], axis=1, inplace=True)\n",
    "    df_merged_test.drop(['buy_user_id', 'creation_date', 'md5_val'], axis=1, inplace=True)\n",
    "\n",
    "    print('df_merged_train.shape df_merged_test.shape: ', df_merged_train.shape, df_merged_test.shape)\n",
    "\n",
    "    df_train_y = df_merged_train['is_pos']\n",
    "    df_train_X = df_merged_train.drop(['is_pos'], axis=1)\n",
    "\n",
    "    df_test_y = df_merged_test['is_pos']\n",
    "    df_test_X = df_merged_test.drop(['is_pos'], axis=1)\n",
    "\n",
    "    feature_names = df_train_X.columns.tolist()\n",
    "\n",
    "    if category_feas_onehot:\n",
    "        d_train = lgb.Dataset(df_train_X.values, label=df_train_y.values, feature_name=feature_names)\n",
    "    else:\n",
    "        d_train = lgb.Dataset(df_train_X.values, label=df_train_y.values, feature_name=feature_names,\n",
    "                              categorical_feature=['address_code', 'call_month', 'call_weekday',\n",
    "                                                   'first_payment_type', 'first_origin_type',\n",
    "                                                   'last_payment_type', 'last_origin_type',\n",
    "                                                   'first_order_status', 'last_order_status'])\n",
    "\n",
    "    # branch_code, class_code\n",
    "    # y_pred.shape is  (263994,)\n",
    "    # auc_score is  0.8494185939558818 predict cost time: 5.660183906555176\n",
    "\n",
    "    params = {'learning_rate': 0.08, 'boosting_type': 'gbdt', 'objective': 'binary',\n",
    "              'metric': 'binary_logloss', 'sub_feature': 0.85, 'sub_sample': 0.7,\n",
    "              'num_leaves': 100, 'min_data': 400, 'max_depth': 13, 'random_state': 42}\n",
    "\n",
    "    # lgbm = lgb.LGBMClassifier(n_estimators=500, n_jobs=-1, learning_rate=0.08,\n",
    "    #                          random_state=42, max_depth=13, min_child_samples=400,\n",
    "    #                          num_leaves=100, subsample=0.7, colsample_bytree=0.85,\n",
    "    #                          silent=-1, verbose=-1, boosting_type='gbdt')\n",
    "\n",
    "    print('lgb training starts')\n",
    "    start_t = time.time()\n",
    "    clf = lgb.train(params, d_train, 500)\n",
    "    print('lgb training ends, cost time', time.time() - start_t)\n",
    "\n",
    "    start_t = time.time()\n",
    "    y_pred = clf.predict(df_test_X.values)\n",
    "    print('y_pred.shape is ', y_pred.shape)\n",
    "    auc_score = roc_auc_score(df_test_y, y_pred)\n",
    "    print('auc_score is ', auc_score, 'predict cost time:', time.time() - start_t)\n",
    "    #\n",
    "    # print('top 200 ratio_multiple is',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=200, by_percentage=False),\n",
    "    #       'top 500 ratio_multiple is',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=500, by_percentage=False),\n",
    "    #       'ratio_multiple top 1 is ',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=1),\n",
    "    #       'ratio_multiple top 5 is ',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=5),\n",
    "    #       'ratio_multiple top 10 is ',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=10),\n",
    "    #       'ratio_multiple top 20 is ',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=20),\n",
    "    #       'ratio_multiple top 30 is',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=30)\n",
    "    # )\n",
    "\n",
    "    importance = clf.feature_importance(importance_type='split')\n",
    "    feature_name = clf.feature_name()\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature_name': feature_name,\n",
    "        'importance': importance}\n",
    "    ).sort_values(by='importance', ascending=False)\n",
    "    feature_importance.to_csv(output_path + '/lgb_feat_importance_split.csv', index=False)\n",
    "\n",
    "    show_features_importance_bar(feature_importance['feature_name'][:25],\n",
    "                                 feature_importance['importance'][:25])\n",
    "    return feature_importance\n",
    "\n",
    "def align_test_data_with_train_data(test_df, train_feas_columns):\n",
    "    missing_cols = set(train_feas_columns) - set(test_df.columns)\n",
    "    for c in missing_cols:\n",
    "        test_df[c] = 0\n",
    "    test_df = test_df[train_feas_columns]\n",
    "    return test_df\n",
    "\n",
    "\n",
    "df_merged = get_training_data()\n",
    "# df_merged = get_training_data_quick()\n",
    "print('df_merged.shape is ', df_merged.shape)\n",
    "feature_importance_out = training_model(df_merged, category_feas_onehot=True)\n",
    "print('program ends')\n",
    "\n",
    "# importance = clf.feature_importance(importance_type='gain')\n",
    "# feature_name = clf.feature_name()\n",
    "# feature_importance = pd.DataFrame({\n",
    "#                          'feature_name':feature_name,\n",
    "#                          'importance':importance}\n",
    "#                      ).sort_values(by='importance', ascending=False)\n",
    "# feature_importance.to_csv('./model_output/lgb_feat_importance_gain.csv',index=False)\n",
    "\n",
    "# plt.figure(figsize=(12,6))\n",
    "# lgb.plot_importance(clf, max_num_features=30,  importance_type='split')\n",
    "# plt.show()\n",
    "# plt.savefig('./lgbm_importances.png')\n",
    "#\n",
    "# del df_train_y, df_train_X, df_test_y, df_test_X, df_merged_train, df_merged_test\n",
    "# gc.collect()\n",
    "#\n",
    "#\n",
    "# ###########################################################################################\n",
    "\n",
    "# df_test_real = generate_real_test_df()\n",
    "# print('finished!')\n",
    "\n",
    "# df_test_real = generate_real_test_df_quick()\n",
    "# print('after generate_real_test_df df_test_real shape is ', df_test_real.shape)\n",
    "#\n",
    "# final_outcome = pd.DataFrame()\n",
    "# final_outcome['buy_user_id'] = df_test_real['buy_user_id']\n",
    "# df_test_real.drop(['buy_user_id', 'creation_date'], axis=1, inplace=True)\n",
    "#\n",
    "# print('start real_test!!! ')\n",
    "# start_t = time.time()\n",
    "# y_pred=clf.predict(df_test_real.values)\n",
    "# final_outcome['y'] = y_pred\n",
    "# # final_outcome['y'] = np.round(y_pred, 8)\n",
    "# print('y_pred.shape is ', y_pred.shape, 'df_test_real.shape is ', df_test_real.shape)\n",
    "# print('final predict cost time:', time.time()-start_t)\n",
    "#\n",
    "# final_outcome.sort_values(by='y', ascending=False, inplace=True)\n",
    "# final_outcome.to_csv('./model_output/final_outcome.csv', index=False, sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_out['importance'][:7].sum()/feature_importance_out['importance'].sum()\n",
    "\n",
    "# (1079808, 269) (249612, 269)\n",
    "\n",
    "373705 + 755964\n",
    "\n",
    "#  (199751, 19) df_merged.shape is  (1129669, 19)\n",
    "    \n",
    "199751/(199751 + 1129669)\n",
    "\n",
    "\n",
    "y_pred.shape is  (198221,) model predict cost time:  4.6558146476745605\n",
    "predict auc_score is  0.8541811797702418\n",
    "get outcome finished!\n",
    "program ends\n",
    "\n",
    "# y_pred.shape is  (373705,)\n",
    "# auc_score is  0.8534907375048523 predict cost time: 9.141904592514038\n",
    "# pickle dump finished, cost time:  0.16290831565856934\n",
    "\n",
    "# df_check.shape is  (198221, 19) df_merged.shape is  (1131199, 19)\n",
    "\n",
    "#   (754824, 263) df_merged_test.shape is  (376375, 263)\n",
    "754824 + 376375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_importance_out_bakup = feature_importance_out.copy()\n",
    "\n",
    "feature_importance_out['feature_name'][:7]\n",
    "\n",
    "def show_features_importance_bar_new(features, feature_importance):\n",
    "    df_feat_importance = pd.DataFrame({\n",
    "            'column': features,\n",
    "            'importance': feature_importance,\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "    df_feat_importance.to_csv(output_path + '/df_feat_importance.csv', index=0, sep='\\t')\n",
    "\n",
    "    plt.figure(figsize=(25, 6))\n",
    "    #plt.yscale('log', nonposy='clip')\n",
    "    plt.bar(range(len(feature_importance)), feature_importance, align='center')\n",
    "    plt.xticks(range(len(feature_importance)), features, rotation='vertical')\n",
    "    plt.title('Feature importance')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    feature_importance_out\n",
    "feature_importance_out['importance'] = feature_importance_out_bakup['importance']/feature_importance_out_bakup['importance'].sum()\n",
    "show_features_importance_bar_new(feature_importance_out['feature_name'][:25],\n",
    "                                 feature_importance_out['importance'][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "667395/(667395+132605)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 bayes ridge regression回归\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE_TSG)\n",
    "# folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "\n",
    "oof_regressor_brr = np.zeros(len(df_train))\n",
    "brr_predictions = np.zeros(len(df_test))\n",
    "\n",
    "def norm_df(df):\n",
    "    print('in norm_df')\n",
    "    df_norm = (df - df.mean()) / (df.max() - df.min())\n",
    "    return df_norm\n",
    "\n",
    "def fill_with_mean(df, feas):\n",
    "    print('in fill_with_mean')\n",
    "    for fea in feas:\n",
    "        df[fea].fillna(df[fea].mean(), inplace=True)\n",
    "    return df\n",
    "    \n",
    "start = time.time()\n",
    "# brr = BayesianRidge(n_iter=1000, verbose=True) \n",
    "brr = BayesianRidge(n_iter=1000, tol=1e-6, alpha_1=1e-05, alpha_2=1e-05, verbose=True) \n",
    "\n",
    "df_train_brr = df_train.copy()\n",
    "df_test_brr = df_test.copy()\n",
    "\n",
    "# df_train_brr = df_train_brr.fillna(0.0)\n",
    "# df_test_brr = df_test_brr.fillna(0.0)\n",
    "\n",
    "print('start fill with mean')\n",
    "\n",
    "df_train_brr = fill_with_mean(df_train_brr, features)\n",
    "df_test_brr = fill_with_mean(df_test_brr, features)\n",
    "\n",
    "print('start norm')\n",
    "\n",
    "df_train_brr[features] = norm_df(df_train_brr[features])\n",
    "df_test_brr[features] = norm_df(df_test_brr[features])\n",
    "\n",
    "print('ending norm')\n",
    "\n",
    "print('start bayes ridge regression training')\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, df_train['outliers'].values)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    brr.fit(df_train_brr.iloc[trn_idx][features], df_train_brr['target'].iloc[trn_idx])\n",
    "    oof_regressor_brr[val_idx] = brr.predict(df_train_brr.iloc[val_idx][features])\n",
    "    brr_predictions += brr.predict(df_test_brr[features].values) / 5.\n",
    "    print('final rmse val is ', np.sqrt(mean_squared_error(df_train_brr.iloc[val_idx]['target'].values, \n",
    "                                                           oof_regressor_brr[val_idx])))\n",
    "    \n",
    "print('final brr rmse val is ', np.sqrt(mean_squared_error(df_train_brr['target'].values, oof_regressor_brr)))\n",
    "\n",
    "# start norm\n",
    "# ending norm\n",
    "# start bayes ridge regression training\n",
    "# fold n°0\n",
    "# final rmse val is  3.806076950362962\n",
    "# fold n°1\n",
    "# final rmse val is  3.779403705505596\n",
    "# fold n°2\n",
    "# final rmse val is  150.34690570346518\n",
    "# fold n°3\n",
    "# final rmse val is  3.7754369642818877\n",
    "# fold n°4\n",
    "# final rmse val is  4.4753798374860585\n",
    "# final brr rmse val is  67.33055964373135\n",
    "\n",
    "#################################################\n",
    "# fill with 0 normal\n",
    "# fold n°0\n",
    "# final rmse val is  3.773954411258298\n",
    "# fold n°1\n",
    "# final rmse val is  3.759998900810395\n",
    "# fold n°2\n",
    "# final rmse val is  3.7582380865818834\n",
    "# fold n°3\n",
    "# final rmse val is  3.7558625132092125\n",
    "# fold n°4\n",
    "# final rmse val is  3.7688611690916343\n",
    "# final brr rmse val is  3.7633893292642013\n",
    "\n",
    "################################################\n",
    "# fill with mean normal\n",
    "\n",
    "# start bayes ridge regression training\n",
    "# fold n°0\n",
    "# final rmse val is  3.7732907463257748\n",
    "# fold n°1\n",
    "# final rmse val is  3.758590156304969\n",
    "# fold n°2\n",
    "# final rmse val is  3.7565954722634456\n",
    "# fold n°3\n",
    "# final rmse val is  3.7551300147222824\n",
    "# fold n°4\n",
    "# final rmse val is  3.767392908342587\n",
    "# final brr rmse val is  3.7622063927908376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.fillna(X_test.mean())\n",
    "\n",
    "X_test = np.array([[1, 2, np.nan], [2, np.nan, 3], [4, 5, 7], [np.nan, 3, 3]])\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n",
    "                  columns=['A', 'B', 'C'])\n",
    "\n",
    "df\n",
    "df.iat[2, 2] = 'abc'\n",
    "print(df)\n",
    "print(type(df.iat[2, 2]), df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime_num(num):\n",
    "    import math\n",
    "    for i in range(2, int(math.sqrt(num))):\n",
    "        if num%i==0:\n",
    "            print('i is ', i)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "for num in range(100000, 200000, 1):\n",
    "    if is_prime_num(num):\n",
    "        print('num is ', num)\n",
    "        break\n",
    "        \n",
    "# is_prime_num(10007)\n",
    "print(is_prime_num(2333))\n",
    "print(is_prime_num(2652124))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold, KFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [3, 9]])\n",
    "y = np.array([0, 0, 1, 1, 0])\n",
    "rkf = RepeatedKFold(n_splits=3, n_repeats=2, random_state=2652124)\n",
    "kf = KFold(n_splits=3, random_state=2652124)\n",
    "# for train_index, test_index in rkf.split(X):\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [77, 99], [88, 22], [11, 88]])\n",
    "y = np.array([0, 1, 1, 0, 0, 1, 0])\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=36851234)\n",
    "for train_index, test_index in rskf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print('#####################')\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, random_state=42)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print('#####################')\n",
    "    \n",
    "skf = StratifiedKFold(n_splits=3, random_state=21)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print('#####################')\n",
    "    \n",
    "skf = StratifiedKFold(n_splits=3, random_state=2019)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Operations\n",
    "\n",
    "\n",
    "| Operation     | Description                       || Operation     | Description                          |\n",
    "|---------------|-----------------------------------||---------------|--------------------------------------|\n",
    "| ``a == b``    | ``a`` equal to ``b``              || ``a != b``    | ``a`` not equal to ``b``             |\n",
    "| ``a < b``     | ``a`` less than ``b``             || ``a > b``     | ``a`` greater than ``b``             |\n",
    "| ``a <= b``    | ``a`` less than or equal to ``b`` || ``a >= b``    | ``a`` greater than or equal to ``b`` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a silly question. Of course it's an error. \n",
    "\n",
    "But what about..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Click the \"output\" button to see the answer)\n",
    "\n",
    "Python has precedence rules that determine the order in which operations get evaluated in expressions like above. For example, `and` has a higher precedence than `or`, which is why the first expression above is `True`. If we had evaluated it from left to right, we would have calculated `True or True` first (which is `True`), and then taken the `and` of that result with `False`, giving a final value of `False`.\n",
    "\n",
    "You could try to [memorize the order of precedence](https://docs.python.org/3/reference/expressions.html#operator-precedence), but a safer bet is to just use liberal parentheses. Not only does this help prevent bugs, it makes your intentions clearer to anyone who reads your code. \n",
    "\n",
    "For example, consider the following expression:\n",
    "\n",
    "```python\n",
    "prepared_for_weather = have_umbrella or rain_level < 5 and have_hood or not rain_level > 0 and is_workday\n",
    "```\n",
    "\n",
    "I'm trying to say that I'm safe from today's weather....\n",
    "- if I have an umbrella...\n",
    "- or if the rain isn't too heavy and I have a hood...\n",
    "- otherwise, I'm still fine unless it's raining *and* it's a workday\n",
    "\n",
    "But not only is my Python code hard to read, it has a bug. We can address both problems by adding some parentheses:\n",
    "\n",
    "```python\n",
    "for i in range(100):\n",
    "    sum += i\n",
    "    print(sum)\n",
    "prepared_for_weather = have_umbrella or (rain_level < 5 and have_hood) or not (rain_level > 0 and is_workday)\n",
    "```\n",
    "\n",
    "You can add even more parentheses if you think it helps readability:\n",
    "\n",
    "```python\n",
    "prepared_for_weather = have_umbrella or ((rain_level < 5) and have_hood) or (not (rain_level > 0 and is_workday))\n",
    "```\n",
    "\n",
    "We can also split it over multiple lines to emphasize the 3-part structure described above:\n",
    "\n",
    "```python\n",
    "prepared_for_weather = (\n",
    "    have_umbrella \n",
    "    or ((rain_level < 5) and have_hood) \n",
    "    or (not (rain_level > 0 and is_workday))\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
