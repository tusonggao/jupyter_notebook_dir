{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Python Course Home Page](https://www.kaggle.com/learn/python)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-182d7c366c99>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\n",
      "mnist.train.num_examples 55000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "\n",
    "mnist = read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(type(mnist))\n",
    "print('mnist.train.num_examples', mnist.train.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 28*28 # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\")\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\")\n",
    "    logits = neuron_layer(hidden2, n_outputs, \"outputs\")    \n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is  0\n",
      "X_batch.shape is  (50, 784) y_batch.shape is  (50, 10)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "labels must be 1-D, but got shape [50,10]\n\t [[Node: loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dnn/outputs/add, _arg_y_0_1/_3)]]\n\nCaused by op 'loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"E:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"E:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"E:\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"E:\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"E:\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"E:\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"E:\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"E:\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"E:\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"E:\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"E:\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"E:\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"E:\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-ac0e2416ffc5>\", line 29, in <module>\n    labels=y, logits=logits)\n  File \"E:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 2063, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"E:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 8062, in sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"E:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"E:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"E:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"E:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): labels must be 1-D, but got shape [50,10]\n\t [[Node: loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dnn/outputs/add, _arg_y_0_1/_3)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: labels must be 1-D, but got shape [50,10]\n\t [[Node: loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dnn/outputs/add, _arg_y_0_1/_3)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f6dfb28a7c18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m             print('X_batch.shape is ', X_batch.shape,\n\u001b[0;32m     11\u001b[0m                   'y_batch.shape is ', y_batch.shape)\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0macc_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1289\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1291\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: labels must be 1-D, but got shape [50,10]\n\t [[Node: loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dnn/outputs/add, _arg_y_0_1/_3)]]\n\nCaused by op 'loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"E:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"E:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"E:\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"E:\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"E:\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"E:\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"E:\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"E:\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"E:\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"E:\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"E:\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"E:\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"E:\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"E:\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-ac0e2416ffc5>\", line 29, in <module>\n    labels=y, logits=logits)\n  File \"E:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 2063, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"E:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 8062, in sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"E:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"E:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"E:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"E:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): labels must be 1-D, but got shape [50,10]\n\t [[Node: loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dnn/outputs/add, _arg_y_0_1/_3)]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 400\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('epoch is ', epoch)\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            print('X_batch.shape is ', X_batch.shape,\n",
    "                  'y_batch.shape is ', y_batch.shape)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                            y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.2224 - acc: 0.9341\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.0982 - acc: 0.9697\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0706 - acc: 0.9780\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0548 - acc: 0.9827\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.0452 - acc: 0.9857\n",
      "10000/10000 [==============================] - 1s 55us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06197628720855573, 0.9825]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ann\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron(max_iter=100, tol=-np.infty, random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "sess.run(init)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(1000, 4, input_length=10))\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be\n",
    "# no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "assert output_array.shape == (32, 10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*3600 + 46*60 + 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "print(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = time.time()\n",
    "\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    print('theta_value.shape is ', theta_value.shape)\n",
    "print('theta_value is ', theta_value, 'cost time', time.time()-start_t)\n",
    "\n",
    "b = np.array([[2,3],[4,5]])\n",
    "print(np.linalg.inv(b))\n",
    "\n",
    "b_t = tf.constant(b, dtype=tf.float32, name='b_t')\n",
    "b_t = tf.matrix_inverse(b_t)\n",
    "with tf.Session() as sess:\n",
    "    b_t_value = b_t.eval()\n",
    "    print('b_t_value is ', b_t_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = time.time()\n",
    "\n",
    "X = np.c_[np.ones((m, 1)), housing.data]\n",
    "y = housing.target.reshape(-1, 1)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "theta = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X), X)), np.transpose(X)), y)\n",
    "\n",
    "# print('theta.shape is', theta.shape)\n",
    "# X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "# y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "# XT = tf.transpose(X)\n",
    "# theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "# with tf.Session() as sess:\n",
    "#     theta_value = theta.eval()\n",
    "#     print('theta_value.shape is ', theta_value.shape)\n",
    "\n",
    "print('theta.shape is ', theta.shape, 'cost time', time.time()-start_t)\n",
    "print('theta.value is ', theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(1,5).reshape(2,2)\n",
    "\n",
    "b = np.arange(5,9).reshape(2,2)\n",
    "\n",
    "print(np.linalg.inv(np.dot(a,b)))\n",
    "\n",
    "a = tf.constant(a, dtype=tf.float32, name='a')\n",
    "b = tf.constant(b, dtype=tf.float32, name='b')\n",
    "c = tf.matmul(a, b)\n",
    "c_i = tf.matrix_inverse(c)\n",
    "with tf.Session() as sess:\n",
    "#     c_value = c.eval()\n",
    "    c_i_value = c_i.eval()\n",
    "    print('c_i_value is', c_i_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_filename = 'C:/Tencent_Word2Vec/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'\n",
    "\n",
    "with open(word2vec_filename, encoding='utf8') as file:\n",
    "    line_cnt = 0\n",
    "    for line in file:\n",
    "        line_cnt += 1\n",
    "        print('line num: ', line_cnt, 'line length is ', len(line.strip().split()), 'content: ', line)\n",
    "        if line_cnt >=1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = time.time()\n",
    "\n",
    "print('start loading')\n",
    "wv_from_text = gensim.models.KeyedVectors.load_word2vec_format(word2vec_filename, binary=False)\n",
    "\n",
    "def is_number(n):\n",
    "    try:\n",
    "        float(n) \n",
    "    except ValueError:\n",
    "        return False\n",
    "    return True\n",
    "# is_number('-357t')\n",
    "\n",
    "print('total cost time: ', time.time()-start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text.init_sims(replace=True)  # 神奇，很省内存，可以运算most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngrams(word, min_n, max_n):\n",
    "    print('in compute_ngrams')\n",
    "    start_t = time.time()\n",
    "    #BOW, EOW = ('<', '>')  # Used by FastText to attach to all words as prefix and suffix\n",
    "    extended_word =  word\n",
    "    ngrams = []\n",
    "    for ngram_length in range(min_n, min(len(extended_word), max_n) + 1):\n",
    "        for i in range(0, len(extended_word) - ngram_length + 1):\n",
    "            ngrams.append(extended_word[i:i + ngram_length])\n",
    "    print('compute_ngrams ends cost time', time.time()-start_t)\n",
    "    return list(set(ngrams))\n",
    "\n",
    "\n",
    "def wordVec(word,wv_from_text,min_n = 1, max_n = 3):\n",
    "    '''\n",
    "    ngrams_single/ngrams_more,主要是为了当出现oov的情况下,最好先不考虑单字词向量\n",
    "    '''\n",
    "    print('in wordVec')\n",
    "    start_t = time.time()\n",
    "    # 确认词向量维度\n",
    "    word_size = wv_from_text.wv.syn0[0].shape[0]   \n",
    "    # 计算word的ngrams词组\n",
    "    ngrams = compute_ngrams(word,min_n = min_n, max_n = max_n)\n",
    "    # 如果在词典之中，直接返回词向量\n",
    "    if word in wv_from_text.wv.vocab.keys():\n",
    "        return wv_from_text[word]\n",
    "    else:  \n",
    "        # 不在词典的情况下\n",
    "        word_vec = np.zeros(word_size, dtype=np.float32)\n",
    "        ngrams_found = 0\n",
    "        ngrams_single = [ng for ng in ngrams if len(ng) == 1]\n",
    "        ngrams_more = [ng for ng in ngrams if len(ng) > 1]\n",
    "        # 先只接受2个单词长度以上的词向量\n",
    "        for ngram in ngrams_more:\n",
    "            if ngram in wv_from_text.wv.vocab.keys():\n",
    "                word_vec += wv_from_text[ngram]\n",
    "                ngrams_found += 1\n",
    "                #print(ngram)\n",
    "        # 如果，没有匹配到，那么最后是考虑单个词向量\n",
    "        if ngrams_found == 0:\n",
    "            for ngram in ngrams_single:\n",
    "                word_vec += wv_from_text[ngram]\n",
    "                ngrams_found += 1\n",
    "        print('wordVec ends cost time', time.time()-start_t)\n",
    "        if word_vec.any():\n",
    "            return word_vec / max(1, ngrams_found)\n",
    "        else:\n",
    "            raise KeyError('all ngrams for word %s absent from model' % word)\n",
    "\n",
    "print('start WordVec')\n",
    "vec = wordVec('千奇百怪的词向量',wv_from_text,min_n = 1, max_n = 3)  # 词向量获取\n",
    "wv_from_text.most_similar(positive=[vec], topn=10)    # 相似词查找\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = wordVec('涂松高',wv_from_text,min_n = 1, max_n = 3)\n",
    "wv_from_text.most_similar(positive=[vec], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'aaa': 3.0, 'bbb': 3.5}\n",
    "r = json.dumps(r)\n",
    "print(r, r[3:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用post方式时，数据放在data或者body中，不能放在url中，放在url中将被忽略。\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "print('start running')\n",
    "\n",
    "\n",
    "def split_all_text(response_text):\n",
    "    idx_right = -1\n",
    "    outcome = {}\n",
    "\n",
    "    while True:\n",
    "        start_idx = idx_right + 1\n",
    "        idx_left = response_text.find('\"', start_idx)\n",
    "        if idx_left==-1:\n",
    "            break\n",
    "\n",
    "        start_idx = idx_left + 1\n",
    "        idx_right = response_text.find('\"', start_idx)\n",
    "        if idx_right==-1:\n",
    "            break\n",
    "\n",
    "        buy_user_id = response_text[idx_left+1:idx_right]\n",
    "\n",
    "        start_idx = idx_right + 1\n",
    "        idx_left = response_text.find('\"', start_idx)\n",
    "        if idx_left==-1:\n",
    "            break\n",
    "\n",
    "        start_idx = idx_left + 1\n",
    "        idx_right = response_text.find('\"', start_idx)\n",
    "        if idx_right==-1:\n",
    "            break\n",
    "\n",
    "        score = float(response_text[idx_left+1:idx_right])\n",
    "        outcome[buy_user_id] = score\n",
    "\n",
    "    return outcome\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# files={'file':open('test.xls','rb')}\n",
    "# # data = {'file': open('./model_outcome_formated.csv_bakup_2019-03-21', 'r')}\n",
    "# # r = requests.post(url_post, data=data)\n",
    "# r = requests.post(url_post, params=data)\n",
    "# r = requests.post(url_post, files=data, headers={'Content-Type':'binary'})\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# start_t = time.time()\n",
    "\n",
    "# url_post = 'http://172.17.30.36:9003/model/v1/upload/csv_model_data'\n",
    "# file_read = open('F:/model_upload/model_outcome_formated.csv_bakup_2019-03-22', 'rb')\n",
    "# data = {'file': file_read}\n",
    "# print('start posting')\n",
    "# r = requests.post(url_post, files=data,\n",
    "#                   headers={'Content-type': 'application/binary', 'Accept': 'text/plain'})\n",
    "# file_read.close()\n",
    "# print('end posting')\n",
    "# print('post r.text is ', r.text)\n",
    "# end_t = time.time()\n",
    "# print('post file cost time: ', end_t-start_t)\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# 99C0972D-B1EC-4000-824F-462791D433F7\n",
    "# AFAC834F-EB7F-4A88-8C9D-3CA9D0523A64\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "url_get_by_ids = 'http://172.17.30.36:9003/callcenter/v1/get_possibility_by_user_ids'\n",
    "user_ids_lst = ['99C0972D-B1EC-4000-824F-462791D433F7', 'AFAC834F-EB7F-4A88-8C9D-3CA9D0523A64']\n",
    "\n",
    "response = requests.get(url_get_by_ids, params={'user_ids': user_ids_lst})\n",
    "print('get by ids response.text', response.text)\n",
    "outcome = split_all_text(response.text)\n",
    "print('outcome is ', outcome)\n",
    "\n",
    "end_t = time.time()\n",
    "print('get by ids cost time: ', end_t-start_t)\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# start_t = time.time()\n",
    "# url_get_all = 'http://172.17.30.36:9003/callcenter/v1/get_all_possibility'\n",
    "\n",
    "# response = requests.get(url_get_all, params={}, timeout=100)\n",
    "# print('get by all response.text ', response.text[:200])\n",
    "\n",
    "# outcome_all = split_all_text(response.text)\n",
    "# print('all outcome len is ', len(outcome_all))\n",
    "# end_t = time.time()\n",
    "# print('get all ids possibilty cost time: ', end_t-start_t)\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df111 = pd.read_csv('F:/model_upload/model_outcome_formated.csv_bakup_2019-03-22')\n",
    "df111.shape\n",
    "# data = {'file': file_read}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(0.9991111, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df111['possibility'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome['99C0972D-B1EC-4000-824F-462791D433F']\n",
    "outcome_all['99C0972D-B1EC-4000-824F-462791D433F7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_features_importance_bar(features, feature_importance):\n",
    "    plt.figure(figsize=(25, 6))\n",
    "    #plt.yscale('log', nonposy='clip')\n",
    "    plt.bar(range(len(feature_importance)), feature_importance, align='center')\n",
    "    plt.xticks(range(len(feature_importance)), features, rotation='vertical')\n",
    "    plt.title('Feature importance')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "df = pd.read_csv('F:/model_upload/lgb_feat_importance_split.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features_importance_bar(df['feature_name'][:20], df['importance'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def split_all_text(response_text):\n",
    "    idx_right = -1\n",
    "    outcome = {}\n",
    "\n",
    "    while True:\n",
    "        start_idx = idx_right + 1\n",
    "        idx_left = response_text.find('\"', start_idx)\n",
    "        if idx_left==-1:\n",
    "            break\n",
    "\n",
    "        start_idx = idx_left + 1\n",
    "        idx_right = response_text.find('\"', start_idx)\n",
    "        if idx_right==-1:\n",
    "            break\n",
    "\n",
    "        buy_user_id = response_text[idx_left+1:idx_right-1]\n",
    "\n",
    "        start_idx = idx_right + 1\n",
    "        idx_left = response_text.find('\"', start_idx)\n",
    "        if idx_left==-1:\n",
    "            break\n",
    "\n",
    "        start_idx = idx_left + 1\n",
    "        idx_right = response_text.find('\"', start_idx)\n",
    "        if idx_right==-1:\n",
    "            break\n",
    "\n",
    "        score = float(response_text[idx_left+1:idx_right-1])\n",
    "        outcome[buy_user_id] = score\n",
    "\n",
    "    return outcome\n",
    "\n",
    "url_get_by_ids = 'http://172.17.30.36:9003/callcenter/v1/get_possibility_by_user_ids'\n",
    "user_ids_lst = ['99C0972D-B1EC-4000-824F-462791D433F7', '9850E2BD-C649-41B7-A543-B6E20D0CF8E2']\n",
    "\n",
    "response = requests.get(url_get_by_ids,\n",
    "                        params={'user_ids':user_ids_lst})\n",
    "print('get by ids response.text', response.text)\n",
    "\n",
    "outcome = split_all_text(response.text)\n",
    "print('outcome is ', outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_get_all = 'http://172.17.30.36:9003/callcenter/v1/get_all_possibility'\n",
    "\n",
    "start_t = time.time()\n",
    "\n",
    "response = requests.get(url_get_all, params={}, timeout=100)\n",
    "print('get by all finished! ', len(response.text))\n",
    "\n",
    "# print('get by all response.text ', response.text)\n",
    "\n",
    "outcome = split_all_text(response.text)\n",
    "\n",
    "end_t = time.time()\n",
    "print('outcome is ', len(outcome), \n",
    "      'cost time: ', end_t-start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(x,y,t=2):\n",
    "    xs,xn = [],[]\n",
    "    for i in range(t):\n",
    "        mask = y>0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xs.append(x1)\n",
    "\n",
    "    for i in range(t//2):\n",
    "        mask = y==0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xn.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "    xn = np.vstack(xn)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "    yn = np.zeros(xn.shape[0])\n",
    "    x = np.vstack([x,xs,xn])\n",
    "    y = np.concatenate([y,ys,yn])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_datetime_str = time.strftime('%Y-%m-%d 00:00:00', time.localtime(time.time()))\n",
    "current_datetime_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = [\n",
    "    0.4764, 0.4764, 0.4488, 0.4276, 0.3976, 0.3716, 0.3576, 0.3728, 0.3372, 0.35, \n",
    "    0.3176, 0.2892, 0.29, 0.2804, 0.2532, 0.26, 0.2388, 0.2316, 0.2292, 0.2084]\n",
    "\n",
    "ratio_list = [ratio*100.0 for ratio in ratio_list]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "# plt.rcParams['font.sans-serif']=['Microsoft YaHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "\n",
    "\n",
    "# Microsoft YaHei\n",
    "\n",
    "fig=plt.figure(figsize=(16,5))\n",
    " \n",
    "name_list = [str(i) for i in range(20)]\n",
    "rects= plt.bar(range(len(ratio_list)), ratio_list, color='rgby')\n",
    "\n",
    "\n",
    "index = list(range(20))\n",
    "plt.ylim(ymax=70, ymin=0)\n",
    "plt.xticks(index, name_list)\n",
    "plt.ylabel(\"ratio(%)\") #X轴标签\n",
    "for rect in rects:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2, height, str(round(height, 1))+'%', ha='center', va='bottom')\n",
    "    \n",
    "plt.title('回访用户在模型top50000中的分布比例')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "name_list = ['lambda=0', 'lambda=0.05', 'lambda=0.1', 'lambda=0.5']\n",
    "num_list = [52.4, 57.8, 59.1, 54.6]\n",
    "rects=plt.bar(range(len(num_list)), num_list, color='rgby')\n",
    "# X轴标题\n",
    "index=[0,1,2,3]\n",
    "index=[float(c)+0.4 for c in index]\n",
    "plt.ylim(ymax=80, ymin=0)\n",
    "plt.xticks(index, name_list)\n",
    "plt.ylabel(\"arrucay(%)\") #X轴标签\n",
    "for rect in rects:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2, height, str(height)+'%', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances_no_loops(X, X_train):\n",
    "    dists = -2 * np.dot(X, X_train.T) + np.sum(X_train**2,    axis=1) + np.sum(X**2, axis=1)[:, np.newaxis]\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "\n",
    "# #############################################################################\n",
    "# Generating simulated data with Gaussian weights\n",
    "np.random.seed(0)\n",
    "n_samples, n_features = 100, 100\n",
    "X = np.random.randn(n_samples, n_features)  # Create Gaussian data\n",
    "# Create weights with a precision lambda_ of 4.\n",
    "lambda_ = 4.\n",
    "w = np.zeros(n_features)\n",
    "# Only keep 10 weights of interest\n",
    "relevant_features = np.random.randint(0, n_features, 10)\n",
    "for i in relevant_features:\n",
    "    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_))\n",
    "# Create noise with a precision alpha of 50.\n",
    "alpha_ = 50.\n",
    "noise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples)\n",
    "# Create the target\n",
    "y = np.dot(X, w) + noise\n",
    "\n",
    "# #############################################################################\n",
    "# Fit the Bayesian Ridge Regression and an OLS for comparison\n",
    "clf = BayesianRidge(compute_score=True)\n",
    "clf.fit(X, y)\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X, y)\n",
    "\n",
    "# #############################################################################\n",
    "# Plot true weights, estimated weights, histogram of the weights, and\n",
    "# predictions with standard deviations\n",
    "lw = 2\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title(\"Weights of the model\")\n",
    "plt.plot(clf.coef_, color='lightgreen', linewidth=lw,\n",
    "         label=\"Bayesian Ridge estimate\")\n",
    "plt.plot(w, color='gold', linewidth=lw, label=\"Ground truth\")\n",
    "plt.plot(ols.coef_, color='navy', linestyle='--', label=\"OLS estimate\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Values of the weights\")\n",
    "plt.legend(loc=\"best\", prop=dict(size=12))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title(\"Histogram of the weights\")\n",
    "plt.hist(clf.coef_, bins=n_features, color='gold', log=True,\n",
    "         edgecolor='black')\n",
    "plt.scatter(clf.coef_[relevant_features], np.full(len(relevant_features), 5.),\n",
    "            color='navy', label=\"Relevant features\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.xlabel(\"Values of the weights\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title(\"Marginal log-likelihood\")\n",
    "plt.plot(clf.scores_, color='navy', linewidth=lw)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "\n",
    "\n",
    "# Plotting some predictions for polynomial regression\n",
    "def f(x, noise_amount):\n",
    "    y = np.sqrt(x) * np.sin(x)\n",
    "    noise = np.random.normal(0, 1, len(x))\n",
    "    return y + noise_amount * noise\n",
    "\n",
    "\n",
    "degree = 10\n",
    "X = np.linspace(0, 10, 100)\n",
    "y = f(X, noise_amount=0.1)\n",
    "clf_poly = BayesianRidge()\n",
    "clf_poly.fit(np.vander(X, degree), y)\n",
    "\n",
    "X_plot = np.linspace(0, 11, 25)\n",
    "y_plot = f(X_plot, noise_amount=0)\n",
    "y_mean, y_std = clf_poly.predict(np.vander(X_plot, degree), return_std=True)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.errorbar(X_plot, y_mean, y_std, color='navy',\n",
    "             label=\"Polynomial Bayesian Ridge Regression\", linewidth=lw)\n",
    "plt.plot(X_plot, y_plot, color='gold', linewidth=lw,\n",
    "         label=\"Ground Truth\")\n",
    "plt.ylabel(\"Output y\")\n",
    "plt.xlabel(\"Feature X\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "\n",
    "def convert_y(y):\n",
    "    y_new_val_lst = []\n",
    "    bias = 0.099\n",
    "    for y_val in y:\n",
    "#         print('y_val is ', y_val)\n",
    "        if y_val>=0.0:\n",
    "            y_new_val = np.e**(y_val)/1\n",
    "        else:\n",
    "            y_new_val = -np.e**(-1*y_val)/1\n",
    "#         y_new += bias\n",
    "        y_new_val_lst.append(y_new_val)\n",
    "    y_new = np.array(y_new_val_lst).T\n",
    "#     y_new = np.clip(y_new, -20, 20)\n",
    "    return y_new\n",
    "\n",
    "def convert_back_y(y):\n",
    "    y_new_val_lst = []\n",
    "    bias = 0.099\n",
    "    for y_val in y:\n",
    "        if y_val<0:\n",
    "            y_new_val = -np.log(-y_val*1)\n",
    "        else:\n",
    "            y_new_val = np.log(y_val*1)\n",
    "        y_new_val_lst.append(y_new_val)\n",
    "    y_new = np.array(y_new_val_lst).T\n",
    "    return y_new\n",
    "\n",
    "# #############################################################################\n",
    "# Generating simulated data with Gaussian weights\n",
    "print('start generate data!')\n",
    "\n",
    "np.random.seed(0)\n",
    "n_samples, n_features, n_features_used = 10000, 300, 100\n",
    "X = np.random.randn(n_samples, n_features)  # Create Gaussian data\n",
    "# Create weights with a precision lambda_ of 4.\n",
    "\n",
    "w = np.zeros(n_features)\n",
    "# Only keep 10 weights of interest\n",
    "relevant_features = np.random.randint(0, n_features, n_features_used)\n",
    "for i in relevant_features:\n",
    "    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(3))\n",
    "# Create noise with a precision alpha of 50.\n",
    "\n",
    "noise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(80.), size=n_samples)\n",
    "# Create the target\n",
    "y = np.dot(X, w) + noise\n",
    "print('before y.shape is ', y.shape)\n",
    "y = convert_y(y)\n",
    "print('after y.shape is ', y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print('generat num finished!')\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(len(y)), np.sort(y))\n",
    "plt.title(\"value distrubition of Loyalty score\")\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('Loyalty Score', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'target'\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(len(y)), np.sort(y))\n",
    "plt.title(\"value distrubition of Loyalty score\")\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('Loyalty Score', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_origial = convert_back_y(y_train)\n",
    "\n",
    "# #############################################################################\n",
    "# Fit the Bayesian Ridge Regression and an OLS for comparison\n",
    "brr = BayesianRidge(compute_score=True)\n",
    "brr.fit(X_train, y_train)\n",
    "y_pred_brr = brr.predict(X_test)\n",
    "\n",
    "print('brr with no convert rmse val is ', np.sqrt(mean_squared_error(y_pred_brr, y_test)))\n",
    "\n",
    "brr = BayesianRidge(compute_score=True)\n",
    "brr.fit(X_train, y_train_origial)\n",
    "y_pred_brr = brr.predict(X_test)\n",
    "y_pred_brr_coverted = convert_back_y(y_pred_brr)\n",
    "\n",
    "print('brr with convert rmse val is ', np.sqrt(mean_squared_error(y_pred_brr_coverted, y_test)))\n",
    "\n",
    "print('clf fit finished!')\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "y_pred_ols = ols.predict(X_test)\n",
    "print('ols rmse val is ', np.sqrt(mean_squared_error(y_pred_ols, y_test)))\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train_origial)\n",
    "y_pred_ols = ols.predict(X_test)\n",
    "y_pred_ols_coverted = convert_back_y(y_pred_ols)\n",
    "print('ols with convert rmse val is ', np.sqrt(mean_squared_error(y_pred_ols_coverted, y_test)))\n",
    "\n",
    "print('ols fit finished!')\n",
    "param_regressor = {\n",
    "         'num_leaves': 101,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.007,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         'random_state': 2019\n",
    "}\n",
    "\n",
    "trn_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# print('start regressor training...')\n",
    "# lgb_model = lgb.train(param_regressor, trn_data, 500, verbose_eval=100)   \n",
    "# y_pred_lgb = lgb_model.predict(X_test)\n",
    "# print('lgb rmse val is ', np.sqrt(mean_squared_error(y_pred_lgb, y_test)))\n",
    "\n",
    "# clf rmse val is  589956.9420349462\n",
    "# clf fit finished!\n",
    "# ols rmse val is  795307.2592917023\n",
    "# ols fit finished!\n",
    "\n",
    "4292015.400922055\n",
    "2063346.6911373537\n",
    "clf fit finished!\n",
    "ols rmse val is  53089918.734005354\n",
    "ols with convert rmse val is  2063346.6911366559\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_regressor = {\n",
    "         'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.007,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         'random_state': 2019\n",
    "}\n",
    "\n",
    "X_train_lgb, X_val_lgb, y_train_lgb, y_val_lgb = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "trn_data = lgb.Dataset(X_train_lgb, label=y_train_lgb)\n",
    "val_data = lgb.Dataset(X_val_lgb, label=y_val_lgb)\n",
    "\n",
    "num_round = 2000\n",
    "print('start regressor training...')\n",
    "lgb_model = lgb.train(param_regressor, trn_data, num_round, valid_sets = [trn_data, val_data], \n",
    "                verbose_eval=100, early_stopping_rounds = 200)\n",
    "y_pred_lgb = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
    "print('lgb rmse val is ', np.sqrt(mean_squared_error(y_pred_lgb, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.81*1.052"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = stats.norm.rvs(loc=0, scale=1. /5, size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.mean(), vv.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.3/4.7-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "53.6/42.1-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32.64*1.042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12*18992.3/208000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': 1.,\n",
    "                    'B': pd.Timestamp('20130102'),\n",
    "                    'C': [100., 50., -30., -50.]})\n",
    "\n",
    "        \n",
    "df2 = pd.DataFrame({'A': 1.,\n",
    "                  'B': pd.Timestamp('20130102'),\n",
    "                  'C': pd.Series(1, index=list(range(4)), dtype='float32')}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_func(df1, df2):\n",
    "    for df in (df1, df2):\n",
    "        df['c_new111'] = df['C']*0.77\n",
    "new_func(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3], [2, 3, 5]])\n",
    "X_train = np.array([1, 2, 3])\n",
    "\n",
    "print('X.shape is', X.shape, 'X_train.shape is', X_train.shape)\n",
    "# results = compute_distances_no_loops(X, X_train)\n",
    "\n",
    "print(X-X_train)\n",
    "\n",
    "np.linalg.norm(X-X_train, axis=1, ord=2), np.sqrt(6)\n",
    "\n",
    "result = np.linalg.norm(X-X_train, axis=1, ord=2)\n",
    "print(result==result.min())\n",
    "\n",
    "# results\n",
    "# from numpy.linalg import norm\n",
    "# norm(X-(X_train[:, np.newaxis]), axis=0, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [(1160, 7792), (693,2375), (369,961), \n",
    "       (301,871), (311,802), (218,656), (221,521)]\n",
    "fork_num_lst = list(map(lambda x: x[1], lst))\n",
    "print(fork_num_lst)\n",
    "\n",
    "# fig = plt.figure(1) \n",
    "\n",
    "# #figure对象的add_axes()可以在其中创建一个axes对象,\n",
    "# # add_axes()的参数为一个形如[left, bottom, width, height]的列表,取值范围在0与1之间;\n",
    "# ax = fig.add_axes([0.1, 0.5, 0.8, 0.5]) # 我们把它放在了figure图形的上半部分，对应参数分别为：left, bottom, width, height;\n",
    "# ax.set_xlabel('time')     #用axes对象的set_xlabel函数来设置它的xlabel\n",
    "\n",
    "# line =ax.plot(range(5))[0]  #用axes对象的plot()进行绘图,它返回一个Line2D的对象;\n",
    "# line.set_color('r')             # 再调用Line2D的对象的set_color函数设置color的属性;\n",
    "# plt.show()\n",
    "\n",
    "plt.plot(fork_num_lst)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([('bird', 389.0),\n",
    "                   ('bird', 24.0),\n",
    "                   ('mammal', 80.5),\n",
    "                   ('mammal', np.nan)],\n",
    "                   index=['falcon', 'parrot', 'lion', 'monkey'],\n",
    "                   columns=('class', 'max_speed'))\n",
    "\n",
    "# >>> df\n",
    "#          class  max_speed\n",
    "# falcon    bird      389.0\n",
    "# parrot    bird       24.0\n",
    "# lion    mammal       80.5\n",
    "# monkey  mammal        NaN\n",
    "df['aaa'] = [4, 5, 6, 8]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.reset_index(name='new_col')\n",
    "\n",
    "# (df.groupby('class')\n",
    "#           .size()\n",
    "#           .reset_index(name='transactions_count'))\n",
    "\n",
    "x = np.arange(6).reshape((2,3))\n",
    "\n",
    "x = np.array([[0, -100], [30, 22], [11, 44]])\n",
    "\n",
    "print('x is', x)\n",
    "np.ptp(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(data))\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "print(scaler.mean_)\n",
    "print(scaler.var_)\n",
    "print(scaler.transform(data))\n",
    "print(scaler.transform([[2, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol_lst = [0.001, 0.01, 0.1]\n",
    "alpha_1_lst = [1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01]\n",
    "alpha_2_lst = [1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01]\n",
    "lambda_1_lst = [1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01]\n",
    "lambda_2_lst = [1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01]\n",
    "normalize_lst = [True, False]\n",
    "\n",
    "import itertools\n",
    "itertools.combinations(tol_lst,2)\n",
    "\n",
    "import itertools\n",
    "a = [[1,2,3],[4,5,6],[7,8,9,10]] # 3*3*4\n",
    "ll = list(itertools.product(*a))\n",
    "ll = list(itertools.product(tol_lst, alpha_1_lst, normalize_lst))\n",
    "\n",
    "for tol, alpha, normalize in itertools.product(tol_lst, alpha_1_lst, normalize_lst):\n",
    "    print('tol ', tol, 'alpha ', alpha, 'normalize ', normalize)\n",
    "# print(ll)\n",
    "print('len of ll is ', len(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "# https://datawhatnow.com/feature-importance/\n",
    "# https://github.com/Microsoft/LightGBM/issues/826\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "data_path = 'F:/liver_disease_patients_intelligent_segmentation/data/'\n",
    "output_path = 'F:/liver_disease_patients_intelligent_segmentation/output/'\n",
    "\n",
    "print('data_path is ', data_path)\n",
    "print('output_path is ', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_features_importance_bar(features, feature_importance):\n",
    "    df_feat_importance = pd.DataFrame({\n",
    "            'column': features,\n",
    "            'importance': feature_importance,\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "    df_feat_importance.to_csv(output_path + '/df_feat_importance.csv', index=0, sep='\\t')\n",
    "\n",
    "    plt.figure(figsize=(25, 6))\n",
    "    #plt.yscale('log', nonposy='clip')\n",
    "    plt.bar(range(len(feature_importance)), feature_importance, align='center')\n",
    "    plt.xticks(range(len(feature_importance)), features, rotation='vertical')\n",
    "    plt.title('Feature importance')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def convert_2_md5(value):\n",
    "    return hashlib.md5(str(value).encode('utf-8')).hexdigest()\n",
    "    # return hashlib.md5(str(value)).hexdigest()\n",
    "\n",
    "def split_by_user_id(df_merged, train_ratio=0.67):\n",
    "    df_merged['md5_val'] = df_merged['buy_user_id'].apply(convert_2_md5)\n",
    "    # df_merged['md5_val'] = df_merged['md5_val'].apply(str)\n",
    "    print('df_merged.dtypes is ', df_merged.dtypes)\n",
    "\n",
    "    df_user_id_md5_val = pd.DataFrame()\n",
    "    df_user_id_md5_val['md5_val'] = df_merged['md5_val'].sort_values(ascending=True)\n",
    "\n",
    "    # df_merged_sorted = df_merged.sort_values(by=['md5_val'])\n",
    "    # print('df_merged_sorted head is ', df_merged_sorted.head(5))\n",
    "\n",
    "    # df_merged_sorted.to_csv('./data/hive_sql_merged_instances_sorted.csv', \n",
    "    #     sep='\\t', date_format='%Y/%m/%d', index=0)  # date_format='%Y-%m-%d %H:%M:%s'\n",
    "    train_num = int(df_merged.shape[0]*train_ratio)\n",
    "    print('train_num is ', train_num)\n",
    "    pivot_val = df_user_id_md5_val['md5_val'][train_num]\n",
    "\n",
    "    print('train_num is: ', train_num, 'pivot_val is: ', pivot_val,\n",
    "          'type of pivot_val is', type(pivot_val))\n",
    "\n",
    "    # df_merged_train = df_merged_sorted[df_merged_sorted['md5_val']<=pivot_val]\n",
    "    # df_merged_test = df_merged_sorted[df_merged_sorted['md5_val']>pivot_val]\n",
    "\n",
    "    df_merged_train = df_merged.loc[df_merged['md5_val'] <= pivot_val]\n",
    "    df_merged_test = df_merged.loc[df_merged['md5_val'] > pivot_val]\n",
    "\n",
    "    print('after split df_merged_train.shape is ', df_merged_train.shape,\n",
    "          'df_merged_test.shape is ', df_merged_test.shape)\n",
    "\n",
    "    # df_merged_train = df_merged_sorted[:train_num]\n",
    "    # df_merged_test = df_merged_sorted[train_num:]\n",
    "\n",
    "    # df_merged_train.to_csv('./data/hive_sql_merged_instances_train.csv', sep='\\t', index=0)\n",
    "    # df_merged_test.to_csv('./data/hive_sql_merged_instances_test.csv', sep='\\t', index=0)\n",
    "\n",
    "    return df_merged_train, df_merged_test\n",
    "\n",
    "\n",
    "def compute_density_multiple(y_true, y_predict, threshold=10, by_percentage=True, top=True):\n",
    "    df = pd.DataFrame({'y_true': y_true, 'y_predict': y_predict})\n",
    "    df.sort_values(by=['y_predict'], ascending=False, inplace=True)\n",
    "\n",
    "    density_whole = sum(df['y_true'])/df.shape[0]\n",
    "    if by_percentage:\n",
    "        if top:\n",
    "            df_target = df[:int(threshold*0.01*df.shape[0])]\n",
    "        else:\n",
    "            df_target = df[-int(threshold*0.01*df.shape[0]):]\n",
    "    else:\n",
    "        if top:\n",
    "            df_target = df[:threshold]\n",
    "        else:\n",
    "            df_target = df[-threshold:]\n",
    "    density_partial = sum(df_target['y_true'])/df_target.shape[0]\n",
    "    density_mutiple = density_partial/density_whole\n",
    "    return density_mutiple\n",
    "\n",
    "def generate_real_test_df_quick():\n",
    "    df_test_real = pd.read_csv(data_path + '/hive_sql_unassigned_buyuser_output_processed.csv',\n",
    "                               sep='\\t')\n",
    "    print('df_test_real.shape is ', df_test_real.shape)\n",
    "    return df_test_real\n",
    "\n",
    "def generate_real_test_df():\n",
    "    print('in generate_real_test_df')\n",
    "\n",
    "    df_test_real = pd.read_csv(data_path + '/hql_liver_merged_visits_instances_df_data.csv',\n",
    "                               parse_dates=[1], infer_datetime_format=True,\n",
    "                               names=['buy_user_id', 'creation_date', 'is_pos'])\n",
    "\n",
    "    print('df_test_real.shape is ', df_test_real.shape)\n",
    "\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###------ 加上F的feature  ----- ###\n",
    "    ###----------------------------###\n",
    "    df_frequency = pd.read_csv('./unassigned/data/hql_liver_F_data.csv', parse_dates=[1], infer_datetime_format=True)\n",
    "    df_test_real = pd.merge(df_test_real, df_frequency, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_test_real.shape after add frequency is ', df_test_real.shape)\n",
    "    del df_frequency\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###------ 加上M的feature  -----###\n",
    "    ###----------------------------###\n",
    "    df_monetary = pd.read_csv('./unassigned/data/hive_sql_M_data.csv', parse_dates=[1], infer_datetime_format=True)\n",
    "    df_test_real = pd.merge(df_test_real, df_monetary, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_test_real.shape after add monetary is ', df_test_real.shape)\n",
    "    del df_monetary\n",
    "\n",
    "    ###---------------------------------###\n",
    "    ###   加上first order的features      ###\n",
    "    ###   包含：                         ###\n",
    "    ###   1. 订单距离电话回访时间的天数    ###\n",
    "    ###   2. 订单金额                    ###\n",
    "    ###   3. 订单来源                    ###\n",
    "    ###   4. 订单支付方式                ###\n",
    "    ###---------------------------------###\n",
    "    df_first_order = pd.read_csv('./unassigned/data/hive_sql_first_order_data.csv', \n",
    "                                 parse_dates=[1, 2], infer_datetime_format=True,\n",
    "                                 dtype={'first_origin_type': str, 'first_payment_type': str})\n",
    "    df_first_order['gap_days_first_order'] = (df_first_order['creation_date'] - df_first_order['order_dt']).dt.days\n",
    "    df_first_order.drop(['order_dt'], axis=1, inplace=True)\n",
    "    df_test_real = pd.merge(df_test_real, df_first_order, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_test_real.shape after add first order is ', df_test_real.shape)\n",
    "    del df_first_order\n",
    "\n",
    "    ###---------------------------------###\n",
    "    ###   加上last order的features       ###\n",
    "    ###   包含：                         ###\n",
    "    ###   1. 订单距离电话回访时间的天数    ###\n",
    "    ###   2. 订单金额                    ###\n",
    "    ###   3. 订单来源                    ###\n",
    "    ###   4. 订单支付方式                ###\n",
    "    ###---------------------------------###\n",
    "    df_last_order = pd.read_csv('./unassigned/data/hive_sql_last_order_data.csv', \n",
    "                                parse_dates=[1, 2], infer_datetime_format=True,\n",
    "                                dtype={'last_origin_type': str, 'last_payment_type': str})\n",
    "    df_last_order['gap_days_last_order'] = (df_last_order['creation_date'] - df_last_order['order_dt']).dt.days\n",
    "    df_last_order.drop(['order_dt'], axis=1, inplace=True)\n",
    "    df_test_real = pd.merge(df_test_real, df_last_order, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_test_real.shape after add last order is ', df_test_real.shape)\n",
    "    del df_last_order\n",
    "\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###--- 收货地址省份的feature  --###\n",
    "    ###----------------------------###\n",
    "    df_address = pd.read_csv('./unassigned/data/hive_sql_address_data.csv', dtype={'rand_address_code': str})\n",
    "    df_address.rename(columns={'rand_address_code':'address_code'}, inplace = True)\n",
    "    # df_address['address_code'] = df_address['rand_address_code'].apply(str)\n",
    "    # df_address.drop(['rand_address_code'], axis=1, inplace=True)\n",
    "    df_test_real = pd.merge(df_test_real, df_address, how='left', on=['buy_user_id'])\n",
    "    print('df_test_real.shape after add address code is ', df_test_real.shape)\n",
    "    del df_address\n",
    "\n",
    "\n",
    "    ###----------------------------------------------------------###\n",
    "    ###------ 加上class_code 和 branch_code的feature -------------###\n",
    "    ###----------------------------------------------------------###\n",
    "    df_class_code = pd.read_csv('./unassigned/data/hive_sql_patient_class_data.csv', \n",
    "                                dtype={'class_code': str, 'branch_code': str})\n",
    "    df_test_real = pd.merge(df_test_real, df_class_code, how='left', on=['buy_user_id'])\n",
    "    print('df_test_real.shape after add class_code, branch_code code is ', df_test_real.shape)\n",
    "    del df_class_code\n",
    "\n",
    "    ###----------------------------------------------------------###\n",
    "    ###------ 加上电话回访时间所在的月份的feature -----------------###\n",
    "    ###----------------------------------------------------------###\n",
    "    df_test_real['call_month'] = df_test_real['creation_date'].dt.month.apply(str)\n",
    "    df_test_real['call_weekday'] = df_test_real['creation_date'].dt.weekday.apply(str)\n",
    "    print('df_test_real.shape after add call_month call_weekday is ', df_test_real.shape)\n",
    "\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###--- 收货地址个数的feature  --###\n",
    "    ###----------------------------###\n",
    "    df_address_num = pd.read_csv('./unassigned/data/hive_sql_address_num_data.csv')\n",
    "    df_merged = pd.merge(df_test_real, df_address_num, how='left', on=['buy_user_id'])\n",
    "    print('df_test_real.shape after add address number feature is ', df_test_real.shape)\n",
    "    # print('df_test_real.dtypes after add address number feature is ', df_test_real.dtypes)\n",
    "    del df_address_num\n",
    "\n",
    "    start_t = time.time()\n",
    "    df_test_real.to_csv('./unassigned/data/hive_sql_unassigned_buyuser_output_processed', \n",
    "                        index=False, sep='\\t')    \n",
    "    print('df_test_real store cost time:', time.time()-start_t)\n",
    "\n",
    "    return df_test_real\n",
    "\n",
    "def get_training_data_quick():\n",
    "    print('in get_training_data_quick()')\n",
    "    start_t = time.time()\n",
    "    df_merged = pd.read_csv(data_path + '/df_merged_processed.csv')\n",
    "    print('read df_merged from csv cost time: ', time.time()-start_t,\n",
    "          'df_merged.shape:', df_merged.shape)\n",
    "    return df_merged\n",
    "\n",
    "def get_training_data():\n",
    "    print('in get_training_data()')\n",
    "    df_merged = pd.read_csv(data_path + '/hql_liver_merged_visits_instances_df_data.csv',\n",
    "                            parse_dates=[1], infer_datetime_format=True)\n",
    "\n",
    "    # df_merged['is_pos']= pd.to_numeric(df_merged['is_pos'], downcast='integer', errors='coerce')\n",
    "    # df_merged['row_num'] = list(np.array(list(range(len(df_merged)))))\n",
    "    # print('the row_num is ', df_merged.loc[df_merged.is_pos.isnull()]['row_num'])\n",
    "#     print('df_merged.head(20) is ', df_merged.head(20))\n",
    "    print(df_merged.isnull().sum())\n",
    "    print('df_merged.shape is ', df_merged.shape)\n",
    "    # df_merged['is_pos'] = df_merged['is_pos'].astype(np.int8)\n",
    "                            # dtype={'is_pos': np.int8})\n",
    "    # df_merged['is_pos'] = df_merged['is_pos'].map({'True': 1, 'False': 0})\n",
    "    # df_merged['is_pos'] = df_merged['is_pos'].astype(np.int32)\n",
    "\n",
    "    #抽样做训练集\n",
    "    sample_num = 800000\n",
    "    df_merged = df_merged.sample(n=sample_num, random_state=42)\n",
    "    print('df_merged shape is ', df_merged.shape)\n",
    "    print('df_merged dtypes is ', df_merged.dtypes)\n",
    "\n",
    "    # return df_merged\n",
    "\n",
    "    # split_by_user_id(df_merged)\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###------ 加上R的feature  -----###\n",
    "    ###----------------------------###\n",
    "    # df_recency = pd.read_csv('./data/hive_sql_R_data.csv', parse_dates=[1, 2], infer_datetime_format=True)\n",
    "    # df_recency = pd.read_csv('./data/hive_sql_R_data.csv')\n",
    "    # df_recency['creation_date'] = pd.to_datetime(df_recency['creation_date'], \n",
    "    #     format='%Y-%m-%d %H:%M:%S', errors='ignore')\n",
    "    # df_recency['recency_date'] = pd.to_datetime(df_recency['recency_date'], \n",
    "    #     format='%Y-%m-%d %H:%M:%S', errors='ignore')\n",
    "\n",
    "    # df_recency['gap_days'] = (df_recency['creation_date'] - df_recency['recency_date']).dt.days\n",
    "    # df_merged = pd.merge(df_merged, df_recency, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    # df_merged.drop(['recency_date'], axis=1, inplace=True)\n",
    "    # print('df_merged.shape after add R is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add R is ', df_merged.dtypes)\n",
    "\n",
    "    # df_merged.drop(['gap_days'], axis=1, inplace=True)\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###------ 加上F的feature  ----- ###\n",
    "    ###----------------------------###\n",
    "    df_frequency = pd.read_csv(data_path + '/hql_liver_F_data.csv', parse_dates=[1],\n",
    "                               infer_datetime_format=True)\n",
    "    df_merged = pd.merge(df_merged, df_frequency, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_merged.shape after add frequency is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add frequency is ', df_merged.dtypes)\n",
    "    del df_frequency\n",
    "\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###------ 加上M的feature  -----###\n",
    "    ###----------------------------###\n",
    "    df_monetary = pd.read_csv(data_path + '/hql_liver_M_data.csv', parse_dates=[1],\n",
    "                              infer_datetime_format=True)\n",
    "    df_merged = pd.merge(df_merged, df_monetary, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_merged.shape after add monetary is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add monetary is ', df_merged.dtypes)\n",
    "    del df_monetary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###---------------------------------###\n",
    "    ###   加上first order的features      ###\n",
    "    ###   包含：                         ###\n",
    "    ###   1. 订单距离电话回访时间的天数    ###\n",
    "    ###   2. 订单金额                    ###\n",
    "    ###   3. 订单来源                    ###\n",
    "    ###   4. 订单支付方式                ###\n",
    "    ###---------------------------------###\n",
    "    df_first_order = pd.read_csv(data_path + '/hql_liver_first_order_data.csv',\n",
    "                                 parse_dates=[1, 2], infer_datetime_format=True,\n",
    "                                 dtype={'first_origin_type': str, 'first_payment_type': str})\n",
    "    df_first_order['gap_days_first_order'] = (df_first_order['creation_date'] - df_first_order['order_dt']).dt.days\n",
    "    df_first_order.drop(['order_dt'], axis=1, inplace=True)\n",
    "    df_merged = pd.merge(df_merged, df_first_order, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_merged.shape after add first order is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add first order is ', df_merged.dtypes)\n",
    "    del df_first_order\n",
    "\n",
    "\n",
    "\n",
    "    ###---------------------------------###\n",
    "    ###   加上last order的features       ###\n",
    "    ###   包含：                         ###\n",
    "    ###   1. 订单距离电话回访时间的天数    ###\n",
    "    ###   2. 订单金额                    ###\n",
    "    ###   3. 订单来源                    ###\n",
    "    ###   4. 订单支付方式                ###\n",
    "    ###---------------------------------###\n",
    "    df_last_order = pd.read_csv(data_path + '/hql_liver_last_order_data.csv',\n",
    "                                parse_dates=[1, 2], infer_datetime_format=True,\n",
    "                                dtype={'last_origin_type': str, 'last_payment_type': str})\n",
    "    df_last_order['gap_days_last_order'] = (df_last_order['creation_date'] - df_last_order['order_dt']).dt.days\n",
    "    df_last_order.drop(['order_dt'], axis=1, inplace=True)\n",
    "    df_merged = pd.merge(df_merged, df_last_order, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    print('df_merged.shape after add last order is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add last order is ', df_merged.dtypes)\n",
    "    del df_last_order\n",
    "\n",
    "\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###--- 收货地址省份的feature  --###\n",
    "    ###----------------------------###\n",
    "    df_address = pd.read_csv(data_path + '/hql_liver_address_data.csv', dtype={'rand_address_code': str})\n",
    "    df_address.rename(columns={'rand_address_code':'address_code'}, inplace = True)\n",
    "    # df_address['address_code'] = df_address['rand_address_code'].apply(str)\n",
    "    # df_address.drop(['rand_address_code'], axis=1, inplace=True)\n",
    "    df_merged = pd.merge(df_merged, df_address, how='left', on=['buy_user_id'])\n",
    "    print('df_merged.shape after add address code is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add address code is ', df_merged.dtypes)\n",
    "    del df_address\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###----------------------------------------------------------###\n",
    "    ###------ 加上class_code 和 branch_code的feature -------------###\n",
    "    ###----------------------------------------------------------###\n",
    "    # df_class_code = pd.read_csv('./data/hive_sql_patient_class_data.csv',\n",
    "    #                             dtype={'class_code': str, 'branch_code': str})\n",
    "    # # df_class_code['class_code'] = df_class_code['class_code'].apply(str)\n",
    "    # # df_class_code['branch_code'] = df_class_code['branch_code'].apply(str)\n",
    "    # df_merged = pd.merge(df_merged, df_class_code, how='left', on=['buy_user_id'])\n",
    "    # print('df_merged.shape after add class_code, branch_code code is ', df_merged.shape)\n",
    "    # # print('df_merged.dtypes after add class_code, branch_code code is ', df_merged.dtypes)\n",
    "    # del df_class_code\n",
    "\n",
    "\n",
    "    ###----------------------------------------------------------###\n",
    "    ###------ 加上start_app count的feature -----------------------###\n",
    "    ###----------------------------------------------------------###\n",
    "    # df_start_app_cnt = pd.read_csv('./data/hive_sql_startapp_cnt_data.csv')\n",
    "    # df_start_app_cnt.rename(columns={'cnt':'start_app_cnt'}, inplace = True)\n",
    "    # df_merged = pd.merge(df_merged, df_start_app_cnt, how='left', on=['buy_user_id', 'creation_date'])\n",
    "    # print('df_merged.shape after add start_app count, branch_code code is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add start_app count, branch_code code is ', df_merged.dtypes)\n",
    "    # del df_start_app_cnt\n",
    "\n",
    "\n",
    "    ###----------------------------------------------------------###\n",
    "    ###------ 加上电话回访时间所在的月份的feature -----------------###\n",
    "    ###----------------------------------------------------------###\n",
    "    df_merged['call_month'] = df_merged['creation_date'].dt.month.apply(str)\n",
    "    df_merged['call_weekday'] = df_merged['creation_date'].dt.weekday.apply(str)\n",
    "    print('df_merged.shape after add start_app count, branch_code code is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add start_app count, branch_code code is ', df_merged.dtypes)\n",
    "\n",
    "\n",
    "    ###----------------------------###\n",
    "    ###--- 收货地址个数的feature  --###\n",
    "    ###----------------------------###\n",
    "    df_address_num = pd.read_csv(data_path + '/hql_liver_address_num_data.csv')\n",
    "    df_merged = pd.merge(df_merged, df_address_num, how='left', on=['buy_user_id'])\n",
    "    print('df_merged.shape after add address number feature is ', df_merged.shape)\n",
    "    # print('df_merged.dtypes after add address number feature is ', df_merged.dtypes)\n",
    "\n",
    "    del df_address_num\n",
    "\n",
    "    print('after all preprocessing, final df_merged.dtypes is ', df_merged.dtypes)\n",
    "\n",
    "    start_t = time.time()\n",
    "    df_merged.to_csv(data_path + '/df_merged_processed.csv', index=False)\n",
    "    print('store df_merged to_csv cost time: ', time.time()-start_t)\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "def training_model(df_merged, category_feas_onehot=False):\n",
    "    if category_feas_onehot:\n",
    "        print('pre get_dummies, df_merged.dtypes: ', df_merged.dtypes)\n",
    "        df_merged = pd.get_dummies(df_merged,\n",
    "                                   columns=['address_code', 'call_month', 'call_weekday',\n",
    "                                            'first_payment_type', 'first_origin_type',\n",
    "                                            'last_payment_type', 'last_origin_type',\n",
    "                                            'first_order_status', 'last_order_status'])\n",
    "                                   # sparse=True)\n",
    "\n",
    "        # print('after get_dummies, df_merged.dtypes: ', df_merged.dtypes)\n",
    "\n",
    "    df_merged_train, df_merged_test = split_by_user_id(df_merged, train_ratio=0.67)\n",
    "    del df_merged\n",
    "\n",
    "    df_merged_train.drop(['buy_user_id', 'creation_date', 'md5_val'], axis=1, inplace=True)\n",
    "    df_merged_test.drop(['buy_user_id', 'creation_date', 'md5_val'], axis=1, inplace=True)\n",
    "\n",
    "    print('df_merged_train.shape df_merged_test.shape: ', df_merged_train.shape, df_merged_test.shape)\n",
    "\n",
    "    df_train_y = df_merged_train['is_pos']\n",
    "    df_train_X = df_merged_train.drop(['is_pos'], axis=1)\n",
    "\n",
    "    df_test_y = df_merged_test['is_pos']\n",
    "    df_test_X = df_merged_test.drop(['is_pos'], axis=1)\n",
    "\n",
    "    feature_names = df_train_X.columns.tolist()\n",
    "\n",
    "    if category_feas_onehot:\n",
    "        d_train = lgb.Dataset(df_train_X.values, label=df_train_y.values, feature_name=feature_names)\n",
    "    else:\n",
    "        d_train = lgb.Dataset(df_train_X.values, label=df_train_y.values, feature_name=feature_names,\n",
    "                              categorical_feature=['address_code', 'call_month', 'call_weekday',\n",
    "                                                   'first_payment_type', 'first_origin_type',\n",
    "                                                   'last_payment_type', 'last_origin_type',\n",
    "                                                   'first_order_status', 'last_order_status'])\n",
    "\n",
    "    # branch_code, class_code\n",
    "    # y_pred.shape is  (263994,)\n",
    "    # auc_score is  0.8494185939558818 predict cost time: 5.660183906555176\n",
    "\n",
    "    params = {'learning_rate': 0.08, 'boosting_type': 'gbdt', 'objective': 'binary',\n",
    "              'metric': 'binary_logloss', 'sub_feature': 0.85, 'sub_sample': 0.7,\n",
    "              'num_leaves': 100, 'min_data': 400, 'max_depth': 13, 'random_state': 42}\n",
    "\n",
    "    # lgbm = lgb.LGBMClassifier(n_estimators=500, n_jobs=-1, learning_rate=0.08,\n",
    "    #                          random_state=42, max_depth=13, min_child_samples=400,\n",
    "    #                          num_leaves=100, subsample=0.7, colsample_bytree=0.85,\n",
    "    #                          silent=-1, verbose=-1, boosting_type='gbdt')\n",
    "\n",
    "    print('lgb training starts')\n",
    "    start_t = time.time()\n",
    "    clf = lgb.train(params, d_train, 500)\n",
    "    print('lgb training ends, cost time', time.time() - start_t)\n",
    "\n",
    "    start_t = time.time()\n",
    "    y_pred = clf.predict(df_test_X.values)\n",
    "    print('y_pred.shape is ', y_pred.shape)\n",
    "    auc_score = roc_auc_score(df_test_y, y_pred)\n",
    "    print('auc_score is ', auc_score, 'predict cost time:', time.time() - start_t)\n",
    "    #\n",
    "    # print('top 200 ratio_multiple is',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=200, by_percentage=False),\n",
    "    #       'top 500 ratio_multiple is',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=500, by_percentage=False),\n",
    "    #       'ratio_multiple top 1 is ',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=1),\n",
    "    #       'ratio_multiple top 5 is ',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=5),\n",
    "    #       'ratio_multiple top 10 is ',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=10),\n",
    "    #       'ratio_multiple top 20 is ',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=20),\n",
    "    #       'ratio_multiple top 30 is',\n",
    "    #       compute_density_multiple(df_test_y, y_pred, threshold=30)\n",
    "    # )\n",
    "\n",
    "    importance = clf.feature_importance(importance_type='split')\n",
    "    feature_name = clf.feature_name()\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature_name': feature_name,\n",
    "        'importance': importance}\n",
    "    ).sort_values(by='importance', ascending=False)\n",
    "    feature_importance.to_csv(output_path + '/lgb_feat_importance_split.csv', index=False)\n",
    "\n",
    "    show_features_importance_bar(feature_importance['feature_name'][:25],\n",
    "                                 feature_importance['importance'][:25])\n",
    "    return feature_importance\n",
    "\n",
    "def align_test_data_with_train_data(test_df, train_feas_columns):\n",
    "    missing_cols = set(train_feas_columns) - set(test_df.columns)\n",
    "    for c in missing_cols:\n",
    "        test_df[c] = 0\n",
    "    test_df = test_df[train_feas_columns]\n",
    "    return test_df\n",
    "\n",
    "\n",
    "df_merged = get_training_data()\n",
    "# df_merged = get_training_data_quick()\n",
    "print('df_merged.shape is ', df_merged.shape)\n",
    "feature_importance_out = training_model(df_merged, category_feas_onehot=True)\n",
    "print('program ends')\n",
    "\n",
    "# importance = clf.feature_importance(importance_type='gain')\n",
    "# feature_name = clf.feature_name()\n",
    "# feature_importance = pd.DataFrame({\n",
    "#                          'feature_name':feature_name,\n",
    "#                          'importance':importance}\n",
    "#                      ).sort_values(by='importance', ascending=False)\n",
    "# feature_importance.to_csv('./model_output/lgb_feat_importance_gain.csv',index=False)\n",
    "\n",
    "# plt.figure(figsize=(12,6))\n",
    "# lgb.plot_importance(clf, max_num_features=30,  importance_type='split')\n",
    "# plt.show()\n",
    "# plt.savefig('./lgbm_importances.png')\n",
    "#\n",
    "# del df_train_y, df_train_X, df_test_y, df_test_X, df_merged_train, df_merged_test\n",
    "# gc.collect()\n",
    "#\n",
    "#\n",
    "# ###########################################################################################\n",
    "\n",
    "# df_test_real = generate_real_test_df()\n",
    "# print('finished!')\n",
    "\n",
    "# df_test_real = generate_real_test_df_quick()\n",
    "# print('after generate_real_test_df df_test_real shape is ', df_test_real.shape)\n",
    "#\n",
    "# final_outcome = pd.DataFrame()\n",
    "# final_outcome['buy_user_id'] = df_test_real['buy_user_id']\n",
    "# df_test_real.drop(['buy_user_id', 'creation_date'], axis=1, inplace=True)\n",
    "#\n",
    "# print('start real_test!!! ')\n",
    "# start_t = time.time()\n",
    "# y_pred=clf.predict(df_test_real.values)\n",
    "# final_outcome['y'] = y_pred\n",
    "# # final_outcome['y'] = np.round(y_pred, 8)\n",
    "# print('y_pred.shape is ', y_pred.shape, 'df_test_real.shape is ', df_test_real.shape)\n",
    "# print('final predict cost time:', time.time()-start_t)\n",
    "#\n",
    "# final_outcome.sort_values(by='y', ascending=False, inplace=True)\n",
    "# final_outcome.to_csv('./model_output/final_outcome.csv', index=False, sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_out['importance'][:7].sum()/feature_importance_out['importance'].sum()\n",
    "\n",
    "# (1079808, 269) (249612, 269)\n",
    "\n",
    "373705 + 755964\n",
    "\n",
    "#  (199751, 19) df_merged.shape is  (1129669, 19)\n",
    "    \n",
    "199751/(199751 + 1129669)\n",
    "\n",
    "\n",
    "y_pred.shape is  (198221,) model predict cost time:  4.6558146476745605\n",
    "predict auc_score is  0.8541811797702418\n",
    "get outcome finished!\n",
    "program ends\n",
    "\n",
    "# y_pred.shape is  (373705,)\n",
    "# auc_score is  0.8534907375048523 predict cost time: 9.141904592514038\n",
    "# pickle dump finished, cost time:  0.16290831565856934\n",
    "\n",
    "# df_check.shape is  (198221, 19) df_merged.shape is  (1131199, 19)\n",
    "\n",
    "#   (754824, 263) df_merged_test.shape is  (376375, 263)\n",
    "754824 + 376375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_importance_out_bakup = feature_importance_out.copy()\n",
    "\n",
    "feature_importance_out['feature_name'][:7]\n",
    "\n",
    "def show_features_importance_bar_new(features, feature_importance):\n",
    "    df_feat_importance = pd.DataFrame({\n",
    "            'column': features,\n",
    "            'importance': feature_importance,\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "    df_feat_importance.to_csv(output_path + '/df_feat_importance.csv', index=0, sep='\\t')\n",
    "\n",
    "    plt.figure(figsize=(25, 6))\n",
    "    #plt.yscale('log', nonposy='clip')\n",
    "    plt.bar(range(len(feature_importance)), feature_importance, align='center')\n",
    "    plt.xticks(range(len(feature_importance)), features, rotation='vertical')\n",
    "    plt.title('Feature importance')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    feature_importance_out\n",
    "feature_importance_out['importance'] = feature_importance_out_bakup['importance']/feature_importance_out_bakup['importance'].sum()\n",
    "show_features_importance_bar_new(feature_importance_out['feature_name'][:25],\n",
    "                                 feature_importance_out['importance'][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "667395/(667395+132605)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 bayes ridge regression回归\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE_TSG)\n",
    "# folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "\n",
    "oof_regressor_brr = np.zeros(len(df_train))\n",
    "brr_predictions = np.zeros(len(df_test))\n",
    "\n",
    "def norm_df(df):\n",
    "    print('in norm_df')\n",
    "    df_norm = (df - df.mean()) / (df.max() - df.min())\n",
    "    return df_norm\n",
    "\n",
    "def fill_with_mean(df, feas):\n",
    "    print('in fill_with_mean')\n",
    "    for fea in feas:\n",
    "        df[fea].fillna(df[fea].mean(), inplace=True)\n",
    "    return df\n",
    "    \n",
    "start = time.time()\n",
    "# brr = BayesianRidge(n_iter=1000, verbose=True) \n",
    "brr = BayesianRidge(n_iter=1000, tol=1e-6, alpha_1=1e-05, alpha_2=1e-05, verbose=True) \n",
    "\n",
    "df_train_brr = df_train.copy()\n",
    "df_test_brr = df_test.copy()\n",
    "\n",
    "# df_train_brr = df_train_brr.fillna(0.0)\n",
    "# df_test_brr = df_test_brr.fillna(0.0)\n",
    "\n",
    "print('start fill with mean')\n",
    "\n",
    "df_train_brr = fill_with_mean(df_train_brr, features)\n",
    "df_test_brr = fill_with_mean(df_test_brr, features)\n",
    "\n",
    "print('start norm')\n",
    "\n",
    "df_train_brr[features] = norm_df(df_train_brr[features])\n",
    "df_test_brr[features] = norm_df(df_test_brr[features])\n",
    "\n",
    "print('ending norm')\n",
    "\n",
    "print('start bayes ridge regression training')\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, df_train['outliers'].values)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    brr.fit(df_train_brr.iloc[trn_idx][features], df_train_brr['target'].iloc[trn_idx])\n",
    "    oof_regressor_brr[val_idx] = brr.predict(df_train_brr.iloc[val_idx][features])\n",
    "    brr_predictions += brr.predict(df_test_brr[features].values) / 5.\n",
    "    print('final rmse val is ', np.sqrt(mean_squared_error(df_train_brr.iloc[val_idx]['target'].values, \n",
    "                                                           oof_regressor_brr[val_idx])))\n",
    "    \n",
    "print('final brr rmse val is ', np.sqrt(mean_squared_error(df_train_brr['target'].values, oof_regressor_brr)))\n",
    "\n",
    "# start norm\n",
    "# ending norm\n",
    "# start bayes ridge regression training\n",
    "# fold n°0\n",
    "# final rmse val is  3.806076950362962\n",
    "# fold n°1\n",
    "# final rmse val is  3.779403705505596\n",
    "# fold n°2\n",
    "# final rmse val is  150.34690570346518\n",
    "# fold n°3\n",
    "# final rmse val is  3.7754369642818877\n",
    "# fold n°4\n",
    "# final rmse val is  4.4753798374860585\n",
    "# final brr rmse val is  67.33055964373135\n",
    "\n",
    "#################################################\n",
    "# fill with 0 normal\n",
    "# fold n°0\n",
    "# final rmse val is  3.773954411258298\n",
    "# fold n°1\n",
    "# final rmse val is  3.759998900810395\n",
    "# fold n°2\n",
    "# final rmse val is  3.7582380865818834\n",
    "# fold n°3\n",
    "# final rmse val is  3.7558625132092125\n",
    "# fold n°4\n",
    "# final rmse val is  3.7688611690916343\n",
    "# final brr rmse val is  3.7633893292642013\n",
    "\n",
    "################################################\n",
    "# fill with mean normal\n",
    "\n",
    "# start bayes ridge regression training\n",
    "# fold n°0\n",
    "# final rmse val is  3.7732907463257748\n",
    "# fold n°1\n",
    "# final rmse val is  3.758590156304969\n",
    "# fold n°2\n",
    "# final rmse val is  3.7565954722634456\n",
    "# fold n°3\n",
    "# final rmse val is  3.7551300147222824\n",
    "# fold n°4\n",
    "# final rmse val is  3.767392908342587\n",
    "# final brr rmse val is  3.7622063927908376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.fillna(X_test.mean())\n",
    "\n",
    "X_test = np.array([[1, 2, np.nan], [2, np.nan, 3], [4, 5, 7], [np.nan, 3, 3]])\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n",
    "                  columns=['A', 'B', 'C'])\n",
    "\n",
    "df\n",
    "df.iat[2, 2] = 'abc'\n",
    "print(df)\n",
    "print(type(df.iat[2, 2]), df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime_num(num):\n",
    "    import math\n",
    "    for i in range(2, int(math.sqrt(num))):\n",
    "        if num%i==0:\n",
    "            print('i is ', i)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "for num in range(100000, 200000, 1):\n",
    "    if is_prime_num(num):\n",
    "        print('num is ', num)\n",
    "        break\n",
    "        \n",
    "# is_prime_num(10007)\n",
    "print(is_prime_num(2333))\n",
    "print(is_prime_num(2652124))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold, KFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [3, 9]])\n",
    "y = np.array([0, 0, 1, 1, 0])\n",
    "rkf = RepeatedKFold(n_splits=3, n_repeats=2, random_state=2652124)\n",
    "kf = KFold(n_splits=3, random_state=2652124)\n",
    "# for train_index, test_index in rkf.split(X):\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [77, 99], [88, 22], [11, 88]])\n",
    "y = np.array([0, 1, 1, 0, 0, 1, 0])\n",
    "rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=36851234)\n",
    "for train_index, test_index in rskf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print('#####################')\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, random_state=42)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print('#####################')\n",
    "    \n",
    "skf = StratifiedKFold(n_splits=3, random_state=21)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print('#####################')\n",
    "    \n",
    "skf = StratifiedKFold(n_splits=3, random_state=2019)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Operations\n",
    "\n",
    "\n",
    "| Operation     | Description                       || Operation     | Description                          |\n",
    "|---------------|-----------------------------------||---------------|--------------------------------------|\n",
    "| ``a == b``    | ``a`` equal to ``b``              || ``a != b``    | ``a`` not equal to ``b``             |\n",
    "| ``a < b``     | ``a`` less than ``b``             || ``a > b``     | ``a`` greater than ``b``             |\n",
    "| ``a <= b``    | ``a`` less than or equal to ``b`` || ``a >= b``    | ``a`` greater than or equal to ``b`` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a silly question. Of course it's an error. \n",
    "\n",
    "But what about..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Click the \"output\" button to see the answer)\n",
    "\n",
    "Python has precedence rules that determine the order in which operations get evaluated in expressions like above. For example, `and` has a higher precedence than `or`, which is why the first expression above is `True`. If we had evaluated it from left to right, we would have calculated `True or True` first (which is `True`), and then taken the `and` of that result with `False`, giving a final value of `False`.\n",
    "\n",
    "You could try to [memorize the order of precedence](https://docs.python.org/3/reference/expressions.html#operator-precedence), but a safer bet is to just use liberal parentheses. Not only does this help prevent bugs, it makes your intentions clearer to anyone who reads your code. \n",
    "\n",
    "For example, consider the following expression:\n",
    "\n",
    "```python\n",
    "prepared_for_weather = have_umbrella or rain_level < 5 and have_hood or not rain_level > 0 and is_workday\n",
    "```\n",
    "\n",
    "I'm trying to say that I'm safe from today's weather....\n",
    "- if I have an umbrella...\n",
    "- or if the rain isn't too heavy and I have a hood...\n",
    "- otherwise, I'm still fine unless it's raining *and* it's a workday\n",
    "\n",
    "But not only is my Python code hard to read, it has a bug. We can address both problems by adding some parentheses:\n",
    "\n",
    "```python\n",
    "for i in range(100):\n",
    "    sum += i\n",
    "    print(sum)\n",
    "prepared_for_weather = have_umbrella or (rain_level < 5 and have_hood) or not (rain_level > 0 and is_workday)\n",
    "```\n",
    "\n",
    "You can add even more parentheses if you think it helps readability:\n",
    "\n",
    "```python\n",
    "prepared_for_weather = have_umbrella or ((rain_level < 5) and have_hood) or (not (rain_level > 0 and is_workday))\n",
    "```\n",
    "\n",
    "We can also split it over multiple lines to emphasize the 3-part structure described above:\n",
    "\n",
    "```python\n",
    "prepared_for_weather = (\n",
    "    have_umbrella \n",
    "    or ((rain_level < 5) and have_hood) \n",
    "    or (not (rain_level > 0 and is_workday))\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
